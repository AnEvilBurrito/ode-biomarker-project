{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(torch.__version__)\n",
    "    # Setup device agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0281, -0.0740, -0.0258,  ..., -0.0820,  0.0069,  0.0379],\n",
      "        [-0.0641, -0.0269, -0.0203,  ..., -0.0015, -0.0187, -0.0585],\n",
      "        [-0.0005, -0.0302, -0.0770,  ..., -0.0720, -0.0771,  0.0402],\n",
      "        ...,\n",
      "        [-0.0576,  0.0650, -0.0482,  ..., -0.0684, -0.0816, -0.0977],\n",
      "        [ 0.0176, -0.0189, -0.0947,  ...,  0.0908,  0.0155, -0.0806],\n",
      "        [ 0.0866,  0.0281,  0.0280,  ...,  0.0916,  0.0967, -0.0675]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.1949e-02, -5.9762e-03,  6.6286e-02, -8.1473e-02, -5.8782e-02,\n",
      "         6.6604e-02,  4.7256e-02,  8.8177e-02,  6.3597e-02, -7.2984e-02,\n",
      "        -5.9919e-02,  1.4836e-03,  7.8904e-02, -7.8673e-02, -9.3304e-02,\n",
      "        -7.4985e-02, -2.5742e-02, -4.3548e-02, -4.1615e-02, -4.2085e-02,\n",
      "         6.6179e-02,  1.8719e-02,  7.7365e-03, -1.2027e-02,  2.6128e-02,\n",
      "        -4.7106e-02, -4.3181e-02,  2.9833e-02, -9.9660e-02,  5.3323e-02,\n",
      "         9.4328e-02, -2.2104e-03, -3.8498e-02, -6.0734e-02,  9.2418e-02,\n",
      "        -6.5804e-02,  4.0446e-02,  2.5886e-02,  1.5883e-02, -6.7907e-02,\n",
      "        -5.6565e-02, -2.2553e-02, -5.5987e-03, -4.1577e-02,  8.4033e-02,\n",
      "        -3.8615e-02,  6.5343e-02,  9.4101e-02,  4.7978e-02, -4.4360e-02,\n",
      "        -7.0638e-02,  2.4908e-02, -1.2812e-02,  3.4365e-02, -2.7418e-02,\n",
      "         8.1122e-02,  7.0249e-02,  3.6378e-02,  4.6255e-02,  6.7147e-02,\n",
      "         4.8988e-02,  2.2777e-02, -6.6960e-02, -4.8256e-03,  3.6618e-02,\n",
      "         5.6093e-02, -5.5983e-02,  3.8849e-02, -1.0059e-02,  2.8262e-02,\n",
      "        -9.7956e-02, -4.7640e-02, -1.8694e-02,  6.0532e-02, -6.4289e-02,\n",
      "        -2.2314e-02, -9.0681e-02, -7.2561e-02, -3.8469e-03, -4.7697e-02,\n",
      "        -9.1121e-02, -7.7512e-02,  4.4123e-02, -3.5786e-02,  8.9514e-02,\n",
      "         3.1355e-02,  2.7766e-04,  6.8384e-02, -4.0351e-02, -7.1346e-03,\n",
      "         3.2855e-02,  2.1609e-02, -3.9030e-02,  6.7064e-02,  6.2284e-02,\n",
      "         2.8758e-02,  3.6044e-02,  5.8593e-03, -5.8745e-02,  9.2652e-02,\n",
      "        -3.4028e-02, -2.5750e-02,  5.1115e-02, -8.9072e-02,  4.6317e-02,\n",
      "        -8.2433e-02,  8.0062e-02,  2.9400e-02,  5.6472e-02,  1.8479e-02,\n",
      "         2.3555e-02,  9.0814e-02, -3.2522e-02,  6.9796e-02, -1.6243e-02,\n",
      "        -9.0605e-02, -9.9819e-03,  2.2731e-02, -2.3132e-02, -6.6558e-02,\n",
      "        -5.1677e-02,  7.0500e-03, -2.4546e-02,  8.8830e-02, -4.8762e-02,\n",
      "         4.3771e-04,  2.9763e-02, -2.7651e-02,  2.9765e-02,  3.5748e-02,\n",
      "         9.0800e-02,  2.2314e-02, -2.0161e-04, -3.4595e-02, -6.8306e-02,\n",
      "        -1.9090e-02, -7.0001e-03,  5.4428e-02,  1.7446e-02,  6.3675e-02,\n",
      "        -9.9947e-02,  9.0872e-02,  8.5943e-02,  8.0148e-02,  8.4784e-02,\n",
      "         1.7586e-02,  4.6640e-02, -5.8155e-02,  2.7940e-02, -6.5669e-02,\n",
      "        -5.8396e-02, -9.0715e-02,  7.2098e-02, -3.1074e-02, -7.7251e-02,\n",
      "         2.0729e-02,  8.3232e-02, -8.2398e-02,  6.9639e-03,  2.7592e-02,\n",
      "         8.4791e-03, -8.7956e-02,  4.3864e-02,  3.3489e-02, -6.3405e-02,\n",
      "         3.2836e-02, -8.0723e-03,  4.3370e-05,  9.8178e-02, -6.6931e-02,\n",
      "        -1.2669e-02,  6.6096e-02, -3.2271e-02, -9.2787e-02,  7.5122e-02,\n",
      "        -4.8462e-02, -4.1316e-04, -1.2077e-02, -1.4616e-02, -3.1357e-02,\n",
      "         4.6199e-02, -1.7989e-02, -1.2861e-02, -4.8683e-02, -4.5789e-02,\n",
      "        -9.3096e-02,  5.8019e-02,  5.1905e-02, -7.5846e-02,  6.4526e-02,\n",
      "        -7.4551e-02,  8.8322e-02, -8.4129e-02,  5.9777e-02, -2.3211e-02,\n",
      "        -5.4766e-03,  9.6377e-02, -1.8041e-02, -3.9743e-02,  5.8390e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0509, -0.0080, -0.0585,  ..., -0.0455,  0.0644, -0.0054],\n",
      "        [-0.0393, -0.0511,  0.0533,  ..., -0.0211, -0.0665, -0.0075],\n",
      "        [ 0.0538,  0.0431,  0.0194,  ..., -0.0169, -0.0601,  0.0394],\n",
      "        ...,\n",
      "        [ 0.0608,  0.0307,  0.0078,  ...,  0.0325, -0.0291, -0.0472],\n",
      "        [ 0.0501, -0.0580,  0.0692,  ...,  0.0468, -0.0637, -0.0110],\n",
      "        [ 0.0068, -0.0410, -0.0437,  ..., -0.0477,  0.0241, -0.0074]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0532,  0.0214, -0.0699, -0.0411, -0.0051,  0.0069,  0.0328,  0.0398,\n",
      "        -0.0460, -0.0535], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0509, -0.0080, -0.0585,  ..., -0.0455,  0.0644, -0.0054],\n",
      "        [-0.0393, -0.0511,  0.0533,  ..., -0.0211, -0.0665, -0.0075],\n",
      "        [ 0.0538,  0.0431,  0.0194,  ..., -0.0169, -0.0601,  0.0394],\n",
      "        ...,\n",
      "        [ 0.0608,  0.0307,  0.0078,  ...,  0.0325, -0.0291, -0.0472],\n",
      "        [ 0.0501, -0.0580,  0.0692,  ...,  0.0468, -0.0637, -0.0110],\n",
      "        [ 0.0068, -0.0410, -0.0437,  ..., -0.0477,  0.0241, -0.0074]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0532,  0.0214, -0.0699, -0.0411, -0.0051,  0.0069,  0.0328,  0.0398,\n",
      "        -0.0460, -0.0535], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "\n",
    "\n",
    "class MySmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySmallModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.network1 = MySmallModel()\n",
    "        self.network2 = MySmallModel()\n",
    "        self.network3 = MySmallModel()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc_out = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = F.relu(self.network1(x1))\n",
    "        x2 = F.relu(self.network2(x2))\n",
    "        x3 = F.relu(self.network3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "N = 10\n",
    "x1, x2, x3 = torch.randn(N, 5), torch.randn(N, 5), torch.randn(N, 5)\n",
    "\n",
    "output = model(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterisation of Feature Size and Group Feature Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1208],\n",
      "        [0.1936],\n",
      "        [0.1864],\n",
      "        [0.2696],\n",
      "        [0.0661],\n",
      "        [0.2202],\n",
      "        [0.1334],\n",
      "        [0.1326],\n",
      "        [0.2051],\n",
      "        [0.1475]], grad_fn=<AddmmBackward0>)\n",
      "TorchModel(\n",
      "  (group_layers): ModuleList(\n",
      "    (0-25): 26 x GroupLayer(\n",
      "      (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
      "  (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GroupLayer(nn.Module):\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x \n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\"Total feature size must be divisible by group feature size\")\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0 \n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "            \n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size)\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(input_data.shape)\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "        \n",
    "        outs = []\n",
    "        for i,x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n",
    "N = 10\n",
    "x = torch.randn(N, total_feat_size)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimisation Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to generate some regression data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=10000, n_features=260, noise=0.1)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_dl = DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch 0\n",
      "Batch 0 loss: 43265.95334051589\n",
      "Batch 1 loss: 43769.70668765815\n",
      "Batch 2 loss: 51063.13610698151\n",
      "Batch 3 loss: 31887.078673868826\n",
      "Batch 4 loss: 27212.87292946202\n",
      "Batch 5 loss: 21852.101070121345\n",
      "Batch 6 loss: 39404.01740497344\n",
      "Batch 7 loss: 33454.19728695883\n",
      "Batch 8 loss: 29582.177416267456\n",
      "Batch 9 loss: 53554.742290062364\n",
      "Batch 10 loss: 27230.351765344778\n",
      "Batch 11 loss: 42616.03873473134\n",
      "Batch 12 loss: 23140.837882220338\n",
      "Batch 13 loss: 27290.00810295282\n",
      "Batch 14 loss: 46852.48649854868\n",
      "Batch 15 loss: 33112.50350914469\n",
      "Batch 16 loss: 28583.38737521213\n",
      "Batch 17 loss: 32850.4925280198\n",
      "Batch 18 loss: 36527.66924541115\n",
      "Batch 19 loss: 33230.5215310286\n",
      "Batch 20 loss: 41223.033694636484\n",
      "Batch 21 loss: 54027.38169258549\n",
      "Batch 22 loss: 38549.625888154056\n",
      "Batch 23 loss: 38897.8306342196\n",
      "Batch 24 loss: 36914.090484084416\n",
      "Batch 25 loss: 40052.6211180996\n",
      "Batch 26 loss: 39344.606266426286\n",
      "Batch 27 loss: 38337.501599303745\n",
      "Batch 28 loss: 38634.78469499729\n",
      "Batch 29 loss: 29934.59573211414\n",
      "Batch 30 loss: 39885.0338233439\n",
      "Batch 31 loss: 45298.00827684257\n",
      "Batch 32 loss: 35171.054821334445\n",
      "Batch 33 loss: 31442.218630653853\n",
      "Batch 34 loss: 39412.98205521723\n",
      "Batch 35 loss: 57678.829858123936\n",
      "Batch 36 loss: 35129.247025937875\n",
      "Batch 37 loss: 47776.775613572376\n",
      "Batch 38 loss: 24931.872710256546\n",
      "Batch 39 loss: 29396.398007797216\n",
      "Batch 40 loss: 38928.72246141473\n",
      "Batch 41 loss: 43201.35124883234\n",
      "Batch 42 loss: 41656.99224835873\n",
      "Batch 43 loss: 25831.197747938422\n",
      "Batch 44 loss: 43622.783752114425\n",
      "Batch 45 loss: 50076.88904445262\n",
      "Batch 46 loss: 31379.696091308644\n",
      "Batch 47 loss: 34403.2647007078\n",
      "Batch 48 loss: 36398.445483170086\n",
      "Batch 49 loss: 31807.975055097904\n",
      "Batch 50 loss: 51994.66232120457\n",
      "Batch 51 loss: 43709.341997522555\n",
      "Batch 52 loss: 62437.46319752621\n",
      "Batch 53 loss: 47270.72785604869\n",
      "Batch 54 loss: 29183.109206116897\n",
      "Batch 55 loss: 31824.659665735962\n",
      "Batch 56 loss: 46736.58178295912\n",
      "Batch 57 loss: 27096.406769964145\n",
      "Batch 58 loss: 46424.821816530166\n",
      "Batch 59 loss: 56438.557746984094\n",
      "Batch 60 loss: 45111.19968301404\n",
      "Batch 61 loss: 29363.336846582573\n",
      "Batch 62 loss: 37052.44720996295\n",
      "Batch 63 loss: 35966.516794502764\n",
      "Batch 64 loss: 32852.95028087151\n",
      "Batch 65 loss: 41057.27516142832\n",
      "Batch 66 loss: 42583.440201383266\n",
      "Batch 67 loss: 26565.815690062507\n",
      "Batch 68 loss: 55100.860290699165\n",
      "Batch 69 loss: 36882.32085267212\n",
      "Batch 70 loss: 37797.77276649678\n",
      "Batch 71 loss: 34183.97813042096\n",
      "Batch 72 loss: 37956.623837573745\n",
      "Batch 73 loss: 34487.18751910577\n",
      "Batch 74 loss: 29029.53038000498\n",
      "Batch 75 loss: 55718.176728212406\n",
      "Batch 76 loss: 41971.514830058244\n",
      "Batch 77 loss: 35578.7009802774\n",
      "Batch 78 loss: 26459.97510174693\n",
      "Batch 79 loss: 29692.23131578833\n",
      "Batch 80 loss: 47281.114062434404\n",
      "Batch 81 loss: 28704.529937097872\n",
      "Batch 82 loss: 34031.36028557717\n",
      "Batch 83 loss: 26692.875954337323\n",
      "Batch 84 loss: 34070.103905107\n",
      "Batch 85 loss: 33930.36652475186\n",
      "Batch 86 loss: 29828.4417494827\n",
      "Batch 87 loss: 26644.54997634876\n",
      "Batch 88 loss: 32895.46396116952\n",
      "Batch 89 loss: 52776.157410331034\n",
      "Batch 90 loss: 35169.02636024072\n",
      "Batch 91 loss: 24124.992131030813\n",
      "Batch 92 loss: 46173.36533008095\n",
      "Batch 93 loss: 40771.30047526735\n",
      "Batch 94 loss: 44272.2422744965\n",
      "Batch 95 loss: 49639.50428632525\n",
      "Batch 96 loss: 25575.3195803952\n",
      "Batch 97 loss: 39348.22311766523\n",
      "Batch 98 loss: 74414.06041860812\n",
      "Batch 99 loss: 28508.52631301182\n",
      "Batch 100 loss: 33032.09863677718\n",
      "Batch 101 loss: 36241.148217469316\n",
      "Batch 102 loss: 58940.81559163963\n",
      "Batch 103 loss: 43694.52410496971\n",
      "Batch 104 loss: 37456.40521668897\n",
      "Batch 105 loss: 37115.18818220115\n",
      "Batch 106 loss: 33304.08401708574\n",
      "Batch 107 loss: 33386.92000368402\n",
      "Batch 108 loss: 42545.92763178791\n",
      "Batch 109 loss: 36796.46032652473\n",
      "Batch 110 loss: 43404.265669534216\n",
      "Batch 111 loss: 39325.179335487024\n",
      "Batch 112 loss: 34151.59254755804\n",
      "Batch 113 loss: 33073.45540940756\n",
      "Batch 114 loss: 30283.136930890647\n",
      "Batch 115 loss: 24316.80186915477\n",
      "Batch 116 loss: 40370.154645853545\n",
      "Batch 117 loss: 31973.23013964436\n",
      "Batch 118 loss: 42791.574930099996\n",
      "Batch 119 loss: 48793.73181191441\n",
      "Batch 120 loss: 37164.23045154997\n",
      "Batch 121 loss: 31549.962973120324\n",
      "Batch 122 loss: 33829.82928781566\n",
      "Batch 123 loss: 61332.70925977604\n",
      "Batch 124 loss: 31061.441452813917\n",
      "Batch 125 loss: 44352.9824926318\n",
      "Batch 126 loss: 37046.31176694785\n",
      "Batch 127 loss: 31500.97543651513\n",
      "Batch 128 loss: 34764.26740960523\n",
      "Batch 129 loss: 42074.219793059\n",
      "Batch 130 loss: 27746.248859592248\n",
      "Batch 131 loss: 25933.30169269506\n",
      "Batch 132 loss: 40581.08912278386\n",
      "Batch 133 loss: 41984.12713874027\n",
      "Batch 134 loss: 43766.63930391154\n",
      "Batch 135 loss: 40072.00633472928\n",
      "Batch 136 loss: 31756.4247979114\n",
      "Batch 137 loss: 29436.542589581084\n",
      "Batch 138 loss: 26587.012191431786\n",
      "Batch 139 loss: 49486.240989718404\n",
      "Batch 140 loss: 28649.292783273337\n",
      "Batch 141 loss: 43493.92132274434\n",
      "Batch 142 loss: 36633.791127162614\n",
      "Batch 143 loss: 28171.98307714937\n",
      "Batch 144 loss: 45785.61202807227\n",
      "Batch 145 loss: 34192.60042652812\n",
      "Batch 146 loss: 19516.230504032163\n",
      "Batch 147 loss: 35581.23603476213\n",
      "Batch 148 loss: 35846.466256962885\n",
      "Batch 149 loss: 39931.86926038006\n",
      "Batch 150 loss: 37921.39197200841\n",
      "Batch 151 loss: 30176.46324982569\n",
      "Batch 152 loss: 37833.77857673979\n",
      "Batch 153 loss: 33947.4005386833\n",
      "Batch 154 loss: 26196.58493788709\n",
      "Batch 155 loss: 32317.772286378546\n",
      "Batch 156 loss: 28827.533009232124\n",
      "Batch 157 loss: 41943.91938139314\n",
      "Batch 158 loss: 29551.748153741748\n",
      "Batch 159 loss: 38341.544312772894\n",
      "Batch 160 loss: 62262.826766330996\n",
      "Batch 161 loss: 44335.80364616448\n",
      "Batch 162 loss: 29248.253540156154\n",
      "Batch 163 loss: 39619.271060299725\n",
      "Batch 164 loss: 26561.462767206318\n",
      "Batch 165 loss: 52187.68648184987\n",
      "Batch 166 loss: 30303.271530030455\n",
      "Batch 167 loss: 15531.009781003651\n",
      "Batch 168 loss: 50333.560209125135\n",
      "Batch 169 loss: 40460.41853426077\n",
      "Batch 170 loss: 42264.9260947537\n",
      "Batch 171 loss: 21606.742264855355\n",
      "Batch 172 loss: 33903.63259368996\n",
      "Batch 173 loss: 31005.59953145114\n",
      "Batch 174 loss: 43939.21586699053\n",
      "Batch 175 loss: 41034.13538486922\n",
      "Batch 176 loss: 36822.01475833957\n",
      "Batch 177 loss: 43936.94869009643\n",
      "Batch 178 loss: 31030.199589782096\n",
      "Batch 179 loss: 38387.15369807885\n",
      "Batch 180 loss: 35186.76550694827\n",
      "Batch 181 loss: 26630.616640547967\n",
      "Batch 182 loss: 55860.32742955999\n",
      "Batch 183 loss: 73036.83906281169\n",
      "Batch 184 loss: 37941.896991764326\n",
      "Batch 185 loss: 29366.05900424961\n",
      "Batch 186 loss: 31647.189172096667\n",
      "Batch 187 loss: 46755.70115046804\n",
      "Batch 188 loss: 55454.70136470009\n",
      "Batch 189 loss: 51483.20336025582\n",
      "Batch 190 loss: 44012.32055135595\n",
      "Batch 191 loss: 44726.25045271854\n",
      "Batch 192 loss: 33617.15250435774\n",
      "Batch 193 loss: 47683.3343412966\n",
      "Batch 194 loss: 40519.61784542926\n",
      "Batch 195 loss: 39891.694190889844\n",
      "Batch 196 loss: 42583.19720232839\n",
      "Batch 197 loss: 40642.6452583033\n",
      "Batch 198 loss: 33275.38095765815\n",
      "Batch 199 loss: 40490.87665104923\n",
      "Batch 200 loss: 47016.76269157571\n",
      "Batch 201 loss: 60405.182789140024\n",
      "Batch 202 loss: 52327.81930501149\n",
      "Batch 203 loss: 59944.32147510263\n",
      "Batch 204 loss: 31524.72653293086\n",
      "Batch 205 loss: 55221.93079703543\n",
      "Batch 206 loss: 24491.6272242496\n",
      "Batch 207 loss: 40964.87002502222\n",
      "Batch 208 loss: 35352.51390275739\n",
      "Batch 209 loss: 39845.94956731022\n",
      "Batch 210 loss: 32841.20017528348\n",
      "Batch 211 loss: 40818.70875606888\n",
      "Batch 212 loss: 28359.30578028609\n",
      "Batch 213 loss: 36727.67886529748\n",
      "Batch 214 loss: 28414.530206426654\n",
      "Batch 215 loss: 40376.9069007059\n",
      "Batch 216 loss: 23900.116829441467\n",
      "Batch 217 loss: 24932.740140852333\n",
      "Batch 218 loss: 37116.21613860405\n",
      "Batch 219 loss: 34858.82832884779\n",
      "Batch 220 loss: 52838.09713314922\n",
      "Batch 221 loss: 22324.640007117807\n",
      "Batch 222 loss: 42505.00798347805\n",
      "Batch 223 loss: 30103.6546886511\n",
      "Batch 224 loss: 41677.752738225376\n",
      "Batch 225 loss: 72730.04594239315\n",
      "Batch 226 loss: 27315.63287789697\n",
      "Batch 227 loss: 27282.166664120505\n",
      "Batch 228 loss: 37967.6964131363\n",
      "Batch 229 loss: 28028.038061858817\n",
      "Batch 230 loss: 29254.502609307034\n",
      "Batch 231 loss: 22949.446899127463\n",
      "Batch 232 loss: 41466.979320034654\n",
      "Batch 233 loss: 29392.734594611233\n",
      "Batch 234 loss: 43518.65376493037\n",
      "Batch 235 loss: 50731.79172374438\n",
      "Batch 236 loss: 45066.10853202225\n",
      "Batch 237 loss: 29143.396311641856\n",
      "Batch 238 loss: 44841.316130115934\n",
      "Batch 239 loss: 26438.57961645128\n",
      "Batch 240 loss: 42677.97906975623\n",
      "Batch 241 loss: 45908.71402552505\n",
      "Batch 242 loss: 38418.58192340621\n",
      "Batch 243 loss: 36756.75690177371\n",
      "Batch 244 loss: 38982.24504433912\n",
      "Batch 245 loss: 37156.163243475734\n",
      "Batch 246 loss: 40587.6695494022\n",
      "Batch 247 loss: 47534.37326325372\n",
      "Batch 248 loss: 37434.00557805463\n",
      "Batch 249 loss: 30186.75512987439\n",
      "### Epoch 1\n",
      "Batch 0 loss: 49784.81093462845\n",
      "Batch 1 loss: 39675.064380279946\n",
      "Batch 2 loss: 35476.3733171723\n",
      "Batch 3 loss: 38291.543757824424\n",
      "Batch 4 loss: 55342.792699630816\n",
      "Batch 5 loss: 32575.77679090795\n",
      "Batch 6 loss: 28181.081952703687\n",
      "Batch 7 loss: 53713.98855474875\n",
      "Batch 8 loss: 40273.8185695606\n",
      "Batch 9 loss: 27953.972911807406\n",
      "Batch 10 loss: 22212.759806124366\n",
      "Batch 11 loss: 37421.986282114376\n",
      "Batch 12 loss: 43617.59843511848\n",
      "Batch 13 loss: 28328.920004393294\n",
      "Batch 14 loss: 46231.69644457777\n",
      "Batch 15 loss: 28093.56681833848\n",
      "Batch 16 loss: 38593.87557929492\n",
      "Batch 17 loss: 31234.918148211254\n",
      "Batch 18 loss: 28023.22743390176\n",
      "Batch 19 loss: 39793.68996316647\n",
      "Batch 20 loss: 38847.12435515797\n",
      "Batch 21 loss: 43238.79868690063\n",
      "Batch 22 loss: 32811.66894066767\n",
      "Batch 23 loss: 23769.7638385625\n",
      "Batch 24 loss: 26657.90686134337\n",
      "Batch 25 loss: 41182.13657085577\n",
      "Batch 26 loss: 26153.048817890878\n",
      "Batch 27 loss: 33728.360275466985\n",
      "Batch 28 loss: 38020.81226860249\n",
      "Batch 29 loss: 40829.463030265986\n",
      "Batch 30 loss: 29133.136807461236\n",
      "Batch 31 loss: 35008.095684011234\n",
      "Batch 32 loss: 33456.57222929539\n",
      "Batch 33 loss: 28287.73473644045\n",
      "Batch 34 loss: 42580.72304061652\n",
      "Batch 35 loss: 25618.560126839835\n",
      "Batch 36 loss: 16700.7960076147\n",
      "Batch 37 loss: 31376.923475071773\n",
      "Batch 38 loss: 36785.78754624433\n",
      "Batch 39 loss: 40129.50676083333\n",
      "Batch 40 loss: 39256.262108064126\n",
      "Batch 41 loss: 50246.80045753012\n",
      "Batch 42 loss: 35891.04030816246\n",
      "Batch 43 loss: 39255.889507910964\n",
      "Batch 44 loss: 35659.295856968136\n",
      "Batch 45 loss: 39127.530204993396\n",
      "Batch 46 loss: 56140.96404157328\n",
      "Batch 47 loss: 33835.21462505702\n",
      "Batch 48 loss: 47682.70719354123\n",
      "Batch 49 loss: 49286.071154221296\n",
      "Batch 50 loss: 51130.754655586534\n",
      "Batch 51 loss: 50065.92946997187\n",
      "Batch 52 loss: 42093.4768036555\n",
      "Batch 53 loss: 34747.030241712804\n",
      "Batch 54 loss: 46111.64352508557\n",
      "Batch 55 loss: 43906.90514370574\n",
      "Batch 56 loss: 41208.389166756046\n",
      "Batch 57 loss: 23238.10207255218\n",
      "Batch 58 loss: 27336.579458727676\n",
      "Batch 59 loss: 35188.25878977444\n",
      "Batch 60 loss: 33584.63146027396\n",
      "Batch 61 loss: 33048.16742982653\n",
      "Batch 62 loss: 39500.22336711702\n",
      "Batch 63 loss: 38648.74845912719\n",
      "Batch 64 loss: 34918.02975745848\n",
      "Batch 65 loss: 25209.97160924309\n",
      "Batch 66 loss: 40544.53689973836\n",
      "Batch 67 loss: 33224.0456511418\n",
      "Batch 68 loss: 31120.32077278108\n",
      "Batch 69 loss: 46551.16685495703\n",
      "Batch 70 loss: 49663.73985302515\n",
      "Batch 71 loss: 31217.31973280704\n",
      "Batch 72 loss: 43171.39234260142\n",
      "Batch 73 loss: 46090.69109234753\n",
      "Batch 74 loss: 23346.112383284693\n",
      "Batch 75 loss: 59869.70610280339\n",
      "Batch 76 loss: 37380.17632017074\n",
      "Batch 77 loss: 48385.097963513625\n",
      "Batch 78 loss: 28689.498392819627\n",
      "Batch 79 loss: 33989.74498422818\n",
      "Batch 80 loss: 36876.6899381284\n",
      "Batch 81 loss: 25071.19207969239\n",
      "Batch 82 loss: 62583.11018899941\n",
      "Batch 83 loss: 38546.94679888576\n",
      "Batch 84 loss: 43475.522603171485\n",
      "Batch 85 loss: 31097.379115251744\n",
      "Batch 86 loss: 39465.11483422342\n",
      "Batch 87 loss: 37956.41608119369\n",
      "Batch 88 loss: 44695.553912014264\n",
      "Batch 89 loss: 43868.348869945636\n",
      "Batch 90 loss: 33836.03655375556\n",
      "Batch 91 loss: 32353.886728862068\n",
      "Batch 92 loss: 41955.50971198143\n",
      "Batch 93 loss: 27982.354780040823\n",
      "Batch 94 loss: 34885.057592638055\n",
      "Batch 95 loss: 38499.07947348991\n",
      "Batch 96 loss: 50501.145440292734\n",
      "Batch 97 loss: 30407.800806607116\n",
      "Batch 98 loss: 45847.309607801435\n",
      "Batch 99 loss: 34405.93143221022\n",
      "Batch 100 loss: 35002.09813588485\n",
      "Batch 101 loss: 39029.098427334065\n",
      "Batch 102 loss: 34890.71239024025\n",
      "Batch 103 loss: 41614.08464096283\n",
      "Batch 104 loss: 51183.64335039065\n",
      "Batch 105 loss: 43439.54218496004\n",
      "Batch 106 loss: 48484.376391161735\n",
      "Batch 107 loss: 29299.78329143246\n",
      "Batch 108 loss: 54647.418056056566\n",
      "Batch 109 loss: 42556.724876698005\n",
      "Batch 110 loss: 45373.82772351669\n",
      "Batch 111 loss: 26736.056591591427\n",
      "Batch 112 loss: 53170.67832681435\n",
      "Batch 113 loss: 39076.75852194489\n",
      "Batch 114 loss: 44614.55783965385\n",
      "Batch 115 loss: 28182.231015392375\n",
      "Batch 116 loss: 43336.82946284154\n",
      "Batch 117 loss: 30121.961356153035\n",
      "Batch 118 loss: 49091.23418266066\n",
      "Batch 119 loss: 33146.58327527438\n",
      "Batch 120 loss: 60272.54495847792\n",
      "Batch 121 loss: 28352.46177117739\n",
      "Batch 122 loss: 35287.2623008317\n",
      "Batch 123 loss: 29423.449332825418\n",
      "Batch 124 loss: 45411.33550074282\n",
      "Batch 125 loss: 30561.576906582188\n",
      "Batch 126 loss: 40315.69578871537\n",
      "Batch 127 loss: 38008.03597803378\n",
      "Batch 128 loss: 49421.40402467237\n",
      "Batch 129 loss: 31501.668748881923\n",
      "Batch 130 loss: 42418.60075260651\n",
      "Batch 131 loss: 51050.041019098106\n",
      "Batch 132 loss: 35458.31726648495\n",
      "Batch 133 loss: 45005.73250700085\n",
      "Batch 134 loss: 33603.08172295219\n",
      "Batch 135 loss: 30609.715452537064\n",
      "Batch 136 loss: 43244.71135814975\n",
      "Batch 137 loss: 44010.974647690135\n",
      "Batch 138 loss: 48087.429168118026\n",
      "Batch 139 loss: 40235.111245883316\n",
      "Batch 140 loss: 28830.883753119528\n",
      "Batch 141 loss: 20046.112459547305\n",
      "Batch 142 loss: 28729.95390390726\n",
      "Batch 143 loss: 30632.759897148186\n",
      "Batch 144 loss: 35840.85726907653\n",
      "Batch 145 loss: 40787.418728604665\n",
      "Batch 146 loss: 44635.32480511409\n",
      "Batch 147 loss: 22861.661717365732\n",
      "Batch 148 loss: 20548.24266589153\n",
      "Batch 149 loss: 36027.795151844766\n",
      "Batch 150 loss: 38031.71478755485\n",
      "Batch 151 loss: 36515.954359942174\n",
      "Batch 152 loss: 36778.37692644051\n",
      "Batch 153 loss: 38084.241944480156\n",
      "Batch 154 loss: 22165.73636569334\n",
      "Batch 155 loss: 27936.65992833209\n",
      "Batch 156 loss: 36462.60743168843\n",
      "Batch 157 loss: 32029.655313354462\n",
      "Batch 158 loss: 47052.30305263055\n",
      "Batch 159 loss: 28580.496777329834\n",
      "Batch 160 loss: 43886.09518778539\n",
      "Batch 161 loss: 28466.44962158485\n",
      "Batch 162 loss: 33370.31794496534\n",
      "Batch 163 loss: 45692.76621991273\n",
      "Batch 164 loss: 34246.028013198986\n",
      "Batch 165 loss: 48311.530080394834\n",
      "Batch 166 loss: 45780.02544462354\n",
      "Batch 167 loss: 36762.432719814504\n",
      "Batch 168 loss: 25535.15737781111\n",
      "Batch 169 loss: 54910.57435625988\n",
      "Batch 170 loss: 25994.895222109684\n",
      "Batch 171 loss: 36501.777751690715\n",
      "Batch 172 loss: 44269.44106716695\n",
      "Batch 173 loss: 50728.65190076806\n",
      "Batch 174 loss: 43628.35277017667\n",
      "Batch 175 loss: 47279.70704335253\n",
      "Batch 176 loss: 42521.534962500824\n",
      "Batch 177 loss: 35868.53690616878\n",
      "Batch 178 loss: 24796.730493328454\n",
      "Batch 179 loss: 42706.13039268847\n",
      "Batch 180 loss: 30119.51698962486\n",
      "Batch 181 loss: 20428.188216628332\n",
      "Batch 182 loss: 33509.86790971091\n",
      "Batch 183 loss: 26587.211231892303\n",
      "Batch 184 loss: 25627.741012189712\n",
      "Batch 185 loss: 45820.334821727796\n",
      "Batch 186 loss: 27452.839231503942\n",
      "Batch 187 loss: 35420.34673894509\n",
      "Batch 188 loss: 36205.16332120643\n",
      "Batch 189 loss: 39319.71292868853\n",
      "Batch 190 loss: 26536.882973198262\n",
      "Batch 191 loss: 35205.91051097184\n",
      "Batch 192 loss: 48643.371671877954\n",
      "Batch 193 loss: 41459.6132942461\n",
      "Batch 194 loss: 45542.735053586686\n",
      "Batch 195 loss: 40122.31420158147\n",
      "Batch 196 loss: 41628.47162739948\n",
      "Batch 197 loss: 37997.61111790188\n",
      "Batch 198 loss: 34320.31912573563\n",
      "Batch 199 loss: 39090.070812906735\n",
      "Batch 200 loss: 32598.519037541555\n",
      "Batch 201 loss: 47666.86317761202\n",
      "Batch 202 loss: 36729.527012562496\n",
      "Batch 203 loss: 34592.70175860356\n",
      "Batch 204 loss: 48048.041797570055\n",
      "Batch 205 loss: 30888.558970250495\n",
      "Batch 206 loss: 40272.800642053575\n",
      "Batch 207 loss: 27326.95339530165\n",
      "Batch 208 loss: 29866.299386611656\n",
      "Batch 209 loss: 39049.840790446375\n",
      "Batch 210 loss: 47868.61783745541\n",
      "Batch 211 loss: 51681.130532383846\n",
      "Batch 212 loss: 59761.758116437166\n",
      "Batch 213 loss: 27868.972789290678\n",
      "Batch 214 loss: 32443.42298112261\n",
      "Batch 215 loss: 49264.97401076133\n",
      "Batch 216 loss: 42080.812000368\n",
      "Batch 217 loss: 51247.633481609424\n",
      "Batch 218 loss: 28347.951714108338\n",
      "Batch 219 loss: 54257.84331776606\n",
      "Batch 220 loss: 45953.42122528622\n",
      "Batch 221 loss: 24592.89149424589\n",
      "Batch 222 loss: 47278.37799306121\n",
      "Batch 223 loss: 28114.66838635309\n",
      "Batch 224 loss: 34501.77005608638\n",
      "Batch 225 loss: 36311.61842549022\n",
      "Batch 226 loss: 29093.026866840315\n",
      "Batch 227 loss: 42836.06490784835\n",
      "Batch 228 loss: 34957.56397057337\n",
      "Batch 229 loss: 28571.170873999075\n",
      "Batch 230 loss: 51849.18533576299\n",
      "Batch 231 loss: 33542.829420916794\n",
      "Batch 232 loss: 49598.35676226628\n",
      "Batch 233 loss: 55508.15367604824\n",
      "Batch 234 loss: 51817.849192590416\n",
      "Batch 235 loss: 51538.56302027744\n",
      "Batch 236 loss: 40604.153704287535\n",
      "Batch 237 loss: 31576.235632271968\n",
      "Batch 238 loss: 48054.858247020624\n",
      "Batch 239 loss: 27620.39661474648\n",
      "Batch 240 loss: 46915.9285514312\n",
      "Batch 241 loss: 39205.71008547314\n",
      "Batch 242 loss: 34314.495740596896\n",
      "Batch 243 loss: 44556.848842294545\n",
      "Batch 244 loss: 40065.756781581076\n",
      "Batch 245 loss: 32142.64815878197\n",
      "Batch 246 loss: 33057.53545449822\n",
      "Batch 247 loss: 27802.782530822722\n",
      "Batch 248 loss: 28076.730630782797\n",
      "Batch 249 loss: 54015.36103479955\n",
      "### Epoch 2\n",
      "Batch 0 loss: 52387.55810196414\n",
      "Batch 1 loss: 45956.429448151706\n",
      "Batch 2 loss: 34456.429048836784\n",
      "Batch 3 loss: 25603.753454425696\n",
      "Batch 4 loss: 25422.312172535363\n",
      "Batch 5 loss: 39796.24417437416\n",
      "Batch 6 loss: 27267.73935045953\n",
      "Batch 7 loss: 71814.94432292464\n",
      "Batch 8 loss: 32514.010962145556\n",
      "Batch 9 loss: 41390.97808777684\n",
      "Batch 10 loss: 34015.69498643015\n",
      "Batch 11 loss: 43986.191296947596\n",
      "Batch 12 loss: 45376.94766753943\n",
      "Batch 13 loss: 41785.68740708554\n",
      "Batch 14 loss: 40652.53442921711\n",
      "Batch 15 loss: 27651.62611158068\n",
      "Batch 16 loss: 34191.4398351495\n",
      "Batch 17 loss: 41686.42947731056\n",
      "Batch 18 loss: 63793.38250129977\n",
      "Batch 19 loss: 39498.04621635255\n",
      "Batch 20 loss: 35506.47980065976\n",
      "Batch 21 loss: 26676.97147632105\n",
      "Batch 22 loss: 42240.820860245534\n",
      "Batch 23 loss: 13469.744363691134\n",
      "Batch 24 loss: 34509.77840451793\n",
      "Batch 25 loss: 34987.23667294425\n",
      "Batch 26 loss: 32854.30643481126\n",
      "Batch 27 loss: 29627.26919860114\n",
      "Batch 28 loss: 30944.113888397427\n",
      "Batch 29 loss: 37522.88138349673\n",
      "Batch 30 loss: 33094.47981973607\n",
      "Batch 31 loss: 20361.839492279953\n",
      "Batch 32 loss: 24601.04862481285\n",
      "Batch 33 loss: 39129.71473951169\n",
      "Batch 34 loss: 70123.01711903632\n",
      "Batch 35 loss: 52117.11452787844\n",
      "Batch 36 loss: 29631.54499568696\n",
      "Batch 37 loss: 41040.90030054811\n",
      "Batch 38 loss: 43050.046315901505\n",
      "Batch 39 loss: 24851.733466586018\n",
      "Batch 40 loss: 53983.31235518203\n",
      "Batch 41 loss: 25692.777897484095\n",
      "Batch 42 loss: 24738.563971858246\n",
      "Batch 43 loss: 45905.5927890897\n",
      "Batch 44 loss: 41152.22964446028\n",
      "Batch 45 loss: 38150.617547339876\n",
      "Batch 46 loss: 56289.15117681713\n",
      "Batch 47 loss: 28640.686758513133\n",
      "Batch 48 loss: 47611.05409205828\n",
      "Batch 49 loss: 45313.37839037037\n",
      "Batch 50 loss: 34338.13201043246\n",
      "Batch 51 loss: 50050.09799502302\n",
      "Batch 52 loss: 29348.79849439243\n",
      "Batch 53 loss: 53863.122007223195\n",
      "Batch 54 loss: 41008.02757749701\n",
      "Batch 55 loss: 30412.704692531144\n",
      "Batch 56 loss: 36320.62639179163\n",
      "Batch 57 loss: 20359.247304069402\n",
      "Batch 58 loss: 43478.42269451867\n",
      "Batch 59 loss: 41307.692262446195\n",
      "Batch 60 loss: 32987.45885076659\n",
      "Batch 61 loss: 30261.217598092524\n",
      "Batch 62 loss: 63505.168530662166\n",
      "Batch 63 loss: 35198.75713892026\n",
      "Batch 64 loss: 46314.871202199676\n",
      "Batch 65 loss: 31151.350602929466\n",
      "Batch 66 loss: 28296.686009889847\n",
      "Batch 67 loss: 42871.893519633515\n",
      "Batch 68 loss: 37635.50629471714\n",
      "Batch 69 loss: 48347.76186964867\n",
      "Batch 70 loss: 54488.35014425337\n",
      "Batch 71 loss: 23563.314813279914\n",
      "Batch 72 loss: 54595.03318862058\n",
      "Batch 73 loss: 31320.5131376575\n",
      "Batch 74 loss: 26934.813165956766\n",
      "Batch 75 loss: 45892.22210907021\n",
      "Batch 76 loss: 41194.3766937483\n",
      "Batch 77 loss: 42140.54599208779\n",
      "Batch 78 loss: 53195.100448522215\n",
      "Batch 79 loss: 24380.66816721531\n",
      "Batch 80 loss: 51879.297341588965\n",
      "Batch 81 loss: 23975.66793387706\n",
      "Batch 82 loss: 33980.37471606546\n",
      "Batch 83 loss: 43952.02331253239\n",
      "Batch 84 loss: 32052.803854328286\n",
      "Batch 85 loss: 33811.16819837932\n",
      "Batch 86 loss: 24532.209980258416\n",
      "Batch 87 loss: 28270.54422185883\n",
      "Batch 88 loss: 43876.54998295474\n",
      "Batch 89 loss: 36296.74146071082\n",
      "Batch 90 loss: 70208.97029648474\n",
      "Batch 91 loss: 39424.430379971425\n",
      "Batch 92 loss: 47676.9762124657\n",
      "Batch 93 loss: 25663.183434282477\n",
      "Batch 94 loss: 44711.28165909203\n",
      "Batch 95 loss: 40343.752887303\n",
      "Batch 96 loss: 30648.286907034304\n",
      "Batch 97 loss: 31666.836527386215\n",
      "Batch 98 loss: 43259.32095321281\n",
      "Batch 99 loss: 30656.690434635784\n",
      "Batch 100 loss: 34410.83639584143\n",
      "Batch 101 loss: 39667.16396337235\n",
      "Batch 102 loss: 49278.5189981867\n",
      "Batch 103 loss: 35263.559906552764\n",
      "Batch 104 loss: 43184.87470671192\n",
      "Batch 105 loss: 34752.66044579986\n",
      "Batch 106 loss: 30939.073889639578\n",
      "Batch 107 loss: 35270.41054713946\n",
      "Batch 108 loss: 36917.707052690195\n",
      "Batch 109 loss: 44936.452312642396\n",
      "Batch 110 loss: 23410.79166193478\n",
      "Batch 111 loss: 42424.63736082796\n",
      "Batch 112 loss: 47378.50492192726\n",
      "Batch 113 loss: 39860.373656402226\n",
      "Batch 114 loss: 29790.789353074608\n",
      "Batch 115 loss: 40774.49472338607\n",
      "Batch 116 loss: 30108.19347804659\n",
      "Batch 117 loss: 52722.586348109166\n",
      "Batch 118 loss: 26744.83755151923\n",
      "Batch 119 loss: 47277.37370266966\n",
      "Batch 120 loss: 31502.295604230116\n",
      "Batch 121 loss: 34804.7994080959\n",
      "Batch 122 loss: 33837.72125938257\n",
      "Batch 123 loss: 36634.933990924146\n",
      "Batch 124 loss: 33431.829370087886\n",
      "Batch 125 loss: 25033.413619954285\n",
      "Batch 126 loss: 47350.68500480928\n",
      "Batch 127 loss: 59485.64088649131\n",
      "Batch 128 loss: 39790.26698012368\n",
      "Batch 129 loss: 24824.30361074621\n",
      "Batch 130 loss: 30120.93938058306\n",
      "Batch 131 loss: 31541.552894760443\n",
      "Batch 132 loss: 43807.63440135561\n",
      "Batch 133 loss: 45176.90558269572\n",
      "Batch 134 loss: 39414.09322089591\n",
      "Batch 135 loss: 39795.338966403244\n",
      "Batch 136 loss: 31222.67187066403\n",
      "Batch 137 loss: 34776.23753161318\n",
      "Batch 138 loss: 31072.08088612133\n",
      "Batch 139 loss: 44008.2205700559\n",
      "Batch 140 loss: 32457.756480793607\n",
      "Batch 141 loss: 38714.880046866776\n",
      "Batch 142 loss: 32760.543594293893\n",
      "Batch 143 loss: 37290.90255502032\n",
      "Batch 144 loss: 46530.37353378265\n",
      "Batch 145 loss: 22758.313128376045\n",
      "Batch 146 loss: 33139.87419568356\n",
      "Batch 147 loss: 38852.83681977374\n",
      "Batch 148 loss: 38355.427532389454\n",
      "Batch 149 loss: 27150.88813432243\n",
      "Batch 150 loss: 60978.65381938711\n",
      "Batch 151 loss: 32780.8403218709\n",
      "Batch 152 loss: 20049.655807523664\n",
      "Batch 153 loss: 33502.376222623185\n",
      "Batch 154 loss: 36728.75706649218\n",
      "Batch 155 loss: 45840.582189188324\n",
      "Batch 156 loss: 23200.605149186256\n",
      "Batch 157 loss: 39571.928908421345\n",
      "Batch 158 loss: 42603.42575880223\n",
      "Batch 159 loss: 24750.38483423013\n",
      "Batch 160 loss: 41066.898608571\n",
      "Batch 161 loss: 20968.360225321452\n",
      "Batch 162 loss: 55425.75758267054\n",
      "Batch 163 loss: 33640.631127228975\n",
      "Batch 164 loss: 32353.254041427943\n",
      "Batch 165 loss: 47848.10779683622\n",
      "Batch 166 loss: 42800.121404998594\n",
      "Batch 167 loss: 32703.30807003892\n",
      "Batch 168 loss: 32919.61639693617\n",
      "Batch 169 loss: 34267.79478382754\n",
      "Batch 170 loss: 21615.778883301948\n",
      "Batch 171 loss: 27932.513409203857\n",
      "Batch 172 loss: 27393.811252945285\n",
      "Batch 173 loss: 32958.25299480291\n",
      "Batch 174 loss: 35662.55684575076\n",
      "Batch 175 loss: 49696.99764446248\n",
      "Batch 176 loss: 57123.39064384898\n",
      "Batch 177 loss: 41017.163141074365\n",
      "Batch 178 loss: 38161.87463771209\n",
      "Batch 179 loss: 40326.04287939209\n",
      "Batch 180 loss: 26331.232417582134\n",
      "Batch 181 loss: 24911.712410314936\n",
      "Batch 182 loss: 25013.410833771308\n",
      "Batch 183 loss: 37472.22298602165\n",
      "Batch 184 loss: 48979.98638762291\n",
      "Batch 185 loss: 35987.95046342182\n",
      "Batch 186 loss: 58156.710033355535\n",
      "Batch 187 loss: 28080.09911340716\n",
      "Batch 188 loss: 45937.00453848857\n",
      "Batch 189 loss: 35785.12847473593\n",
      "Batch 190 loss: 34092.55462185593\n",
      "Batch 191 loss: 37674.96662039394\n",
      "Batch 192 loss: 39403.80819077522\n",
      "Batch 193 loss: 39415.7747050232\n",
      "Batch 194 loss: 23379.905306142136\n",
      "Batch 195 loss: 39105.09378478509\n",
      "Batch 196 loss: 39257.302985121845\n",
      "Batch 197 loss: 42107.346053129964\n",
      "Batch 198 loss: 47872.266449769886\n",
      "Batch 199 loss: 30023.228845150177\n",
      "Batch 200 loss: 32502.20447442826\n",
      "Batch 201 loss: 53375.85397159284\n",
      "Batch 202 loss: 36696.22220292165\n",
      "Batch 203 loss: 44186.59812822632\n",
      "Batch 204 loss: 33979.130160702356\n",
      "Batch 205 loss: 46869.4130962327\n",
      "Batch 206 loss: 40723.65449410364\n",
      "Batch 207 loss: 51023.523651996686\n",
      "Batch 208 loss: 41096.92371972648\n",
      "Batch 209 loss: 40842.73304417985\n",
      "Batch 210 loss: 52460.00689085271\n",
      "Batch 211 loss: 47478.12196804745\n",
      "Batch 212 loss: 33709.698032253094\n",
      "Batch 213 loss: 52729.927037833535\n",
      "Batch 214 loss: 30881.920165154035\n",
      "Batch 215 loss: 24744.972065996724\n",
      "Batch 216 loss: 33026.629532100465\n",
      "Batch 217 loss: 30790.325075498236\n",
      "Batch 218 loss: 46621.77573471582\n",
      "Batch 219 loss: 40043.5893968211\n",
      "Batch 220 loss: 49420.87572515354\n",
      "Batch 221 loss: 47163.83072161611\n",
      "Batch 222 loss: 38091.96553992969\n",
      "Batch 223 loss: 37652.968795592686\n",
      "Batch 224 loss: 42355.011834673685\n",
      "Batch 225 loss: 45730.08493564289\n",
      "Batch 226 loss: 35285.64611928288\n",
      "Batch 227 loss: 40574.17239084585\n",
      "Batch 228 loss: 46520.75618722216\n",
      "Batch 229 loss: 54213.266532947004\n",
      "Batch 230 loss: 46108.48163722759\n",
      "Batch 231 loss: 48624.49283197292\n",
      "Batch 232 loss: 23640.019556677496\n",
      "Batch 233 loss: 54666.05319164838\n",
      "Batch 234 loss: 34590.74053558351\n",
      "Batch 235 loss: 33191.28938110737\n",
      "Batch 236 loss: 30447.757188442753\n",
      "Batch 237 loss: 26259.462114679503\n",
      "Batch 238 loss: 23178.585868227485\n",
      "Batch 239 loss: 49168.08141179808\n",
      "Batch 240 loss: 34412.62323871918\n",
      "Batch 241 loss: 29560.990723225834\n",
      "Batch 242 loss: 46467.08428701473\n",
      "Batch 243 loss: 34153.42431461498\n",
      "Batch 244 loss: 60617.436834708875\n",
      "Batch 245 loss: 39450.024019336466\n",
      "Batch 246 loss: 33150.447075607095\n",
      "Batch 247 loss: 24521.720533556716\n",
      "Batch 248 loss: 36898.781678020256\n",
      "Batch 249 loss: 18609.93574553736\n",
      "### Epoch 3\n",
      "Batch 0 loss: 33910.1645465179\n",
      "Batch 1 loss: 60040.435930006366\n",
      "Batch 2 loss: 47082.72155103558\n",
      "Batch 3 loss: 25208.069011119267\n",
      "Batch 4 loss: 30752.68348865437\n",
      "Batch 5 loss: 28553.260756334017\n",
      "Batch 6 loss: 46529.00454233854\n",
      "Batch 7 loss: 40275.003440716086\n",
      "Batch 8 loss: 28723.291413989566\n",
      "Batch 9 loss: 39514.38602273977\n",
      "Batch 10 loss: 33766.51890774693\n",
      "Batch 11 loss: 29981.620709988718\n",
      "Batch 12 loss: 43767.65647426038\n",
      "Batch 13 loss: 34797.74680660563\n",
      "Batch 14 loss: 58104.129410752\n",
      "Batch 15 loss: 33880.794676634265\n",
      "Batch 16 loss: 35527.07136943676\n",
      "Batch 17 loss: 35913.199243591895\n",
      "Batch 18 loss: 17569.291802183667\n",
      "Batch 19 loss: 34310.708286494286\n",
      "Batch 20 loss: 49394.21456298504\n",
      "Batch 21 loss: 54349.37612145235\n",
      "Batch 22 loss: 36965.65249555769\n",
      "Batch 23 loss: 26683.586924730807\n",
      "Batch 24 loss: 40294.0420228929\n",
      "Batch 25 loss: 24180.323762220847\n",
      "Batch 26 loss: 39845.36898390584\n",
      "Batch 27 loss: 41666.791631092805\n",
      "Batch 28 loss: 39091.88155417898\n",
      "Batch 29 loss: 53127.20928546544\n",
      "Batch 30 loss: 23252.945244575076\n",
      "Batch 31 loss: 25740.973896266965\n",
      "Batch 32 loss: 37473.872844180754\n",
      "Batch 33 loss: 30664.989451344067\n",
      "Batch 34 loss: 36198.143233749506\n",
      "Batch 35 loss: 43460.45508591687\n",
      "Batch 36 loss: 56023.58354385838\n",
      "Batch 37 loss: 40938.06420999469\n",
      "Batch 38 loss: 41103.68782974301\n",
      "Batch 39 loss: 42031.16312438094\n",
      "Batch 40 loss: 25044.986999610956\n",
      "Batch 41 loss: 26318.86379009571\n",
      "Batch 42 loss: 36819.5879544988\n",
      "Batch 43 loss: 48389.734816712895\n",
      "Batch 44 loss: 42438.29590533101\n",
      "Batch 45 loss: 35465.72130143671\n",
      "Batch 46 loss: 50948.56284387086\n",
      "Batch 47 loss: 45415.3525987999\n",
      "Batch 48 loss: 40515.323148172916\n",
      "Batch 49 loss: 39728.94052516699\n",
      "Batch 50 loss: 16912.116026538883\n",
      "Batch 51 loss: 35068.236080784875\n",
      "Batch 52 loss: 21506.0861667573\n",
      "Batch 53 loss: 65674.498359701\n",
      "Batch 54 loss: 30281.92041567679\n",
      "Batch 55 loss: 26658.29758582113\n",
      "Batch 56 loss: 27326.414223744523\n",
      "Batch 57 loss: 34687.49401033345\n",
      "Batch 58 loss: 32153.348065941296\n",
      "Batch 59 loss: 31606.431551791662\n",
      "Batch 60 loss: 44884.51497116686\n",
      "Batch 61 loss: 58419.606290463315\n",
      "Batch 62 loss: 40567.177451126365\n",
      "Batch 63 loss: 28894.39503152938\n",
      "Batch 64 loss: 37809.61557348994\n",
      "Batch 65 loss: 36349.861748963114\n",
      "Batch 66 loss: 58322.89285217396\n",
      "Batch 67 loss: 36834.048276035355\n",
      "Batch 68 loss: 34546.89196774399\n",
      "Batch 69 loss: 49976.191054558265\n",
      "Batch 70 loss: 35122.817023928415\n",
      "Batch 71 loss: 32832.68261609685\n",
      "Batch 72 loss: 37494.0985740356\n",
      "Batch 73 loss: 31047.00887055457\n",
      "Batch 74 loss: 45298.8435009826\n",
      "Batch 75 loss: 35525.14506389325\n",
      "Batch 76 loss: 44493.22281887659\n",
      "Batch 77 loss: 43652.33103226798\n",
      "Batch 78 loss: 47861.31922193329\n",
      "Batch 79 loss: 57190.89771235206\n",
      "Batch 80 loss: 52800.849621875546\n",
      "Batch 81 loss: 30859.7263095086\n",
      "Batch 82 loss: 36720.777189116416\n",
      "Batch 83 loss: 40115.1518237806\n",
      "Batch 84 loss: 38833.58057021677\n",
      "Batch 85 loss: 30649.65905857379\n",
      "Batch 86 loss: 21015.983621405212\n",
      "Batch 87 loss: 38993.63682743452\n",
      "Batch 88 loss: 31076.1174885455\n",
      "Batch 89 loss: 37844.96749855472\n",
      "Batch 90 loss: 42339.899870167035\n",
      "Batch 91 loss: 45230.097610287354\n",
      "Batch 92 loss: 51103.82011354591\n",
      "Batch 93 loss: 44328.19051616149\n",
      "Batch 94 loss: 29406.236653270767\n",
      "Batch 95 loss: 27768.70942418048\n",
      "Batch 96 loss: 36659.7495476991\n",
      "Batch 97 loss: 35735.548646512216\n",
      "Batch 98 loss: 50167.121047351306\n",
      "Batch 99 loss: 35493.75103543856\n",
      "Batch 100 loss: 23778.053682415135\n",
      "Batch 101 loss: 38867.85876596642\n",
      "Batch 102 loss: 42607.67889193444\n",
      "Batch 103 loss: 30435.255376687142\n",
      "Batch 104 loss: 25372.31807119107\n",
      "Batch 105 loss: 27186.38976664016\n",
      "Batch 106 loss: 41502.89020637042\n",
      "Batch 107 loss: 40755.839574917045\n",
      "Batch 108 loss: 54106.50552069952\n",
      "Batch 109 loss: 36743.123301439846\n",
      "Batch 110 loss: 37309.884079279276\n",
      "Batch 111 loss: 33957.81273239239\n",
      "Batch 112 loss: 35153.513019725986\n",
      "Batch 113 loss: 44616.4146948431\n",
      "Batch 114 loss: 36764.847273260755\n",
      "Batch 115 loss: 32933.0289459064\n",
      "Batch 116 loss: 42891.63740203307\n",
      "Batch 117 loss: 52482.70610140964\n",
      "Batch 118 loss: 37102.571932233564\n",
      "Batch 119 loss: 28176.0209625378\n",
      "Batch 120 loss: 66664.82664635853\n",
      "Batch 121 loss: 50057.30297116485\n",
      "Batch 122 loss: 33470.48342598041\n",
      "Batch 123 loss: 54697.03771141067\n",
      "Batch 124 loss: 30005.576099659396\n",
      "Batch 125 loss: 46364.0711603759\n",
      "Batch 126 loss: 22486.180190637835\n",
      "Batch 127 loss: 38523.45331544333\n",
      "Batch 128 loss: 41957.28224172289\n",
      "Batch 129 loss: 32936.04911754012\n",
      "Batch 130 loss: 27872.10122131665\n",
      "Batch 131 loss: 24863.15311934475\n",
      "Batch 132 loss: 50440.647775348385\n",
      "Batch 133 loss: 34193.71425184026\n",
      "Batch 134 loss: 41198.166918161856\n",
      "Batch 135 loss: 26958.131324148566\n",
      "Batch 136 loss: 50671.1427081779\n",
      "Batch 137 loss: 46461.976581469826\n",
      "Batch 138 loss: 18459.46384963191\n",
      "Batch 139 loss: 44660.208234399004\n",
      "Batch 140 loss: 33944.06708804733\n",
      "Batch 141 loss: 38689.35053986021\n",
      "Batch 142 loss: 28107.099707885987\n",
      "Batch 143 loss: 43399.96138597497\n",
      "Batch 144 loss: 29993.73709154642\n",
      "Batch 145 loss: 52088.77807775205\n",
      "Batch 146 loss: 32670.831458222456\n",
      "Batch 147 loss: 38696.26481880256\n",
      "Batch 148 loss: 23839.476999012742\n",
      "Batch 149 loss: 40005.84196536581\n",
      "Batch 150 loss: 39455.039852326016\n",
      "Batch 151 loss: 47590.63967445172\n",
      "Batch 152 loss: 22136.566922135826\n",
      "Batch 153 loss: 36564.52758326335\n",
      "Batch 154 loss: 33741.77900601819\n",
      "Batch 155 loss: 30790.20110157586\n",
      "Batch 156 loss: 34803.12037413079\n",
      "Batch 157 loss: 40354.082362659676\n",
      "Batch 158 loss: 38067.18517805822\n",
      "Batch 159 loss: 38562.40307911373\n",
      "Batch 160 loss: 26875.986053112403\n",
      "Batch 161 loss: 41699.147580489545\n",
      "Batch 162 loss: 24869.713586539146\n",
      "Batch 163 loss: 42358.50143655143\n",
      "Batch 164 loss: 34371.39473173191\n",
      "Batch 165 loss: 35265.445777178975\n",
      "Batch 166 loss: 33234.26810851006\n",
      "Batch 167 loss: 16140.979266313516\n",
      "Batch 168 loss: 41578.8411000994\n",
      "Batch 169 loss: 37711.7413342907\n",
      "Batch 170 loss: 43221.83264604308\n",
      "Batch 171 loss: 36862.92506933206\n",
      "Batch 172 loss: 40663.79819304733\n",
      "Batch 173 loss: 34762.87727772823\n",
      "Batch 174 loss: 49478.302233664974\n",
      "Batch 175 loss: 48626.35934798929\n",
      "Batch 176 loss: 42393.22827670933\n",
      "Batch 177 loss: 46783.870075705025\n",
      "Batch 178 loss: 16376.984911922187\n",
      "Batch 179 loss: 43314.547273321645\n",
      "Batch 180 loss: 30441.264691908327\n",
      "Batch 181 loss: 41879.66937153224\n",
      "Batch 182 loss: 38858.849217647876\n",
      "Batch 183 loss: 58448.75900993172\n",
      "Batch 184 loss: 56859.60581415013\n",
      "Batch 185 loss: 39997.15213816535\n",
      "Batch 186 loss: 37082.822966332766\n",
      "Batch 187 loss: 56037.528741592076\n",
      "Batch 188 loss: 20872.386291687788\n",
      "Batch 189 loss: 45051.82929066806\n",
      "Batch 190 loss: 58083.31342917033\n",
      "Batch 191 loss: 38934.11472552566\n",
      "Batch 192 loss: 37255.73651714914\n",
      "Batch 193 loss: 44504.01490776427\n",
      "Batch 194 loss: 51823.06588991981\n",
      "Batch 195 loss: 40168.43107093392\n",
      "Batch 196 loss: 38615.95594722641\n",
      "Batch 197 loss: 24291.617583242205\n",
      "Batch 198 loss: 34434.53722357964\n",
      "Batch 199 loss: 46113.122356898275\n",
      "Batch 200 loss: 30566.192614486725\n",
      "Batch 201 loss: 47832.91252316673\n",
      "Batch 202 loss: 29163.173694960842\n",
      "Batch 203 loss: 23353.645785099572\n",
      "Batch 204 loss: 40673.92807392849\n",
      "Batch 205 loss: 52061.81000834318\n",
      "Batch 206 loss: 40252.15131044381\n",
      "Batch 207 loss: 37814.496139496165\n",
      "Batch 208 loss: 27992.354040055918\n",
      "Batch 209 loss: 49632.89950172782\n",
      "Batch 210 loss: 56310.83217270804\n",
      "Batch 211 loss: 30108.72425747552\n",
      "Batch 212 loss: 31583.06465989832\n",
      "Batch 213 loss: 39756.06024871634\n",
      "Batch 214 loss: 46197.26746601754\n",
      "Batch 215 loss: 31671.151979580904\n",
      "Batch 216 loss: 37680.47010366635\n",
      "Batch 217 loss: 31065.91966950473\n",
      "Batch 218 loss: 41581.86741982318\n",
      "Batch 219 loss: 24966.475408728023\n",
      "Batch 220 loss: 29240.77292133635\n",
      "Batch 221 loss: 28812.608867719424\n",
      "Batch 222 loss: 43647.096643381985\n",
      "Batch 223 loss: 31631.0782734742\n",
      "Batch 224 loss: 60037.72369130297\n",
      "Batch 225 loss: 53987.47603479751\n",
      "Batch 226 loss: 49008.87631069617\n",
      "Batch 227 loss: 33088.35593209722\n",
      "Batch 228 loss: 32485.872881020754\n",
      "Batch 229 loss: 30860.088502202372\n",
      "Batch 230 loss: 42459.059351127114\n",
      "Batch 231 loss: 35922.43781273876\n",
      "Batch 232 loss: 37758.05129559072\n",
      "Batch 233 loss: 34444.122549787106\n",
      "Batch 234 loss: 36914.841856624895\n",
      "Batch 235 loss: 37610.10384515639\n",
      "Batch 236 loss: 37362.48851670377\n",
      "Batch 237 loss: 31741.85489623875\n",
      "Batch 238 loss: 38650.13753665004\n",
      "Batch 239 loss: 43922.01740156622\n",
      "Batch 240 loss: 32100.387863532433\n",
      "Batch 241 loss: 25887.878677683846\n",
      "Batch 242 loss: 40332.600595132586\n",
      "Batch 243 loss: 22831.817750498984\n",
      "Batch 244 loss: 34225.93594631\n",
      "Batch 245 loss: 38714.033081789785\n",
      "Batch 246 loss: 32708.376233451207\n",
      "Batch 247 loss: 32151.04532608609\n",
      "Batch 248 loss: 49377.91983606261\n",
      "Batch 249 loss: 36919.84555920941\n",
      "### Epoch 4\n",
      "Batch 0 loss: 55310.02620767763\n",
      "Batch 1 loss: 28768.157112336547\n",
      "Batch 2 loss: 37396.47384013201\n",
      "Batch 3 loss: 54118.45125473727\n",
      "Batch 4 loss: 40333.51945389804\n",
      "Batch 5 loss: 24838.326155066075\n",
      "Batch 6 loss: 30044.4643150212\n",
      "Batch 7 loss: 40838.05995584088\n",
      "Batch 8 loss: 33739.14773087431\n",
      "Batch 9 loss: 26830.06302245832\n",
      "Batch 10 loss: 28610.7211499124\n",
      "Batch 11 loss: 56326.0108953146\n",
      "Batch 12 loss: 20932.92873324044\n",
      "Batch 13 loss: 46271.138753845546\n",
      "Batch 14 loss: 29739.39485835417\n",
      "Batch 15 loss: 35591.436920996835\n",
      "Batch 16 loss: 27938.313310633464\n",
      "Batch 17 loss: 50556.155118166884\n",
      "Batch 18 loss: 33575.927027427344\n",
      "Batch 19 loss: 32586.19468742403\n",
      "Batch 20 loss: 41428.98502145506\n",
      "Batch 21 loss: 42745.83143332541\n",
      "Batch 22 loss: 27244.592535336793\n",
      "Batch 23 loss: 48741.44193129972\n",
      "Batch 24 loss: 48841.274764232265\n",
      "Batch 25 loss: 60636.44452108897\n",
      "Batch 26 loss: 34638.63828312513\n",
      "Batch 27 loss: 38611.53887240586\n",
      "Batch 28 loss: 36181.168844982094\n",
      "Batch 29 loss: 40742.604368078246\n",
      "Batch 30 loss: 37803.33441225798\n",
      "Batch 31 loss: 70085.23632897082\n",
      "Batch 32 loss: 39225.27501937019\n",
      "Batch 33 loss: 23710.21065946485\n",
      "Batch 34 loss: 34655.270665252334\n",
      "Batch 35 loss: 44450.30997691958\n",
      "Batch 36 loss: 34531.13925758098\n",
      "Batch 37 loss: 37756.82865211734\n",
      "Batch 38 loss: 37603.73310427465\n",
      "Batch 39 loss: 55496.964897777885\n",
      "Batch 40 loss: 36933.449745779915\n",
      "Batch 41 loss: 42599.05558130822\n",
      "Batch 42 loss: 54941.01454185843\n",
      "Batch 43 loss: 42005.93227690784\n",
      "Batch 44 loss: 31619.003182288634\n",
      "Batch 45 loss: 39349.637867105084\n",
      "Batch 46 loss: 44834.02561688639\n",
      "Batch 47 loss: 37625.450115275875\n",
      "Batch 48 loss: 50702.312785622074\n",
      "Batch 49 loss: 24090.151939672272\n",
      "Batch 50 loss: 56043.040758909214\n",
      "Batch 51 loss: 43314.308216199715\n",
      "Batch 52 loss: 30072.184700869962\n",
      "Batch 53 loss: 45426.421429011454\n",
      "Batch 54 loss: 33150.446371843034\n",
      "Batch 55 loss: 29018.874044423304\n",
      "Batch 56 loss: 58663.40296476075\n",
      "Batch 57 loss: 43491.212645779364\n",
      "Batch 58 loss: 25284.779670898948\n",
      "Batch 59 loss: 37855.200248867775\n",
      "Batch 60 loss: 39443.384076971604\n",
      "Batch 61 loss: 31904.017571464476\n",
      "Batch 62 loss: 32459.448927576308\n",
      "Batch 63 loss: 32704.538335075966\n",
      "Batch 64 loss: 25769.825932431715\n",
      "Batch 65 loss: 37824.90934865707\n",
      "Batch 66 loss: 34784.47031889171\n",
      "Batch 67 loss: 47533.63311110223\n",
      "Batch 68 loss: 41430.33288980447\n",
      "Batch 69 loss: 66902.60715159286\n",
      "Batch 70 loss: 41994.53389339865\n",
      "Batch 71 loss: 40450.08539903157\n",
      "Batch 72 loss: 38288.86164402586\n",
      "Batch 73 loss: 49426.705268861384\n",
      "Batch 74 loss: 39473.10127016154\n",
      "Batch 75 loss: 39644.44169443349\n",
      "Batch 76 loss: 21096.837595240093\n",
      "Batch 77 loss: 35940.06171864339\n",
      "Batch 78 loss: 66516.0628564532\n",
      "Batch 79 loss: 34723.256616019324\n",
      "Batch 80 loss: 29518.29054686112\n",
      "Batch 81 loss: 24967.026854797266\n",
      "Batch 82 loss: 51592.433431854835\n",
      "Batch 83 loss: 40023.11317844184\n",
      "Batch 84 loss: 37332.727186414515\n",
      "Batch 85 loss: 52469.61929178334\n",
      "Batch 86 loss: 54606.6579184247\n",
      "Batch 87 loss: 39409.727268111346\n",
      "Batch 88 loss: 49252.44130103605\n",
      "Batch 89 loss: 33186.51389492246\n",
      "Batch 90 loss: 39337.66080482188\n",
      "Batch 91 loss: 41248.51235602879\n",
      "Batch 92 loss: 47445.010787347885\n",
      "Batch 93 loss: 50115.844151544334\n",
      "Batch 94 loss: 43715.81467732654\n",
      "Batch 95 loss: 35082.21866116597\n",
      "Batch 96 loss: 21188.889318510275\n",
      "Batch 97 loss: 44307.51430328619\n",
      "Batch 98 loss: 41638.07099058959\n",
      "Batch 99 loss: 42340.07613224792\n",
      "Batch 100 loss: 59689.3997858192\n",
      "Batch 101 loss: 32071.79027657798\n",
      "Batch 102 loss: 39743.90793232056\n",
      "Batch 103 loss: 36797.96004355515\n",
      "Batch 104 loss: 40613.65564311892\n",
      "Batch 105 loss: 38761.379310975506\n",
      "Batch 106 loss: 37799.08749775876\n",
      "Batch 107 loss: 40194.64267319617\n",
      "Batch 108 loss: 40574.683484489564\n",
      "Batch 109 loss: 15927.633628162019\n",
      "Batch 110 loss: 43570.43133225391\n",
      "Batch 111 loss: 26832.712061403447\n",
      "Batch 112 loss: 31849.96165236849\n",
      "Batch 113 loss: 39295.24291653186\n",
      "Batch 114 loss: 32233.165428365734\n",
      "Batch 115 loss: 31100.638828180585\n",
      "Batch 116 loss: 24848.89588710222\n",
      "Batch 117 loss: 28908.26594647693\n",
      "Batch 118 loss: 36695.05885718274\n",
      "Batch 119 loss: 33351.92229985835\n",
      "Batch 120 loss: 33825.05232772982\n",
      "Batch 121 loss: 31909.750939510384\n",
      "Batch 122 loss: 50791.27916983534\n",
      "Batch 123 loss: 28087.228591455103\n",
      "Batch 124 loss: 29707.51959622784\n",
      "Batch 125 loss: 31464.369080629644\n",
      "Batch 126 loss: 45287.39614016982\n",
      "Batch 127 loss: 30428.453645638743\n",
      "Batch 128 loss: 40295.94296893009\n",
      "Batch 129 loss: 33778.75130244783\n",
      "Batch 130 loss: 35687.916716270935\n",
      "Batch 131 loss: 31446.39571916493\n",
      "Batch 132 loss: 36465.686090996336\n",
      "Batch 133 loss: 38655.98661197294\n",
      "Batch 134 loss: 41405.10444620614\n",
      "Batch 135 loss: 26651.286874150606\n",
      "Batch 136 loss: 39247.821090576355\n",
      "Batch 137 loss: 35868.16334341878\n",
      "Batch 138 loss: 26934.98231105262\n",
      "Batch 139 loss: 38137.64000432291\n",
      "Batch 140 loss: 41125.486950678845\n",
      "Batch 141 loss: 59413.261461264236\n",
      "Batch 142 loss: 21158.192950043136\n",
      "Batch 143 loss: 31284.90749483346\n",
      "Batch 144 loss: 38289.76151853528\n",
      "Batch 145 loss: 37991.81838169566\n",
      "Batch 146 loss: 30388.967148138327\n",
      "Batch 147 loss: 34241.94461906231\n",
      "Batch 148 loss: 39892.64872985993\n",
      "Batch 149 loss: 30319.63953694698\n",
      "Batch 150 loss: 35181.370396832244\n",
      "Batch 151 loss: 34987.971459646666\n",
      "Batch 152 loss: 51297.97777615064\n",
      "Batch 153 loss: 38351.278047830754\n",
      "Batch 154 loss: 42254.15656664798\n",
      "Batch 155 loss: 25761.207674730824\n",
      "Batch 156 loss: 48160.51102272464\n",
      "Batch 157 loss: 42427.57018655352\n",
      "Batch 158 loss: 49540.105404534464\n",
      "Batch 159 loss: 35227.58286823296\n",
      "Batch 160 loss: 33073.22330877689\n",
      "Batch 161 loss: 34942.79159669878\n",
      "Batch 162 loss: 49115.98026249457\n",
      "Batch 163 loss: 27973.852699720603\n",
      "Batch 164 loss: 52093.343397986275\n",
      "Batch 165 loss: 27562.634425317778\n",
      "Batch 166 loss: 50316.843955417266\n",
      "Batch 167 loss: 22125.107575213024\n",
      "Batch 168 loss: 37561.78193513738\n",
      "Batch 169 loss: 17842.461992506098\n",
      "Batch 170 loss: 41370.67178242529\n",
      "Batch 171 loss: 38101.056487343325\n",
      "Batch 172 loss: 30266.51752759454\n",
      "Batch 173 loss: 28244.417064431087\n",
      "Batch 174 loss: 43288.67808949879\n",
      "Batch 175 loss: 29558.673098982574\n",
      "Batch 176 loss: 27921.087684907652\n",
      "Batch 177 loss: 34666.73967978877\n",
      "Batch 178 loss: 49617.70347485424\n",
      "Batch 179 loss: 38713.063185060484\n",
      "Batch 180 loss: 34055.09054697142\n",
      "Batch 181 loss: 36303.53009314793\n",
      "Batch 182 loss: 32396.779650211643\n",
      "Batch 183 loss: 40907.472428000736\n",
      "Batch 184 loss: 40436.53069323604\n",
      "Batch 185 loss: 32237.48163769892\n",
      "Batch 186 loss: 59974.79232148755\n",
      "Batch 187 loss: 52931.79946049502\n",
      "Batch 188 loss: 46301.85783696176\n",
      "Batch 189 loss: 34757.09838985781\n",
      "Batch 190 loss: 51554.08914355402\n",
      "Batch 191 loss: 37651.0537221829\n",
      "Batch 192 loss: 40037.11689102149\n",
      "Batch 193 loss: 36540.34860336353\n",
      "Batch 194 loss: 47487.71651812982\n",
      "Batch 195 loss: 55669.70722597058\n",
      "Batch 196 loss: 24191.291260698057\n",
      "Batch 197 loss: 47415.77139708215\n",
      "Batch 198 loss: 34567.605541056895\n",
      "Batch 199 loss: 20300.573398176966\n",
      "Batch 200 loss: 43118.55075596226\n",
      "Batch 201 loss: 34775.693959273216\n",
      "Batch 202 loss: 43555.469532548595\n",
      "Batch 203 loss: 48311.99898850726\n",
      "Batch 204 loss: 38378.77371689984\n",
      "Batch 205 loss: 24738.596531735573\n",
      "Batch 206 loss: 26184.326048421128\n",
      "Batch 207 loss: 43972.45855140424\n",
      "Batch 208 loss: 40048.65138919498\n",
      "Batch 209 loss: 45608.411388091045\n",
      "Batch 210 loss: 33144.85050413312\n",
      "Batch 211 loss: 46475.30492754428\n",
      "Batch 212 loss: 40407.821249236935\n",
      "Batch 213 loss: 41192.670939307085\n",
      "Batch 214 loss: 32326.863062169148\n",
      "Batch 215 loss: 38957.98400485065\n",
      "Batch 216 loss: 40038.691177369765\n",
      "Batch 217 loss: 35099.154743419225\n",
      "Batch 218 loss: 29096.4448313373\n",
      "Batch 219 loss: 33231.650946144815\n",
      "Batch 220 loss: 47618.40971662287\n",
      "Batch 221 loss: 43398.48809455511\n",
      "Batch 222 loss: 35777.08994550266\n",
      "Batch 223 loss: 50594.88365206969\n",
      "Batch 224 loss: 19897.100520130523\n",
      "Batch 225 loss: 42755.94527422353\n",
      "Batch 226 loss: 26112.859121599828\n",
      "Batch 227 loss: 36045.51506241652\n",
      "Batch 228 loss: 33434.894928162656\n",
      "Batch 229 loss: 28834.62569242101\n",
      "Batch 230 loss: 26441.227837005346\n",
      "Batch 231 loss: 30015.92880101762\n",
      "Batch 232 loss: 38290.12024499811\n",
      "Batch 233 loss: 37589.67844591105\n",
      "Batch 234 loss: 34259.35312515592\n",
      "Batch 235 loss: 36090.92377980379\n",
      "Batch 236 loss: 32239.746040235616\n",
      "Batch 237 loss: 34324.521137504766\n",
      "Batch 238 loss: 40781.06654224615\n",
      "Batch 239 loss: 29472.19842534481\n",
      "Batch 240 loss: 23500.547821771936\n",
      "Batch 241 loss: 41829.84976366699\n",
      "Batch 242 loss: 37545.455112386844\n",
      "Batch 243 loss: 38956.41142890866\n",
      "Batch 244 loss: 31914.685423296927\n",
      "Batch 245 loss: 43857.370524909915\n",
      "Batch 246 loss: 33465.10234778104\n",
      "Batch 247 loss: 26804.22917407773\n",
      "Batch 248 loss: 34249.245280185394\n",
      "Batch 249 loss: 45447.51908143466\n",
      "### Epoch 5\n",
      "Batch 0 loss: 37787.95910738953\n",
      "Batch 1 loss: 34509.45880898921\n",
      "Batch 2 loss: 24304.060998405035\n",
      "Batch 3 loss: 34606.26031363108\n",
      "Batch 4 loss: 35445.08603955173\n",
      "Batch 5 loss: 35319.234980847395\n",
      "Batch 6 loss: 43882.38777952269\n",
      "Batch 7 loss: 41143.76377368668\n",
      "Batch 8 loss: 42411.079596642456\n",
      "Batch 9 loss: 47076.500139317184\n",
      "Batch 10 loss: 25845.212895690438\n",
      "Batch 11 loss: 29681.016376251275\n",
      "Batch 12 loss: 45470.993721703344\n",
      "Batch 13 loss: 51488.023567392\n",
      "Batch 14 loss: 41504.452525025044\n",
      "Batch 15 loss: 39128.32547490748\n",
      "Batch 16 loss: 44427.750163064025\n",
      "Batch 17 loss: 47034.56792768001\n",
      "Batch 18 loss: 24935.92407498946\n",
      "Batch 19 loss: 37977.97950435463\n",
      "Batch 20 loss: 43543.7398450927\n",
      "Batch 21 loss: 24524.615940259522\n",
      "Batch 22 loss: 38717.82776759536\n",
      "Batch 23 loss: 35511.55016497311\n",
      "Batch 24 loss: 41703.235485971396\n",
      "Batch 25 loss: 19584.28328401812\n",
      "Batch 26 loss: 46583.709338621804\n",
      "Batch 27 loss: 41321.16586066648\n",
      "Batch 28 loss: 39956.10772736653\n",
      "Batch 29 loss: 45803.46102956145\n",
      "Batch 30 loss: 37257.833417951624\n",
      "Batch 31 loss: 51734.631366977366\n",
      "Batch 32 loss: 31551.20603569158\n",
      "Batch 33 loss: 43492.73177773002\n",
      "Batch 34 loss: 40061.7620983527\n",
      "Batch 35 loss: 46575.09343003399\n",
      "Batch 36 loss: 33537.275560764814\n",
      "Batch 37 loss: 48269.81795552456\n",
      "Batch 38 loss: 53270.07374832225\n",
      "Batch 39 loss: 40649.22407839929\n",
      "Batch 40 loss: 40186.17877357358\n",
      "Batch 41 loss: 31563.809263013325\n",
      "Batch 42 loss: 24173.520104950534\n",
      "Batch 43 loss: 25400.614790663356\n",
      "Batch 44 loss: 33702.65923861171\n",
      "Batch 45 loss: 53216.26855457376\n",
      "Batch 46 loss: 34434.39932332584\n",
      "Batch 47 loss: 27522.851699721105\n",
      "Batch 48 loss: 42744.06506333302\n",
      "Batch 49 loss: 36925.43080036799\n",
      "Batch 50 loss: 33137.74141903082\n",
      "Batch 51 loss: 26546.947557946707\n",
      "Batch 52 loss: 33925.44435676072\n",
      "Batch 53 loss: 22639.382734230654\n",
      "Batch 54 loss: 33957.20723168039\n",
      "Batch 55 loss: 57723.70251190789\n",
      "Batch 56 loss: 41878.78135481383\n",
      "Batch 57 loss: 41981.58688315813\n",
      "Batch 58 loss: 61273.207241937474\n",
      "Batch 59 loss: 46022.51960164972\n",
      "Batch 60 loss: 47879.059216951195\n",
      "Batch 61 loss: 30986.98293594794\n",
      "Batch 62 loss: 28670.321803357074\n",
      "Batch 63 loss: 34269.37019314419\n",
      "Batch 64 loss: 39871.6781725702\n",
      "Batch 65 loss: 37060.01430663119\n",
      "Batch 66 loss: 40638.48127716572\n",
      "Batch 67 loss: 30838.046679220766\n",
      "Batch 68 loss: 32677.284694380556\n",
      "Batch 69 loss: 23064.7136279614\n",
      "Batch 70 loss: 49645.10768480254\n",
      "Batch 71 loss: 51837.353022776995\n",
      "Batch 72 loss: 23155.71320701358\n",
      "Batch 73 loss: 39890.16525814374\n",
      "Batch 74 loss: 26420.612003076352\n",
      "Batch 75 loss: 43054.811100451916\n",
      "Batch 76 loss: 48151.763271637385\n",
      "Batch 77 loss: 30993.355276274015\n",
      "Batch 78 loss: 34229.014045596676\n",
      "Batch 79 loss: 35118.451272716884\n",
      "Batch 80 loss: 43043.466519341244\n",
      "Batch 81 loss: 51288.92291986472\n",
      "Batch 82 loss: 37427.47495334336\n",
      "Batch 83 loss: 53132.37048409762\n",
      "Batch 84 loss: 32121.843400386057\n",
      "Batch 85 loss: 42303.28030139594\n",
      "Batch 86 loss: 57033.467906985374\n",
      "Batch 87 loss: 37040.50558010692\n",
      "Batch 88 loss: 27607.09587576529\n",
      "Batch 89 loss: 40789.0949573964\n",
      "Batch 90 loss: 49702.20147699803\n",
      "Batch 91 loss: 45124.61758407703\n",
      "Batch 92 loss: 34986.464535313935\n",
      "Batch 93 loss: 30575.80616385418\n",
      "Batch 94 loss: 29678.83357272532\n",
      "Batch 95 loss: 36729.74988125108\n",
      "Batch 96 loss: 34581.529537214956\n",
      "Batch 97 loss: 43404.200295471404\n",
      "Batch 98 loss: 31343.30099115659\n",
      "Batch 99 loss: 31950.810309911372\n",
      "Batch 100 loss: 44230.86701173409\n",
      "Batch 101 loss: 39405.673403220855\n",
      "Batch 102 loss: 24937.112298359178\n",
      "Batch 103 loss: 26285.971158969533\n",
      "Batch 104 loss: 35092.67953175201\n",
      "Batch 105 loss: 36485.07800181141\n",
      "Batch 106 loss: 27456.00500010765\n",
      "Batch 107 loss: 33480.35350602497\n",
      "Batch 108 loss: 44591.8693522966\n",
      "Batch 109 loss: 16925.460411519165\n",
      "Batch 110 loss: 28519.737004839146\n",
      "Batch 111 loss: 43029.749939271984\n",
      "Batch 112 loss: 46964.38127011634\n",
      "Batch 113 loss: 38938.69336527708\n",
      "Batch 114 loss: 33705.840014702626\n",
      "Batch 115 loss: 39370.26449485477\n",
      "Batch 116 loss: 33007.95762720493\n",
      "Batch 117 loss: 33866.06649700595\n",
      "Batch 118 loss: 44338.37449011592\n",
      "Batch 119 loss: 41745.99549542974\n",
      "Batch 120 loss: 39799.23706273878\n",
      "Batch 121 loss: 28349.079993091094\n",
      "Batch 122 loss: 27791.14203395523\n",
      "Batch 123 loss: 32537.31792629636\n",
      "Batch 124 loss: 34729.65417545687\n",
      "Batch 125 loss: 41495.23956839474\n",
      "Batch 126 loss: 31812.21843326291\n",
      "Batch 127 loss: 51172.23808238303\n",
      "Batch 128 loss: 30035.898943773576\n",
      "Batch 129 loss: 55338.642250216704\n",
      "Batch 130 loss: 57170.341621283136\n",
      "Batch 131 loss: 24838.41132910646\n",
      "Batch 132 loss: 43931.79545450279\n",
      "Batch 133 loss: 40869.051922484156\n",
      "Batch 134 loss: 39224.90575636471\n",
      "Batch 135 loss: 28626.279252877135\n",
      "Batch 136 loss: 45106.70941047347\n",
      "Batch 137 loss: 37271.70770982865\n",
      "Batch 138 loss: 37107.592686336226\n",
      "Batch 139 loss: 43271.06690837961\n",
      "Batch 140 loss: 28921.774909958734\n",
      "Batch 141 loss: 46102.25161862558\n",
      "Batch 142 loss: 39865.236215750614\n",
      "Batch 143 loss: 17775.62460512226\n",
      "Batch 144 loss: 41266.61317555583\n",
      "Batch 145 loss: 43852.93093392006\n",
      "Batch 146 loss: 29636.917708908673\n",
      "Batch 147 loss: 40211.03041670338\n",
      "Batch 148 loss: 47272.06606574774\n",
      "Batch 149 loss: 39696.325730859935\n",
      "Batch 150 loss: 49819.25708114712\n",
      "Batch 151 loss: 20670.022432147078\n",
      "Batch 152 loss: 44193.919611923\n",
      "Batch 153 loss: 36781.90536475469\n",
      "Batch 154 loss: 55552.0490497992\n",
      "Batch 155 loss: 41989.26572163733\n",
      "Batch 156 loss: 33212.50107623975\n",
      "Batch 157 loss: 46671.63229640022\n",
      "Batch 158 loss: 30802.21056207401\n",
      "Batch 159 loss: 32808.8754097125\n",
      "Batch 160 loss: 18003.891211184848\n",
      "Batch 161 loss: 35088.95486025763\n",
      "Batch 162 loss: 40200.02723336585\n",
      "Batch 163 loss: 53203.437641891156\n",
      "Batch 164 loss: 40876.23415674311\n",
      "Batch 165 loss: 31429.277691343093\n",
      "Batch 166 loss: 35185.572634928445\n",
      "Batch 167 loss: 47996.92374453557\n",
      "Batch 168 loss: 31803.603809420674\n",
      "Batch 169 loss: 34622.022406792974\n",
      "Batch 170 loss: 50297.983385311585\n",
      "Batch 171 loss: 38577.70404234881\n",
      "Batch 172 loss: 30984.43076349364\n",
      "Batch 173 loss: 39476.94568740318\n",
      "Batch 174 loss: 37207.43404524621\n",
      "Batch 175 loss: 41408.36179344321\n",
      "Batch 176 loss: 44722.24367247232\n",
      "Batch 177 loss: 32190.058169082247\n",
      "Batch 178 loss: 42211.54704585229\n",
      "Batch 179 loss: 34796.105715209866\n",
      "Batch 180 loss: 38451.0449433014\n",
      "Batch 181 loss: 55286.06825103928\n",
      "Batch 182 loss: 25724.298013004038\n",
      "Batch 183 loss: 27067.946475807534\n",
      "Batch 184 loss: 32560.784960154517\n",
      "Batch 185 loss: 45249.76967646835\n",
      "Batch 186 loss: 36527.03693735105\n",
      "Batch 187 loss: 41175.17470274434\n",
      "Batch 188 loss: 44278.68753449352\n",
      "Batch 189 loss: 34095.14000123259\n",
      "Batch 190 loss: 43014.77061303261\n",
      "Batch 191 loss: 40101.37921667277\n",
      "Batch 192 loss: 40318.482289911066\n",
      "Batch 193 loss: 36152.13869435292\n",
      "Batch 194 loss: 38072.23535453314\n",
      "Batch 195 loss: 39507.13540290719\n",
      "Batch 196 loss: 38792.09094405492\n",
      "Batch 197 loss: 54511.180974282775\n",
      "Batch 198 loss: 26366.243586185825\n",
      "Batch 199 loss: 38506.414573625174\n",
      "Batch 200 loss: 28371.462309284434\n",
      "Batch 201 loss: 26506.213286093604\n",
      "Batch 202 loss: 37048.16454076354\n",
      "Batch 203 loss: 33695.531310920174\n",
      "Batch 204 loss: 42370.97386123145\n",
      "Batch 205 loss: 37571.12226049939\n",
      "Batch 206 loss: 55654.996410582266\n",
      "Batch 207 loss: 33500.68116973626\n",
      "Batch 208 loss: 38715.80481742762\n",
      "Batch 209 loss: 35492.51033663684\n",
      "Batch 210 loss: 48762.06289138454\n",
      "Batch 211 loss: 51610.34385879421\n",
      "Batch 212 loss: 33945.54012158645\n",
      "Batch 213 loss: 43665.78876464011\n",
      "Batch 214 loss: 29206.94886447622\n",
      "Batch 215 loss: 29511.465050265033\n",
      "Batch 216 loss: 46138.267898874736\n",
      "Batch 217 loss: 42801.61084049457\n",
      "Batch 218 loss: 29682.885994877215\n",
      "Batch 219 loss: 37889.38354176626\n",
      "Batch 220 loss: 32810.73625311292\n",
      "Batch 221 loss: 24990.905843647695\n",
      "Batch 222 loss: 33242.17479233685\n",
      "Batch 223 loss: 39519.33083249284\n",
      "Batch 224 loss: 49177.151729515506\n",
      "Batch 225 loss: 40944.38264954145\n",
      "Batch 226 loss: 46271.74998180456\n",
      "Batch 227 loss: 43236.02924340954\n",
      "Batch 228 loss: 24083.444682848014\n",
      "Batch 229 loss: 32384.226319832094\n",
      "Batch 230 loss: 36784.477477978144\n",
      "Batch 231 loss: 39731.520973372666\n",
      "Batch 232 loss: 25216.837564854097\n",
      "Batch 233 loss: 27307.225963558856\n",
      "Batch 234 loss: 33632.268653047875\n",
      "Batch 235 loss: 31030.467708940643\n",
      "Batch 236 loss: 41212.03612774731\n",
      "Batch 237 loss: 51791.00956161935\n",
      "Batch 238 loss: 27734.62243112866\n",
      "Batch 239 loss: 41633.10976664154\n",
      "Batch 240 loss: 50709.71371714635\n",
      "Batch 241 loss: 40097.33594222129\n",
      "Batch 242 loss: 24921.318338304434\n",
      "Batch 243 loss: 48915.72354137481\n",
      "Batch 244 loss: 46141.45544657288\n",
      "Batch 245 loss: 36004.52839951647\n",
      "Batch 246 loss: 40137.936605804345\n",
      "Batch 247 loss: 33965.13802996487\n",
      "Batch 248 loss: 48839.7513502145\n",
      "Batch 249 loss: 41878.84774228759\n",
      "### Epoch 6\n",
      "Batch 0 loss: 33820.35478417642\n",
      "Batch 1 loss: 42190.25965025381\n",
      "Batch 2 loss: 28401.858111722046\n",
      "Batch 3 loss: 40948.42383845038\n",
      "Batch 4 loss: 31311.720721653008\n",
      "Batch 5 loss: 45419.70182205814\n",
      "Batch 6 loss: 44166.64064094733\n",
      "Batch 7 loss: 33047.47900863805\n",
      "Batch 8 loss: 47903.210283148146\n",
      "Batch 9 loss: 47442.12735650547\n",
      "Batch 10 loss: 30550.512930008463\n",
      "Batch 11 loss: 41160.14059928732\n",
      "Batch 12 loss: 54027.9248084899\n",
      "Batch 13 loss: 42823.836774690695\n",
      "Batch 14 loss: 46205.94654405367\n",
      "Batch 15 loss: 49111.42840224582\n",
      "Batch 16 loss: 48202.827697021305\n",
      "Batch 17 loss: 38608.25434772162\n",
      "Batch 18 loss: 38039.02941228314\n",
      "Batch 19 loss: 39227.35750589629\n",
      "Batch 20 loss: 29693.738811794952\n",
      "Batch 21 loss: 24138.53394359748\n",
      "Batch 22 loss: 38833.730819673874\n",
      "Batch 23 loss: 60540.41958842567\n",
      "Batch 24 loss: 42930.88597750629\n",
      "Batch 25 loss: 43803.60536334797\n",
      "Batch 26 loss: 56238.58795730777\n",
      "Batch 27 loss: 39704.99028524217\n",
      "Batch 28 loss: 33246.745875614855\n",
      "Batch 29 loss: 49453.02861086563\n",
      "Batch 30 loss: 41678.23142787181\n",
      "Batch 31 loss: 48147.321309299616\n",
      "Batch 32 loss: 39004.33801824135\n",
      "Batch 33 loss: 50853.25797040216\n",
      "Batch 34 loss: 40287.18504504732\n",
      "Batch 35 loss: 23449.421081941204\n",
      "Batch 36 loss: 28503.468825707918\n",
      "Batch 37 loss: 47627.99422176988\n",
      "Batch 38 loss: 38430.459310753744\n",
      "Batch 39 loss: 21027.427625035798\n",
      "Batch 40 loss: 51926.56555022535\n",
      "Batch 41 loss: 49958.13236572913\n",
      "Batch 42 loss: 26274.201947957845\n",
      "Batch 43 loss: 46425.40873540557\n",
      "Batch 44 loss: 39092.17519049211\n",
      "Batch 45 loss: 29038.452508772443\n",
      "Batch 46 loss: 35929.9263673133\n",
      "Batch 47 loss: 36590.43544669377\n",
      "Batch 48 loss: 39294.793630519926\n",
      "Batch 49 loss: 37434.835132577246\n",
      "Batch 50 loss: 46873.91608725405\n",
      "Batch 51 loss: 45547.28144141747\n",
      "Batch 52 loss: 30450.45516816737\n",
      "Batch 53 loss: 40910.29380217314\n",
      "Batch 54 loss: 55374.696491143\n",
      "Batch 55 loss: 49440.715152483\n",
      "Batch 56 loss: 55226.74481432973\n",
      "Batch 57 loss: 31764.516832834564\n",
      "Batch 58 loss: 45976.34143633199\n",
      "Batch 59 loss: 29038.400672788386\n",
      "Batch 60 loss: 17531.812633780646\n",
      "Batch 61 loss: 28760.503286588224\n",
      "Batch 62 loss: 52056.04119775784\n",
      "Batch 63 loss: 34292.372566633756\n",
      "Batch 64 loss: 41763.425074605286\n",
      "Batch 65 loss: 26963.126305712365\n",
      "Batch 66 loss: 46895.80776665354\n",
      "Batch 67 loss: 30960.663757736285\n",
      "Batch 68 loss: 29919.00907075849\n",
      "Batch 69 loss: 26485.864781864617\n",
      "Batch 70 loss: 45869.02862004889\n",
      "Batch 71 loss: 48005.671437826895\n",
      "Batch 72 loss: 43587.92003841724\n",
      "Batch 73 loss: 39018.69321995099\n",
      "Batch 74 loss: 27568.534545483824\n",
      "Batch 75 loss: 37498.23782606991\n",
      "Batch 76 loss: 31870.59619045222\n",
      "Batch 77 loss: 32896.798479563426\n",
      "Batch 78 loss: 27471.942744573218\n",
      "Batch 79 loss: 22935.73551475126\n",
      "Batch 80 loss: 40689.1750496927\n",
      "Batch 81 loss: 41138.68678152076\n",
      "Batch 82 loss: 38512.31062807357\n",
      "Batch 83 loss: 33816.442196167685\n",
      "Batch 84 loss: 50364.80942350754\n",
      "Batch 85 loss: 26632.33263454264\n",
      "Batch 86 loss: 46805.7533571585\n",
      "Batch 87 loss: 32622.094844359865\n",
      "Batch 88 loss: 27767.435144381656\n",
      "Batch 89 loss: 23016.2897525147\n",
      "Batch 90 loss: 27690.27448928282\n",
      "Batch 91 loss: 44684.02138157021\n",
      "Batch 92 loss: 39196.99482376471\n",
      "Batch 93 loss: 22501.562456378277\n",
      "Batch 94 loss: 24941.430167348826\n",
      "Batch 95 loss: 38420.92246339955\n",
      "Batch 96 loss: 44731.45330606949\n",
      "Batch 97 loss: 24074.258440548045\n",
      "Batch 98 loss: 66447.44743897876\n",
      "Batch 99 loss: 28534.046899212157\n",
      "Batch 100 loss: 42216.33596828983\n",
      "Batch 101 loss: 33102.93717488858\n",
      "Batch 102 loss: 32841.01126138579\n",
      "Batch 103 loss: 33258.753124294904\n",
      "Batch 104 loss: 34484.98782045062\n",
      "Batch 105 loss: 31052.028633402442\n",
      "Batch 106 loss: 31759.87455450934\n",
      "Batch 107 loss: 44258.980552450026\n",
      "Batch 108 loss: 42820.58539466225\n",
      "Batch 109 loss: 38389.57732134233\n",
      "Batch 110 loss: 35710.344291428584\n",
      "Batch 111 loss: 40778.35644246676\n",
      "Batch 112 loss: 44125.07611161102\n",
      "Batch 113 loss: 29133.780744975465\n",
      "Batch 114 loss: 44272.79993600992\n",
      "Batch 115 loss: 44666.56816951743\n",
      "Batch 116 loss: 36276.680217141344\n",
      "Batch 117 loss: 30988.782941684567\n",
      "Batch 118 loss: 38683.759051637004\n",
      "Batch 119 loss: 35206.8157817016\n",
      "Batch 120 loss: 32211.460872312608\n",
      "Batch 121 loss: 33044.223214443686\n",
      "Batch 122 loss: 29196.50541766795\n",
      "Batch 123 loss: 30988.173422837703\n",
      "Batch 124 loss: 24311.864147894856\n",
      "Batch 125 loss: 26696.560140482063\n",
      "Batch 126 loss: 49482.37477422456\n",
      "Batch 127 loss: 44920.18158598764\n",
      "Batch 128 loss: 54532.27106088784\n",
      "Batch 129 loss: 38265.768927861216\n",
      "Batch 130 loss: 46392.25064973418\n",
      "Batch 131 loss: 31515.574128742388\n",
      "Batch 132 loss: 20943.43893912037\n",
      "Batch 133 loss: 31263.587687343326\n",
      "Batch 134 loss: 35095.44664857604\n",
      "Batch 135 loss: 67089.00954988468\n",
      "Batch 136 loss: 38103.878029475665\n",
      "Batch 137 loss: 35541.796457698314\n",
      "Batch 138 loss: 32238.899419001693\n",
      "Batch 139 loss: 28033.164329249103\n",
      "Batch 140 loss: 44643.84396752874\n",
      "Batch 141 loss: 35463.370797846124\n",
      "Batch 142 loss: 39105.42640359576\n",
      "Batch 143 loss: 44390.981928704496\n",
      "Batch 144 loss: 34825.22464005859\n",
      "Batch 145 loss: 25055.139805512496\n",
      "Batch 146 loss: 28364.724896563417\n",
      "Batch 147 loss: 31196.43060045342\n",
      "Batch 148 loss: 36096.73594706493\n",
      "Batch 149 loss: 30644.9020101965\n",
      "Batch 150 loss: 47809.99916510268\n",
      "Batch 151 loss: 27969.87271660405\n",
      "Batch 152 loss: 43842.24696226754\n",
      "Batch 153 loss: 31381.79683278319\n",
      "Batch 154 loss: 40190.14163127757\n",
      "Batch 155 loss: 53837.71074436428\n",
      "Batch 156 loss: 35084.46852898609\n",
      "Batch 157 loss: 38092.214471221\n",
      "Batch 158 loss: 32893.25613081955\n",
      "Batch 159 loss: 42148.35146768694\n",
      "Batch 160 loss: 32687.69913968288\n",
      "Batch 161 loss: 35716.947072523064\n",
      "Batch 162 loss: 42799.62662250675\n",
      "Batch 163 loss: 35742.93705811126\n",
      "Batch 164 loss: 39298.67890959691\n",
      "Batch 165 loss: 36128.211938768465\n",
      "Batch 166 loss: 66816.96130803417\n",
      "Batch 167 loss: 24965.074863420956\n",
      "Batch 168 loss: 31764.78523339235\n",
      "Batch 169 loss: 42819.5393836725\n",
      "Batch 170 loss: 49480.86547346987\n",
      "Batch 171 loss: 38921.934248973805\n",
      "Batch 172 loss: 41846.792617072744\n",
      "Batch 173 loss: 38683.82927270471\n",
      "Batch 174 loss: 47477.22292048413\n",
      "Batch 175 loss: 41584.51573720477\n",
      "Batch 176 loss: 32434.71363906785\n",
      "Batch 177 loss: 49304.94945427543\n",
      "Batch 178 loss: 33934.92430876053\n",
      "Batch 179 loss: 30791.28864732231\n",
      "Batch 180 loss: 31403.83981802229\n",
      "Batch 181 loss: 43144.44937511455\n",
      "Batch 182 loss: 43988.7321851201\n",
      "Batch 183 loss: 25476.2307821192\n",
      "Batch 184 loss: 28612.740541016967\n",
      "Batch 185 loss: 22335.488091744555\n",
      "Batch 186 loss: 40143.47060858235\n",
      "Batch 187 loss: 37145.93729051205\n",
      "Batch 188 loss: 68234.96993122678\n",
      "Batch 189 loss: 34551.22964244393\n",
      "Batch 190 loss: 65920.72712010535\n",
      "Batch 191 loss: 29817.889629878635\n",
      "Batch 192 loss: 30564.578365315487\n",
      "Batch 193 loss: 39005.66320362578\n",
      "Batch 194 loss: 26393.779745733336\n",
      "Batch 195 loss: 53874.567509518485\n",
      "Batch 196 loss: 57742.10388253722\n",
      "Batch 197 loss: 30035.077466354847\n",
      "Batch 198 loss: 28829.767270218526\n",
      "Batch 199 loss: 26383.04919632367\n",
      "Batch 200 loss: 39639.98663971295\n",
      "Batch 201 loss: 37770.01257683319\n",
      "Batch 202 loss: 50708.13394965154\n",
      "Batch 203 loss: 33348.07990428923\n",
      "Batch 204 loss: 37868.73812959283\n",
      "Batch 205 loss: 26234.68885754964\n",
      "Batch 206 loss: 34014.38446695708\n",
      "Batch 207 loss: 36491.253705519164\n",
      "Batch 208 loss: 28618.048282815987\n",
      "Batch 209 loss: 22423.35379453609\n",
      "Batch 210 loss: 27748.507004163308\n",
      "Batch 211 loss: 30986.048331640337\n",
      "Batch 212 loss: 38307.421211176974\n",
      "Batch 213 loss: 63186.72077966154\n",
      "Batch 214 loss: 37540.260683801025\n",
      "Batch 215 loss: 32916.016918416586\n",
      "Batch 216 loss: 31257.156063764676\n",
      "Batch 217 loss: 43673.45201094757\n",
      "Batch 218 loss: 29555.078791695152\n",
      "Batch 219 loss: 30204.65666865608\n",
      "Batch 220 loss: 37096.673459787045\n",
      "Batch 221 loss: 35384.563106871916\n",
      "Batch 222 loss: 32078.53282255294\n",
      "Batch 223 loss: 26494.782646787306\n",
      "Batch 224 loss: 37449.77752906733\n",
      "Batch 225 loss: 42154.7939419639\n",
      "Batch 226 loss: 34888.95835482089\n",
      "Batch 227 loss: 35497.62441347146\n",
      "Batch 228 loss: 37937.89691907496\n",
      "Batch 229 loss: 47275.22015985498\n",
      "Batch 230 loss: 52523.17553719582\n",
      "Batch 231 loss: 50414.24205734579\n",
      "Batch 232 loss: 38059.42175307163\n",
      "Batch 233 loss: 42672.77040167596\n",
      "Batch 234 loss: 34829.73898381235\n",
      "Batch 235 loss: 48626.68966241734\n",
      "Batch 236 loss: 37198.87066951295\n",
      "Batch 237 loss: 39481.96671183604\n",
      "Batch 238 loss: 24905.0621428273\n",
      "Batch 239 loss: 33678.77001678654\n",
      "Batch 240 loss: 44385.44494716235\n",
      "Batch 241 loss: 41556.686900107736\n",
      "Batch 242 loss: 40972.548387722476\n",
      "Batch 243 loss: 42376.02990826722\n",
      "Batch 244 loss: 40761.670502647976\n",
      "Batch 245 loss: 28895.360523829448\n",
      "Batch 246 loss: 49372.78291772282\n",
      "Batch 247 loss: 35064.25713506543\n",
      "Batch 248 loss: 41603.390360652695\n",
      "Batch 249 loss: 38619.38265974757\n",
      "### Epoch 7\n",
      "Batch 0 loss: 25101.62800861466\n",
      "Batch 1 loss: 34446.74000909858\n",
      "Batch 2 loss: 25735.78380998761\n",
      "Batch 3 loss: 34856.72660851776\n",
      "Batch 4 loss: 25982.648719930803\n",
      "Batch 5 loss: 31451.635352224956\n",
      "Batch 6 loss: 37589.53798737784\n",
      "Batch 7 loss: 49691.39627475253\n",
      "Batch 8 loss: 24977.549073985392\n",
      "Batch 9 loss: 32137.873304869034\n",
      "Batch 10 loss: 24137.351978316652\n",
      "Batch 11 loss: 35502.126558720665\n",
      "Batch 12 loss: 53674.61961239177\n",
      "Batch 13 loss: 31073.714400575554\n",
      "Batch 14 loss: 16675.07681388539\n",
      "Batch 15 loss: 42990.36720729633\n",
      "Batch 16 loss: 36206.90748364667\n",
      "Batch 17 loss: 34612.00540880847\n",
      "Batch 18 loss: 33067.98466625397\n",
      "Batch 19 loss: 32754.23690194356\n",
      "Batch 20 loss: 43462.17811459633\n",
      "Batch 21 loss: 29827.7564018956\n",
      "Batch 22 loss: 30383.487744726055\n",
      "Batch 23 loss: 48547.74803810297\n",
      "Batch 24 loss: 31030.306505149852\n",
      "Batch 25 loss: 41153.473828892544\n",
      "Batch 26 loss: 31957.628440368135\n",
      "Batch 27 loss: 36469.66683361924\n",
      "Batch 28 loss: 57276.000931489776\n",
      "Batch 29 loss: 32416.624319459086\n",
      "Batch 30 loss: 40866.77090283979\n",
      "Batch 31 loss: 33639.26671689621\n",
      "Batch 32 loss: 34642.070610350536\n",
      "Batch 33 loss: 31134.625767163183\n",
      "Batch 34 loss: 24389.91596432851\n",
      "Batch 35 loss: 67937.78527832268\n",
      "Batch 36 loss: 43299.648018786786\n",
      "Batch 37 loss: 25484.336503511084\n",
      "Batch 38 loss: 40052.78898883311\n",
      "Batch 39 loss: 39215.79094549845\n",
      "Batch 40 loss: 61855.31990357513\n",
      "Batch 41 loss: 45125.260395804675\n",
      "Batch 42 loss: 38294.02602336072\n",
      "Batch 43 loss: 34555.29392940283\n",
      "Batch 44 loss: 43455.89222710331\n",
      "Batch 45 loss: 30207.421574100103\n",
      "Batch 46 loss: 42957.94906430274\n",
      "Batch 47 loss: 39066.38254535649\n",
      "Batch 48 loss: 54763.31189347179\n",
      "Batch 49 loss: 29124.08205658337\n",
      "Batch 50 loss: 53330.443317968085\n",
      "Batch 51 loss: 38925.43273274356\n",
      "Batch 52 loss: 37987.64634613741\n",
      "Batch 53 loss: 46008.266381737456\n",
      "Batch 54 loss: 36519.560148832905\n",
      "Batch 55 loss: 37366.980585083256\n",
      "Batch 56 loss: 51222.1813668169\n",
      "Batch 57 loss: 28140.07241577847\n",
      "Batch 58 loss: 39357.23725918452\n",
      "Batch 59 loss: 45882.042484237216\n",
      "Batch 60 loss: 21093.74957490882\n",
      "Batch 61 loss: 34788.9489133688\n",
      "Batch 62 loss: 51535.69185606978\n",
      "Batch 63 loss: 35520.784267772244\n",
      "Batch 64 loss: 62317.37616972638\n",
      "Batch 65 loss: 31395.700802396364\n",
      "Batch 66 loss: 29331.541200601056\n",
      "Batch 67 loss: 59409.845684606895\n",
      "Batch 68 loss: 30403.945636597644\n",
      "Batch 69 loss: 33000.465372223574\n",
      "Batch 70 loss: 43115.497961555244\n",
      "Batch 71 loss: 26304.082201795005\n",
      "Batch 72 loss: 27698.628223356292\n",
      "Batch 73 loss: 30870.308167636053\n",
      "Batch 74 loss: 36887.17917549165\n",
      "Batch 75 loss: 45608.08348279088\n",
      "Batch 76 loss: 52785.790689661175\n",
      "Batch 77 loss: 36754.19397166835\n",
      "Batch 78 loss: 38121.803644739164\n",
      "Batch 79 loss: 30144.346900251443\n",
      "Batch 80 loss: 27821.868695110672\n",
      "Batch 81 loss: 22463.262648074582\n",
      "Batch 82 loss: 41221.3370817074\n",
      "Batch 83 loss: 41674.868649848715\n",
      "Batch 84 loss: 26095.797336512\n",
      "Batch 85 loss: 70276.43858306712\n",
      "Batch 86 loss: 37263.59222355213\n",
      "Batch 87 loss: 30470.304285083555\n",
      "Batch 88 loss: 27283.42669645329\n",
      "Batch 89 loss: 44859.03404420536\n",
      "Batch 90 loss: 32766.921789474844\n",
      "Batch 91 loss: 38596.351062506255\n",
      "Batch 92 loss: 45489.80365848022\n",
      "Batch 93 loss: 32973.33920240812\n",
      "Batch 94 loss: 31909.195879378312\n",
      "Batch 95 loss: 48743.14325486263\n",
      "Batch 96 loss: 44086.40304606968\n",
      "Batch 97 loss: 31922.189886122942\n",
      "Batch 98 loss: 27892.9288166493\n",
      "Batch 99 loss: 51151.54427746837\n",
      "Batch 100 loss: 36242.67855795508\n",
      "Batch 101 loss: 29264.073003970458\n",
      "Batch 102 loss: 33985.522119866975\n",
      "Batch 103 loss: 29161.960820632474\n",
      "Batch 104 loss: 36840.84503867203\n",
      "Batch 105 loss: 57645.49829021128\n",
      "Batch 106 loss: 28411.437276479734\n",
      "Batch 107 loss: 28782.786292215987\n",
      "Batch 108 loss: 36178.60820510742\n",
      "Batch 109 loss: 42182.13702539591\n",
      "Batch 110 loss: 23177.17478401689\n",
      "Batch 111 loss: 44441.68088167595\n",
      "Batch 112 loss: 43232.036894065706\n",
      "Batch 113 loss: 70058.42015992117\n",
      "Batch 114 loss: 32406.112384213196\n",
      "Batch 115 loss: 36230.906167752946\n",
      "Batch 116 loss: 35748.90289657911\n",
      "Batch 117 loss: 21201.97996875699\n",
      "Batch 118 loss: 38930.18701995322\n",
      "Batch 119 loss: 22287.44179808318\n",
      "Batch 120 loss: 20461.74113283818\n",
      "Batch 121 loss: 26345.913428890737\n",
      "Batch 122 loss: 38177.04780347041\n",
      "Batch 123 loss: 47485.16977159325\n",
      "Batch 124 loss: 34132.882552054885\n",
      "Batch 125 loss: 38760.5682010461\n",
      "Batch 126 loss: 45118.16844672258\n",
      "Batch 127 loss: 45232.88076406392\n",
      "Batch 128 loss: 36850.02520279105\n",
      "Batch 129 loss: 23039.745162587784\n",
      "Batch 130 loss: 43444.5546295706\n",
      "Batch 131 loss: 50473.41894569587\n",
      "Batch 132 loss: 35215.271020210464\n",
      "Batch 133 loss: 51767.498920498605\n",
      "Batch 134 loss: 45229.31578340686\n",
      "Batch 135 loss: 48410.41034793158\n",
      "Batch 136 loss: 44308.49938035453\n",
      "Batch 137 loss: 33003.78952353395\n",
      "Batch 138 loss: 25940.823186916277\n",
      "Batch 139 loss: 43406.53862539111\n",
      "Batch 140 loss: 37918.594210495416\n",
      "Batch 141 loss: 30751.063365040485\n",
      "Batch 142 loss: 21311.89309090664\n",
      "Batch 143 loss: 67732.67917493713\n",
      "Batch 144 loss: 24957.822378736815\n",
      "Batch 145 loss: 36392.353305507364\n",
      "Batch 146 loss: 38560.180643918226\n",
      "Batch 147 loss: 38250.123580843574\n",
      "Batch 148 loss: 49283.04877565432\n",
      "Batch 149 loss: 36859.89612876745\n",
      "Batch 150 loss: 64108.97189421277\n",
      "Batch 151 loss: 43561.32794494153\n",
      "Batch 152 loss: 39952.80974005513\n",
      "Batch 153 loss: 52003.496458522975\n",
      "Batch 154 loss: 23005.920358899584\n",
      "Batch 155 loss: 32964.99925113481\n",
      "Batch 156 loss: 37429.028423922915\n",
      "Batch 157 loss: 26382.623967818436\n",
      "Batch 158 loss: 31703.570425612066\n",
      "Batch 159 loss: 41986.02657449232\n",
      "Batch 160 loss: 39610.47173040897\n",
      "Batch 161 loss: 49445.74843500722\n",
      "Batch 162 loss: 18719.559124512376\n",
      "Batch 163 loss: 38409.503636387686\n",
      "Batch 164 loss: 38697.944727971284\n",
      "Batch 165 loss: 43806.26487956074\n",
      "Batch 166 loss: 37191.399318229174\n",
      "Batch 167 loss: 44551.14033407447\n",
      "Batch 168 loss: 42017.14917739014\n",
      "Batch 169 loss: 43947.217053655506\n",
      "Batch 170 loss: 39963.6109708751\n",
      "Batch 171 loss: 53088.251909582475\n",
      "Batch 172 loss: 32190.92396240364\n",
      "Batch 173 loss: 35808.308827933106\n",
      "Batch 174 loss: 48447.41811116021\n",
      "Batch 175 loss: 26037.034272089193\n",
      "Batch 176 loss: 40557.45421987245\n",
      "Batch 177 loss: 39487.418102170894\n",
      "Batch 178 loss: 27644.02942879606\n",
      "Batch 179 loss: 28468.543793065917\n",
      "Batch 180 loss: 55470.57533586961\n",
      "Batch 181 loss: 34626.88889902373\n",
      "Batch 182 loss: 26993.153763342103\n",
      "Batch 183 loss: 35216.440105991554\n",
      "Batch 184 loss: 42103.58477651332\n",
      "Batch 185 loss: 35737.64985069229\n",
      "Batch 186 loss: 33714.44860445376\n",
      "Batch 187 loss: 40126.12922038764\n",
      "Batch 188 loss: 37979.46914811443\n",
      "Batch 189 loss: 44733.99928386652\n",
      "Batch 190 loss: 30839.0889738581\n",
      "Batch 191 loss: 24365.547578097725\n",
      "Batch 192 loss: 34394.30887780426\n",
      "Batch 193 loss: 49493.34736887249\n",
      "Batch 194 loss: 29613.02401158824\n",
      "Batch 195 loss: 34998.031856506124\n",
      "Batch 196 loss: 43045.17202120574\n",
      "Batch 197 loss: 26088.691161339288\n",
      "Batch 198 loss: 46440.770937603666\n",
      "Batch 199 loss: 41919.21773391945\n",
      "Batch 200 loss: 42337.88977441826\n",
      "Batch 201 loss: 32698.39835919183\n",
      "Batch 202 loss: 25075.335850839954\n",
      "Batch 203 loss: 28475.556398897512\n",
      "Batch 204 loss: 42973.002761613905\n",
      "Batch 205 loss: 38978.024548776346\n",
      "Batch 206 loss: 30573.15938699532\n",
      "Batch 207 loss: 12362.650634774727\n",
      "Batch 208 loss: 35489.74568006177\n",
      "Batch 209 loss: 40286.79908291408\n",
      "Batch 210 loss: 38220.99564337491\n",
      "Batch 211 loss: 23072.446906913523\n",
      "Batch 212 loss: 50065.90183280947\n",
      "Batch 213 loss: 40909.80684149534\n",
      "Batch 214 loss: 40153.68279710975\n",
      "Batch 215 loss: 63330.73832806594\n",
      "Batch 216 loss: 29010.03812859813\n",
      "Batch 217 loss: 24607.35750553017\n",
      "Batch 218 loss: 35812.45035620178\n",
      "Batch 219 loss: 39399.42915280723\n",
      "Batch 220 loss: 37471.095409377805\n",
      "Batch 221 loss: 33989.675750554554\n",
      "Batch 222 loss: 42813.06433601335\n",
      "Batch 223 loss: 46629.90833747851\n",
      "Batch 224 loss: 42108.30729261708\n",
      "Batch 225 loss: 43187.26797388478\n",
      "Batch 226 loss: 47739.905631188696\n",
      "Batch 227 loss: 47492.45285148941\n",
      "Batch 228 loss: 41400.34854759515\n",
      "Batch 229 loss: 36936.705188641376\n",
      "Batch 230 loss: 55232.937812719785\n",
      "Batch 231 loss: 39997.786188949816\n",
      "Batch 232 loss: 35478.47947694149\n",
      "Batch 233 loss: 48779.66875760059\n",
      "Batch 234 loss: 42176.676394008384\n",
      "Batch 235 loss: 28005.560540321792\n",
      "Batch 236 loss: 50685.34970372801\n",
      "Batch 237 loss: 47333.3195432791\n",
      "Batch 238 loss: 45724.72471983243\n",
      "Batch 239 loss: 45449.52130127067\n",
      "Batch 240 loss: 33681.91341038566\n",
      "Batch 241 loss: 50374.661868754614\n",
      "Batch 242 loss: 40441.95714523671\n",
      "Batch 243 loss: 40063.62029500684\n",
      "Batch 244 loss: 40804.05036593592\n",
      "Batch 245 loss: 30914.08821046641\n",
      "Batch 246 loss: 44277.962111341556\n",
      "Batch 247 loss: 36069.403441988165\n",
      "Batch 248 loss: 29667.66610966719\n",
      "Batch 249 loss: 29090.58376503206\n",
      "### Epoch 8\n",
      "Batch 0 loss: 39050.00341061732\n",
      "Batch 1 loss: 59847.73256224718\n",
      "Batch 2 loss: 43401.00823636395\n",
      "Batch 3 loss: 35193.260151577866\n",
      "Batch 4 loss: 23771.68985504801\n",
      "Batch 5 loss: 31875.87558169739\n",
      "Batch 6 loss: 39875.924666548446\n",
      "Batch 7 loss: 31629.87452037041\n",
      "Batch 8 loss: 45634.3052725446\n",
      "Batch 9 loss: 43645.48605906798\n",
      "Batch 10 loss: 47598.1800232177\n",
      "Batch 11 loss: 51750.9967906714\n",
      "Batch 12 loss: 35559.150756635245\n",
      "Batch 13 loss: 35600.86271355148\n",
      "Batch 14 loss: 49986.704628116495\n",
      "Batch 15 loss: 63221.52796699421\n",
      "Batch 16 loss: 43614.39347860765\n",
      "Batch 17 loss: 40587.17099723007\n",
      "Batch 18 loss: 41614.267042568805\n",
      "Batch 19 loss: 34498.95672022144\n",
      "Batch 20 loss: 46435.05581141083\n",
      "Batch 21 loss: 37226.19979050108\n",
      "Batch 22 loss: 26982.884732730123\n",
      "Batch 23 loss: 27659.690692103504\n",
      "Batch 24 loss: 35914.74214289716\n",
      "Batch 25 loss: 29612.696077546167\n",
      "Batch 26 loss: 38022.97213090552\n",
      "Batch 27 loss: 23854.100378456365\n",
      "Batch 28 loss: 37062.26999633937\n",
      "Batch 29 loss: 45629.30515479665\n",
      "Batch 30 loss: 41671.612456804694\n",
      "Batch 31 loss: 35425.74911223208\n",
      "Batch 32 loss: 38012.26255240359\n",
      "Batch 33 loss: 33123.42465065942\n",
      "Batch 34 loss: 62087.38583307492\n",
      "Batch 35 loss: 60961.18644862815\n",
      "Batch 36 loss: 25627.782424677112\n",
      "Batch 37 loss: 30052.288372722513\n",
      "Batch 38 loss: 31383.689029223864\n",
      "Batch 39 loss: 34036.93485426355\n",
      "Batch 40 loss: 44418.32248034204\n",
      "Batch 41 loss: 29531.361424180664\n",
      "Batch 42 loss: 26820.492834088007\n",
      "Batch 43 loss: 19732.980730785064\n",
      "Batch 44 loss: 60544.67572331015\n",
      "Batch 45 loss: 47216.43214815859\n",
      "Batch 46 loss: 33480.52625144129\n",
      "Batch 47 loss: 35660.34761147022\n",
      "Batch 48 loss: 43159.48710736609\n",
      "Batch 49 loss: 21445.72251934853\n",
      "Batch 50 loss: 29919.78762315855\n",
      "Batch 51 loss: 32269.99883869884\n",
      "Batch 52 loss: 47750.053459701754\n",
      "Batch 53 loss: 38140.163339976614\n",
      "Batch 54 loss: 31648.16216265021\n",
      "Batch 55 loss: 43996.99008474804\n",
      "Batch 56 loss: 49762.241752795504\n",
      "Batch 57 loss: 18316.212689771986\n",
      "Batch 58 loss: 56039.20768885802\n",
      "Batch 59 loss: 31010.986114294625\n",
      "Batch 60 loss: 39505.592658399444\n",
      "Batch 61 loss: 53733.52394539405\n",
      "Batch 62 loss: 50396.35383357778\n",
      "Batch 63 loss: 34475.09513013997\n",
      "Batch 64 loss: 40027.962153237095\n",
      "Batch 65 loss: 43443.46799050291\n",
      "Batch 66 loss: 38559.35167593244\n",
      "Batch 67 loss: 24733.49108091977\n",
      "Batch 68 loss: 34010.485988243774\n",
      "Batch 69 loss: 24960.88238153199\n",
      "Batch 70 loss: 31534.423657579788\n",
      "Batch 71 loss: 44225.97804578701\n",
      "Batch 72 loss: 28564.56434521347\n",
      "Batch 73 loss: 37509.70371222371\n",
      "Batch 74 loss: 34816.1734948809\n",
      "Batch 75 loss: 44315.8121653155\n",
      "Batch 76 loss: 30898.671907736374\n",
      "Batch 77 loss: 36036.864726869724\n",
      "Batch 78 loss: 30087.402649376134\n",
      "Batch 79 loss: 40907.31892297705\n",
      "Batch 80 loss: 40432.099617664746\n",
      "Batch 81 loss: 46970.94992491288\n",
      "Batch 82 loss: 56370.93526476456\n",
      "Batch 83 loss: 38095.836636539694\n",
      "Batch 84 loss: 44266.767952329144\n",
      "Batch 85 loss: 44903.318040752565\n",
      "Batch 86 loss: 37060.37492219428\n",
      "Batch 87 loss: 33652.176686044506\n",
      "Batch 88 loss: 28487.446645179327\n",
      "Batch 89 loss: 49000.88915316694\n",
      "Batch 90 loss: 44721.60388519704\n",
      "Batch 91 loss: 37930.61243802039\n",
      "Batch 92 loss: 35225.77690287291\n",
      "Batch 93 loss: 22757.699820651345\n",
      "Batch 94 loss: 38677.847202397854\n",
      "Batch 95 loss: 36519.464144620906\n",
      "Batch 96 loss: 35467.18238326437\n",
      "Batch 97 loss: 35808.52666289229\n",
      "Batch 98 loss: 42867.79760073098\n",
      "Batch 99 loss: 47715.43073282659\n",
      "Batch 100 loss: 32800.41405807962\n",
      "Batch 101 loss: 56468.637889749254\n",
      "Batch 102 loss: 34921.76450675375\n",
      "Batch 103 loss: 31927.706249732226\n",
      "Batch 104 loss: 41253.12302913319\n",
      "Batch 105 loss: 37951.87813201946\n",
      "Batch 106 loss: 43359.0836364362\n",
      "Batch 107 loss: 34435.2515190458\n",
      "Batch 108 loss: 54155.442032021594\n",
      "Batch 109 loss: 60799.39677023041\n",
      "Batch 110 loss: 45773.581002780986\n",
      "Batch 111 loss: 37691.73667054185\n",
      "Batch 112 loss: 52927.434409437046\n",
      "Batch 113 loss: 28321.391481201757\n",
      "Batch 114 loss: 56684.21543958028\n",
      "Batch 115 loss: 24576.547039448033\n",
      "Batch 116 loss: 56261.16444691331\n",
      "Batch 117 loss: 32090.598227705468\n",
      "Batch 118 loss: 19476.70136573512\n",
      "Batch 119 loss: 26321.566593624502\n",
      "Batch 120 loss: 30173.434132774728\n",
      "Batch 121 loss: 48221.74003119375\n",
      "Batch 122 loss: 41006.60297728234\n",
      "Batch 123 loss: 19855.038945185708\n",
      "Batch 124 loss: 38446.40290270939\n",
      "Batch 125 loss: 37351.03189712581\n",
      "Batch 126 loss: 51616.737296139494\n",
      "Batch 127 loss: 38498.60506719819\n",
      "Batch 128 loss: 32848.182900755346\n",
      "Batch 129 loss: 34412.494417373025\n",
      "Batch 130 loss: 36811.14020423684\n",
      "Batch 131 loss: 19456.887192108898\n",
      "Batch 132 loss: 23866.572804149673\n",
      "Batch 133 loss: 24900.959369786506\n",
      "Batch 134 loss: 34199.566614933385\n",
      "Batch 135 loss: 40935.899606856576\n",
      "Batch 136 loss: 50452.93573692044\n",
      "Batch 137 loss: 33341.18845699977\n",
      "Batch 138 loss: 54681.11192229109\n",
      "Batch 139 loss: 43140.2124914245\n",
      "Batch 140 loss: 41626.52074633276\n",
      "Batch 141 loss: 29865.986171478635\n",
      "Batch 142 loss: 35588.17064901905\n",
      "Batch 143 loss: 46658.80560774843\n",
      "Batch 144 loss: 35021.27646345609\n",
      "Batch 145 loss: 38425.24082843431\n",
      "Batch 146 loss: 39414.07394211068\n",
      "Batch 147 loss: 37665.82811765258\n",
      "Batch 148 loss: 30066.550839711846\n",
      "Batch 149 loss: 35849.71863781016\n",
      "Batch 150 loss: 36564.40847420774\n",
      "Batch 151 loss: 40062.17765715157\n",
      "Batch 152 loss: 56711.40451301616\n",
      "Batch 153 loss: 23629.961282159344\n",
      "Batch 154 loss: 21739.583483969593\n",
      "Batch 155 loss: 30289.03742574278\n",
      "Batch 156 loss: 29903.175723131866\n",
      "Batch 157 loss: 35176.95790328932\n",
      "Batch 158 loss: 33070.33598025595\n",
      "Batch 159 loss: 38132.839345471926\n",
      "Batch 160 loss: 28097.297555063888\n",
      "Batch 161 loss: 22710.262643020185\n",
      "Batch 162 loss: 43646.45976271309\n",
      "Batch 163 loss: 33616.55384811455\n",
      "Batch 164 loss: 37600.83490682719\n",
      "Batch 165 loss: 47341.260305731266\n",
      "Batch 166 loss: 41089.671963259956\n",
      "Batch 167 loss: 29637.97199807702\n",
      "Batch 168 loss: 33443.66416873525\n",
      "Batch 169 loss: 33233.552508652836\n",
      "Batch 170 loss: 28847.64719000285\n",
      "Batch 171 loss: 29380.908861863893\n",
      "Batch 172 loss: 33256.6706032464\n",
      "Batch 173 loss: 33795.276757626816\n",
      "Batch 174 loss: 50007.03883321097\n",
      "Batch 175 loss: 35659.543109167695\n",
      "Batch 176 loss: 37884.33154919378\n",
      "Batch 177 loss: 40096.088337359804\n",
      "Batch 178 loss: 27502.63527083785\n",
      "Batch 179 loss: 37921.928084258376\n",
      "Batch 180 loss: 41260.86546394446\n",
      "Batch 181 loss: 43050.212389006636\n",
      "Batch 182 loss: 31775.869170734775\n",
      "Batch 183 loss: 45679.15886803431\n",
      "Batch 184 loss: 34949.82821763639\n",
      "Batch 185 loss: 28977.805867218412\n",
      "Batch 186 loss: 35030.435689046695\n",
      "Batch 187 loss: 50366.248885490306\n",
      "Batch 188 loss: 22948.94558517075\n",
      "Batch 189 loss: 38292.48928555107\n",
      "Batch 190 loss: 28923.244383206973\n",
      "Batch 191 loss: 39275.39028428059\n",
      "Batch 192 loss: 39624.52767097431\n",
      "Batch 193 loss: 40479.773203999765\n",
      "Batch 194 loss: 31470.880994865875\n",
      "Batch 195 loss: 26178.316356868374\n",
      "Batch 196 loss: 40733.70457444756\n",
      "Batch 197 loss: 38113.29069267062\n",
      "Batch 198 loss: 46257.64474502165\n",
      "Batch 199 loss: 31750.21249969526\n",
      "Batch 200 loss: 36957.39270753803\n",
      "Batch 201 loss: 45191.25351880696\n",
      "Batch 202 loss: 26676.311142617393\n",
      "Batch 203 loss: 25232.059244279466\n",
      "Batch 204 loss: 42017.6416953215\n",
      "Batch 205 loss: 29883.70291176985\n",
      "Batch 206 loss: 53135.33053119251\n",
      "Batch 207 loss: 41342.30186910735\n",
      "Batch 208 loss: 48599.42403584262\n",
      "Batch 209 loss: 38545.25440823205\n",
      "Batch 210 loss: 30468.33642834757\n",
      "Batch 211 loss: 35748.625988799686\n",
      "Batch 212 loss: 32679.38805984016\n",
      "Batch 213 loss: 46726.06725210068\n",
      "Batch 214 loss: 35853.835756439235\n",
      "Batch 215 loss: 30330.256153470975\n",
      "Batch 216 loss: 57982.38339566907\n",
      "Batch 217 loss: 35025.05451209417\n",
      "Batch 218 loss: 40244.93241857886\n",
      "Batch 219 loss: 35219.78345782951\n",
      "Batch 220 loss: 29857.657260379267\n",
      "Batch 221 loss: 67127.28215429116\n",
      "Batch 222 loss: 48250.01813611628\n",
      "Batch 223 loss: 35098.42022689089\n",
      "Batch 224 loss: 31496.68654737539\n",
      "Batch 225 loss: 38215.18168516368\n",
      "Batch 226 loss: 34984.52143533335\n",
      "Batch 227 loss: 43789.8772896777\n",
      "Batch 228 loss: 49331.168464454975\n",
      "Batch 229 loss: 33419.726673311256\n",
      "Batch 230 loss: 28136.04676441397\n",
      "Batch 231 loss: 40140.85763174379\n",
      "Batch 232 loss: 37766.69741780187\n",
      "Batch 233 loss: 35470.63378776898\n",
      "Batch 234 loss: 23929.775902822574\n",
      "Batch 235 loss: 59149.47199298403\n",
      "Batch 236 loss: 21515.806179136107\n",
      "Batch 237 loss: 49852.013490625104\n",
      "Batch 238 loss: 29293.763177605568\n",
      "Batch 239 loss: 47355.78896885404\n",
      "Batch 240 loss: 30306.361331415064\n",
      "Batch 241 loss: 34644.01817353473\n",
      "Batch 242 loss: 22814.745882368912\n",
      "Batch 243 loss: 48806.69816683431\n",
      "Batch 244 loss: 34348.23444827161\n",
      "Batch 245 loss: 59080.16263605078\n",
      "Batch 246 loss: 38928.55651508428\n",
      "Batch 247 loss: 27440.822850116274\n",
      "Batch 248 loss: 38273.44286025907\n",
      "Batch 249 loss: 67400.92932490198\n",
      "### Epoch 9\n",
      "Batch 0 loss: 61942.28307029269\n",
      "Batch 1 loss: 29020.195518796387\n",
      "Batch 2 loss: 38856.51131238199\n",
      "Batch 3 loss: 48617.001798071775\n",
      "Batch 4 loss: 46718.392632431394\n",
      "Batch 5 loss: 49172.3534459502\n",
      "Batch 6 loss: 36108.62236541422\n",
      "Batch 7 loss: 28643.322652856\n",
      "Batch 8 loss: 45948.50158835712\n",
      "Batch 9 loss: 40631.231812269936\n",
      "Batch 10 loss: 49049.59761070678\n",
      "Batch 11 loss: 36516.92244836372\n",
      "Batch 12 loss: 23309.617085918082\n",
      "Batch 13 loss: 49849.82400279487\n",
      "Batch 14 loss: 36281.73530622599\n",
      "Batch 15 loss: 33873.62801779216\n",
      "Batch 16 loss: 36869.02297448186\n",
      "Batch 17 loss: 35991.433216945996\n",
      "Batch 18 loss: 45495.58452448694\n",
      "Batch 19 loss: 37625.58361160902\n",
      "Batch 20 loss: 27861.62433541978\n",
      "Batch 21 loss: 32515.22774680607\n",
      "Batch 22 loss: 41529.44478783193\n",
      "Batch 23 loss: 40284.3097799673\n",
      "Batch 24 loss: 42755.82681726046\n",
      "Batch 25 loss: 34344.40866888223\n",
      "Batch 26 loss: 56724.232168292685\n",
      "Batch 27 loss: 34327.21372847671\n",
      "Batch 28 loss: 27834.086723844477\n",
      "Batch 29 loss: 34329.12096107038\n",
      "Batch 30 loss: 53136.81721617071\n",
      "Batch 31 loss: 48888.919792108114\n",
      "Batch 32 loss: 23514.906644775594\n",
      "Batch 33 loss: 25869.631314471673\n",
      "Batch 34 loss: 40692.874508689856\n",
      "Batch 35 loss: 41161.89275874491\n",
      "Batch 36 loss: 36839.60777107984\n",
      "Batch 37 loss: 29216.71126808838\n",
      "Batch 38 loss: 20722.657745943936\n",
      "Batch 39 loss: 44169.70930576332\n",
      "Batch 40 loss: 26628.208266361493\n",
      "Batch 41 loss: 32028.15877607202\n",
      "Batch 42 loss: 35001.350608587105\n",
      "Batch 43 loss: 61427.95017464271\n",
      "Batch 44 loss: 36813.47050269964\n",
      "Batch 45 loss: 32886.95587893605\n",
      "Batch 46 loss: 36713.00573861181\n",
      "Batch 47 loss: 32292.076348785195\n",
      "Batch 48 loss: 40112.88171770584\n",
      "Batch 49 loss: 38060.059542120835\n",
      "Batch 50 loss: 29081.10615573873\n",
      "Batch 51 loss: 34530.54311710592\n",
      "Batch 52 loss: 27089.58642633723\n",
      "Batch 53 loss: 33153.47665745758\n",
      "Batch 54 loss: 29318.02975474068\n",
      "Batch 55 loss: 37516.04883382071\n",
      "Batch 56 loss: 31875.41214419196\n",
      "Batch 57 loss: 41922.81317751348\n",
      "Batch 58 loss: 29376.5822014106\n",
      "Batch 59 loss: 22803.880851480946\n",
      "Batch 60 loss: 37956.16733027241\n",
      "Batch 61 loss: 23718.470365768\n",
      "Batch 62 loss: 42409.20961029163\n",
      "Batch 63 loss: 39748.42331785972\n",
      "Batch 64 loss: 35859.95804766777\n",
      "Batch 65 loss: 30641.05246047485\n",
      "Batch 66 loss: 35919.97856224544\n",
      "Batch 67 loss: 39446.59286150981\n",
      "Batch 68 loss: 35107.33693464608\n",
      "Batch 69 loss: 35408.635109387156\n",
      "Batch 70 loss: 51261.10613020602\n",
      "Batch 71 loss: 30410.858317603088\n",
      "Batch 72 loss: 27090.9710822406\n",
      "Batch 73 loss: 34409.92095281168\n",
      "Batch 74 loss: 47075.558017872594\n",
      "Batch 75 loss: 23846.87561032929\n",
      "Batch 76 loss: 31731.235589889053\n",
      "Batch 77 loss: 49327.037227070075\n",
      "Batch 78 loss: 31936.863011672\n",
      "Batch 79 loss: 37610.847741024176\n",
      "Batch 80 loss: 40957.60990620019\n",
      "Batch 81 loss: 50913.70664309299\n",
      "Batch 82 loss: 52381.13555566133\n",
      "Batch 83 loss: 27303.966902604818\n",
      "Batch 84 loss: 27609.083624004033\n",
      "Batch 85 loss: 33757.29135011314\n",
      "Batch 86 loss: 35185.16790597405\n",
      "Batch 87 loss: 46484.51124751135\n",
      "Batch 88 loss: 56468.91180796386\n",
      "Batch 89 loss: 37052.15849323747\n",
      "Batch 90 loss: 47986.117893522875\n",
      "Batch 91 loss: 27877.085282998447\n",
      "Batch 92 loss: 25660.703436871772\n",
      "Batch 93 loss: 40703.94788501591\n",
      "Batch 94 loss: 35549.459639253924\n",
      "Batch 95 loss: 34614.4573797642\n",
      "Batch 96 loss: 35988.397552054776\n",
      "Batch 97 loss: 24812.30295525465\n",
      "Batch 98 loss: 63224.56835025665\n",
      "Batch 99 loss: 27161.801576970793\n",
      "Batch 100 loss: 41282.05323804803\n",
      "Batch 101 loss: 21748.352794064915\n",
      "Batch 102 loss: 34204.33806622909\n",
      "Batch 103 loss: 53297.514252080844\n",
      "Batch 104 loss: 44933.30325022807\n",
      "Batch 105 loss: 48628.62057995665\n",
      "Batch 106 loss: 43423.96327899809\n",
      "Batch 107 loss: 48210.13249406278\n",
      "Batch 108 loss: 37419.4923147681\n",
      "Batch 109 loss: 45787.0756595035\n",
      "Batch 110 loss: 30778.20358178176\n",
      "Batch 111 loss: 27879.386509470787\n",
      "Batch 112 loss: 47528.48414060601\n",
      "Batch 113 loss: 40257.005843484185\n",
      "Batch 114 loss: 38421.258919397624\n",
      "Batch 115 loss: 35805.66379935428\n",
      "Batch 116 loss: 46865.884569675836\n",
      "Batch 117 loss: 69331.14903357069\n",
      "Batch 118 loss: 36830.91591849743\n",
      "Batch 119 loss: 38953.277576262844\n",
      "Batch 120 loss: 37964.552874547444\n",
      "Batch 121 loss: 45746.94931360428\n",
      "Batch 122 loss: 67797.11223962234\n",
      "Batch 123 loss: 29672.29618193403\n",
      "Batch 124 loss: 28095.67594061318\n",
      "Batch 125 loss: 29104.59014729673\n",
      "Batch 126 loss: 40325.56598127751\n",
      "Batch 127 loss: 28970.06566715806\n",
      "Batch 128 loss: 39550.79444919111\n",
      "Batch 129 loss: 24050.53353952674\n",
      "Batch 130 loss: 59496.79256435277\n",
      "Batch 131 loss: 50010.6770341073\n",
      "Batch 132 loss: 33780.411147539104\n",
      "Batch 133 loss: 22063.239524596924\n",
      "Batch 134 loss: 36436.499584249104\n",
      "Batch 135 loss: 57610.528430350125\n",
      "Batch 136 loss: 30087.311495408452\n",
      "Batch 137 loss: 38811.432448537445\n",
      "Batch 138 loss: 26683.569675240255\n",
      "Batch 139 loss: 35458.55452874094\n",
      "Batch 140 loss: 39189.807741515586\n",
      "Batch 141 loss: 36442.017166964055\n",
      "Batch 142 loss: 37375.811761940684\n",
      "Batch 143 loss: 36278.37357634038\n",
      "Batch 144 loss: 22988.284107406336\n",
      "Batch 145 loss: 55821.72514708321\n",
      "Batch 146 loss: 45165.371051192444\n",
      "Batch 147 loss: 35583.66755768524\n",
      "Batch 148 loss: 50209.800573069835\n",
      "Batch 149 loss: 28693.99830949946\n",
      "Batch 150 loss: 36340.408555386064\n",
      "Batch 151 loss: 41760.02742631205\n",
      "Batch 152 loss: 23149.027266698755\n",
      "Batch 153 loss: 35934.536660551355\n",
      "Batch 154 loss: 43651.615277758574\n",
      "Batch 155 loss: 35854.62233565652\n",
      "Batch 156 loss: 43565.11406093898\n",
      "Batch 157 loss: 31104.8355222169\n",
      "Batch 158 loss: 39011.47181989112\n",
      "Batch 159 loss: 42505.813105939844\n",
      "Batch 160 loss: 43159.652172189424\n",
      "Batch 161 loss: 34045.778288314046\n",
      "Batch 162 loss: 36063.88951445536\n",
      "Batch 163 loss: 26275.167301396843\n",
      "Batch 164 loss: 44348.08389829347\n",
      "Batch 165 loss: 48085.383281361246\n",
      "Batch 166 loss: 44473.49299972373\n",
      "Batch 167 loss: 41885.59005124366\n",
      "Batch 168 loss: 30221.862779067535\n",
      "Batch 169 loss: 57792.98410568976\n",
      "Batch 170 loss: 46456.87184306098\n",
      "Batch 171 loss: 59986.630179683016\n",
      "Batch 172 loss: 50311.08758919506\n",
      "Batch 173 loss: 45031.5684676667\n",
      "Batch 174 loss: 36972.403340725716\n",
      "Batch 175 loss: 35876.857312750646\n",
      "Batch 176 loss: 36461.41926487633\n",
      "Batch 177 loss: 36373.53825218752\n",
      "Batch 178 loss: 47626.52164028715\n",
      "Batch 179 loss: 34215.31913631643\n",
      "Batch 180 loss: 36312.68907208399\n",
      "Batch 181 loss: 42068.02313680546\n",
      "Batch 182 loss: 47219.20862687402\n",
      "Batch 183 loss: 25808.892316351677\n",
      "Batch 184 loss: 29355.05938333305\n",
      "Batch 185 loss: 39067.6813802608\n",
      "Batch 186 loss: 35710.19703112287\n",
      "Batch 187 loss: 28933.054470730316\n",
      "Batch 188 loss: 47974.70464664072\n",
      "Batch 189 loss: 32514.40936966711\n",
      "Batch 190 loss: 63829.103910336904\n",
      "Batch 191 loss: 50704.167458316784\n",
      "Batch 192 loss: 28859.57580895208\n",
      "Batch 193 loss: 24767.03108860461\n",
      "Batch 194 loss: 37351.98978970502\n",
      "Batch 195 loss: 37350.35865776597\n",
      "Batch 196 loss: 35426.66242051759\n",
      "Batch 197 loss: 51478.62798829715\n",
      "Batch 198 loss: 26085.92149748051\n",
      "Batch 199 loss: 32426.236152714657\n",
      "Batch 200 loss: 45577.10517773614\n",
      "Batch 201 loss: 35724.57525584477\n",
      "Batch 202 loss: 26486.86615659146\n",
      "Batch 203 loss: 26984.849016719647\n",
      "Batch 204 loss: 36623.192449380396\n",
      "Batch 205 loss: 26960.87116093263\n",
      "Batch 206 loss: 46471.98964324297\n",
      "Batch 207 loss: 35427.51010531522\n",
      "Batch 208 loss: 37264.29202479498\n",
      "Batch 209 loss: 20406.75954514304\n",
      "Batch 210 loss: 41586.692235550996\n",
      "Batch 211 loss: 29001.20060599109\n",
      "Batch 212 loss: 56556.51673149471\n",
      "Batch 213 loss: 39246.08755498913\n",
      "Batch 214 loss: 29776.04558590655\n",
      "Batch 215 loss: 36465.330089127485\n",
      "Batch 216 loss: 44759.12897485208\n",
      "Batch 217 loss: 26333.86297447239\n",
      "Batch 218 loss: 46960.21645751003\n",
      "Batch 219 loss: 30531.01154809773\n",
      "Batch 220 loss: 17848.249720260927\n",
      "Batch 221 loss: 33234.86491086526\n",
      "Batch 222 loss: 28808.951969779093\n",
      "Batch 223 loss: 37366.24516153366\n",
      "Batch 224 loss: 32722.854243536225\n",
      "Batch 225 loss: 36478.758094198936\n",
      "Batch 226 loss: 31038.285908787624\n",
      "Batch 227 loss: 24000.098445826352\n",
      "Batch 228 loss: 46264.24591287902\n",
      "Batch 229 loss: 45389.91771920303\n",
      "Batch 230 loss: 41293.426542516914\n",
      "Batch 231 loss: 38593.25491097808\n",
      "Batch 232 loss: 28562.866643174653\n",
      "Batch 233 loss: 40780.16875974843\n",
      "Batch 234 loss: 27120.37320148618\n",
      "Batch 235 loss: 35169.1911579915\n",
      "Batch 236 loss: 38028.273774475034\n",
      "Batch 237 loss: 34025.31777413448\n",
      "Batch 238 loss: 27361.091539173878\n",
      "Batch 239 loss: 40012.62025763358\n",
      "Batch 240 loss: 40638.326575143205\n",
      "Batch 241 loss: 25823.223193460923\n",
      "Batch 242 loss: 34235.053868788214\n",
      "Batch 243 loss: 38551.038398041914\n",
      "Batch 244 loss: 45366.61389121858\n",
      "Batch 245 loss: 82758.99939874654\n",
      "Batch 246 loss: 39063.05054326844\n",
      "Batch 247 loss: 30148.5408182847\n",
      "Batch 248 loss: 43252.79320242962\n",
      "Batch 249 loss: 52271.581873846066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380638.296480749"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(model, train_dl, optimizer, criterion, epochs=10):\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"### Epoch {epoch}\")\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Batch {i} loss: {loss.item()}\")\n",
    "            \n",
    "    return running_loss / len(train_dl)\n",
    "                            \n",
    "\n",
    "train(model, train_dl, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_test, output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input_data'"
     ]
    }
   ],
   "source": [
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, output.detach().numpy())\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "class GroupLayer(nn.Module):\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\n",
    "                \"Total feature size must be divisible by group feature size\")\n",
    "\n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0\n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "\n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size).double()\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1).double()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(input_data.shape)\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "\n",
    "        outs = []\n",
    "        for i, x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    TorchModel,\n",
    "    module__group_feat_size=group_feat_size,\n",
    "    module__total_feat_size=total_feat_size,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=2000, n_features=260, noise=0.01)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: group_feat_size, total_feat_size.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m25136.4242\u001b[0m    \u001b[32m28184.4529\u001b[0m  0.2992\n",
      "      2    \u001b[36m25131.6008\u001b[0m    \u001b[32m28180.9656\u001b[0m  0.4549\n",
      "      3    \u001b[36m25123.8036\u001b[0m    \u001b[32m28172.6188\u001b[0m  0.2994\n",
      "      4    \u001b[36m25107.4279\u001b[0m    \u001b[32m28155.9001\u001b[0m  0.3518\n",
      "      5    \u001b[36m25076.7573\u001b[0m    \u001b[32m28121.9955\u001b[0m  0.3075\n",
      "      6    \u001b[36m25019.4746\u001b[0m    \u001b[32m28054.9637\u001b[0m  0.3289\n",
      "      7    \u001b[36m24920.8820\u001b[0m    \u001b[32m27951.3269\u001b[0m  0.3097\n",
      "      8    \u001b[36m24765.2670\u001b[0m    \u001b[32m27787.6675\u001b[0m  0.3423\n",
      "      9    \u001b[36m24534.5411\u001b[0m    \u001b[32m27549.5329\u001b[0m  0.4599\n",
      "     10    \u001b[36m24206.8254\u001b[0m    \u001b[32m27207.2966\u001b[0m  0.3357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=TorchModel(\n",
       "    (group_layers): ModuleList(\n",
       "      (0-25): 26 x GroupLayer(\n",
       "        (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
       "    (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make prediction \n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "\n",
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n",
    "# Bring in CCLE data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in original ccle data\n",
    "loading_code = 'generic-gdsc-1-FGFR_0939-LN_IC50-fgfr4_ccle_dynamic_features-true-Row'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "feature_data_numpy = feature_data.to_numpy()\n",
    "label_data_numpy = label_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 260)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data_numpy.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
