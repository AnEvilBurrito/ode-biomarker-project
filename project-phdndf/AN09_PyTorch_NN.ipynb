{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(torch.__version__)\n",
    "    # Setup device agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0281, -0.0740, -0.0258,  ..., -0.0820,  0.0069,  0.0379],\n",
      "        [-0.0641, -0.0269, -0.0203,  ..., -0.0015, -0.0187, -0.0585],\n",
      "        [-0.0005, -0.0302, -0.0770,  ..., -0.0720, -0.0771,  0.0402],\n",
      "        ...,\n",
      "        [-0.0576,  0.0650, -0.0482,  ..., -0.0684, -0.0816, -0.0977],\n",
      "        [ 0.0176, -0.0189, -0.0947,  ...,  0.0908,  0.0155, -0.0806],\n",
      "        [ 0.0866,  0.0281,  0.0280,  ...,  0.0916,  0.0967, -0.0675]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.1949e-02, -5.9762e-03,  6.6286e-02, -8.1473e-02, -5.8782e-02,\n",
      "         6.6604e-02,  4.7256e-02,  8.8177e-02,  6.3597e-02, -7.2984e-02,\n",
      "        -5.9919e-02,  1.4836e-03,  7.8904e-02, -7.8673e-02, -9.3304e-02,\n",
      "        -7.4985e-02, -2.5742e-02, -4.3548e-02, -4.1615e-02, -4.2085e-02,\n",
      "         6.6179e-02,  1.8719e-02,  7.7365e-03, -1.2027e-02,  2.6128e-02,\n",
      "        -4.7106e-02, -4.3181e-02,  2.9833e-02, -9.9660e-02,  5.3323e-02,\n",
      "         9.4328e-02, -2.2104e-03, -3.8498e-02, -6.0734e-02,  9.2418e-02,\n",
      "        -6.5804e-02,  4.0446e-02,  2.5886e-02,  1.5883e-02, -6.7907e-02,\n",
      "        -5.6565e-02, -2.2553e-02, -5.5987e-03, -4.1577e-02,  8.4033e-02,\n",
      "        -3.8615e-02,  6.5343e-02,  9.4101e-02,  4.7978e-02, -4.4360e-02,\n",
      "        -7.0638e-02,  2.4908e-02, -1.2812e-02,  3.4365e-02, -2.7418e-02,\n",
      "         8.1122e-02,  7.0249e-02,  3.6378e-02,  4.6255e-02,  6.7147e-02,\n",
      "         4.8988e-02,  2.2777e-02, -6.6960e-02, -4.8256e-03,  3.6618e-02,\n",
      "         5.6093e-02, -5.5983e-02,  3.8849e-02, -1.0059e-02,  2.8262e-02,\n",
      "        -9.7956e-02, -4.7640e-02, -1.8694e-02,  6.0532e-02, -6.4289e-02,\n",
      "        -2.2314e-02, -9.0681e-02, -7.2561e-02, -3.8469e-03, -4.7697e-02,\n",
      "        -9.1121e-02, -7.7512e-02,  4.4123e-02, -3.5786e-02,  8.9514e-02,\n",
      "         3.1355e-02,  2.7766e-04,  6.8384e-02, -4.0351e-02, -7.1346e-03,\n",
      "         3.2855e-02,  2.1609e-02, -3.9030e-02,  6.7064e-02,  6.2284e-02,\n",
      "         2.8758e-02,  3.6044e-02,  5.8593e-03, -5.8745e-02,  9.2652e-02,\n",
      "        -3.4028e-02, -2.5750e-02,  5.1115e-02, -8.9072e-02,  4.6317e-02,\n",
      "        -8.2433e-02,  8.0062e-02,  2.9400e-02,  5.6472e-02,  1.8479e-02,\n",
      "         2.3555e-02,  9.0814e-02, -3.2522e-02,  6.9796e-02, -1.6243e-02,\n",
      "        -9.0605e-02, -9.9819e-03,  2.2731e-02, -2.3132e-02, -6.6558e-02,\n",
      "        -5.1677e-02,  7.0500e-03, -2.4546e-02,  8.8830e-02, -4.8762e-02,\n",
      "         4.3771e-04,  2.9763e-02, -2.7651e-02,  2.9765e-02,  3.5748e-02,\n",
      "         9.0800e-02,  2.2314e-02, -2.0161e-04, -3.4595e-02, -6.8306e-02,\n",
      "        -1.9090e-02, -7.0001e-03,  5.4428e-02,  1.7446e-02,  6.3675e-02,\n",
      "        -9.9947e-02,  9.0872e-02,  8.5943e-02,  8.0148e-02,  8.4784e-02,\n",
      "         1.7586e-02,  4.6640e-02, -5.8155e-02,  2.7940e-02, -6.5669e-02,\n",
      "        -5.8396e-02, -9.0715e-02,  7.2098e-02, -3.1074e-02, -7.7251e-02,\n",
      "         2.0729e-02,  8.3232e-02, -8.2398e-02,  6.9639e-03,  2.7592e-02,\n",
      "         8.4791e-03, -8.7956e-02,  4.3864e-02,  3.3489e-02, -6.3405e-02,\n",
      "         3.2836e-02, -8.0723e-03,  4.3370e-05,  9.8178e-02, -6.6931e-02,\n",
      "        -1.2669e-02,  6.6096e-02, -3.2271e-02, -9.2787e-02,  7.5122e-02,\n",
      "        -4.8462e-02, -4.1316e-04, -1.2077e-02, -1.4616e-02, -3.1357e-02,\n",
      "         4.6199e-02, -1.7989e-02, -1.2861e-02, -4.8683e-02, -4.5789e-02,\n",
      "        -9.3096e-02,  5.8019e-02,  5.1905e-02, -7.5846e-02,  6.4526e-02,\n",
      "        -7.4551e-02,  8.8322e-02, -8.4129e-02,  5.9777e-02, -2.3211e-02,\n",
      "        -5.4766e-03,  9.6377e-02, -1.8041e-02, -3.9743e-02,  5.8390e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0509, -0.0080, -0.0585,  ..., -0.0455,  0.0644, -0.0054],\n",
      "        [-0.0393, -0.0511,  0.0533,  ..., -0.0211, -0.0665, -0.0075],\n",
      "        [ 0.0538,  0.0431,  0.0194,  ..., -0.0169, -0.0601,  0.0394],\n",
      "        ...,\n",
      "        [ 0.0608,  0.0307,  0.0078,  ...,  0.0325, -0.0291, -0.0472],\n",
      "        [ 0.0501, -0.0580,  0.0692,  ...,  0.0468, -0.0637, -0.0110],\n",
      "        [ 0.0068, -0.0410, -0.0437,  ..., -0.0477,  0.0241, -0.0074]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0532,  0.0214, -0.0699, -0.0411, -0.0051,  0.0069,  0.0328,  0.0398,\n",
      "        -0.0460, -0.0535], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0509, -0.0080, -0.0585,  ..., -0.0455,  0.0644, -0.0054],\n",
      "        [-0.0393, -0.0511,  0.0533,  ..., -0.0211, -0.0665, -0.0075],\n",
      "        [ 0.0538,  0.0431,  0.0194,  ..., -0.0169, -0.0601,  0.0394],\n",
      "        ...,\n",
      "        [ 0.0608,  0.0307,  0.0078,  ...,  0.0325, -0.0291, -0.0472],\n",
      "        [ 0.0501, -0.0580,  0.0692,  ...,  0.0468, -0.0637, -0.0110],\n",
      "        [ 0.0068, -0.0410, -0.0437,  ..., -0.0477,  0.0241, -0.0074]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0532,  0.0214, -0.0699, -0.0411, -0.0051,  0.0069,  0.0328,  0.0398,\n",
      "        -0.0460, -0.0535], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "\n",
    "\n",
    "class MySmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySmallModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.network1 = MySmallModel()\n",
    "        self.network2 = MySmallModel()\n",
    "        self.network3 = MySmallModel()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc_out = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = F.relu(self.network1(x1))\n",
    "        x2 = F.relu(self.network2(x2))\n",
    "        x3 = F.relu(self.network3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "N = 10\n",
    "x1, x2, x3 = torch.randn(N, 5), torch.randn(N, 5), torch.randn(N, 5)\n",
    "\n",
    "output = model(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterisation of Feature Size and Group Feature Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1208],\n",
      "        [0.1936],\n",
      "        [0.1864],\n",
      "        [0.2696],\n",
      "        [0.0661],\n",
      "        [0.2202],\n",
      "        [0.1334],\n",
      "        [0.1326],\n",
      "        [0.2051],\n",
      "        [0.1475]], grad_fn=<AddmmBackward0>)\n",
      "TorchModel(\n",
      "  (group_layers): ModuleList(\n",
      "    (0-25): 26 x GroupLayer(\n",
      "      (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
      "  (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GroupLayer(nn.Module):\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x \n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\"Total feature size must be divisible by group feature size\")\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0 \n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "            \n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size)\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(input_data.shape)\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "        \n",
    "        outs = []\n",
    "        for i,x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n",
    "N = 10\n",
    "x = torch.randn(N, total_feat_size)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimisation Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to generate some regression data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=10000, n_features=260, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_dl = DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch 0\n",
      "Batch 0 loss: 25056.66015625\n",
      "Batch 1 loss: 37702.10546875\n",
      "Batch 2 loss: 28584.150390625\n",
      "Batch 3 loss: 30954.6640625\n",
      "Batch 4 loss: 33792.21484375\n",
      "Batch 5 loss: 18256.232421875\n",
      "Batch 6 loss: 37850.1640625\n",
      "Batch 7 loss: 13701.810546875\n",
      "Batch 8 loss: 32320.6796875\n",
      "Batch 9 loss: 28602.5078125\n",
      "Batch 10 loss: 26769.7578125\n",
      "Batch 11 loss: 32571.67578125\n",
      "Batch 12 loss: 32792.32421875\n",
      "Batch 13 loss: 34237.96484375\n",
      "Batch 14 loss: 27931.5\n",
      "Batch 15 loss: 29189.37109375\n",
      "Batch 16 loss: 25940.294921875\n",
      "Batch 17 loss: 31196.46484375\n",
      "Batch 18 loss: 25889.625\n",
      "Batch 19 loss: 35671.4375\n",
      "Batch 20 loss: 21281.140625\n",
      "Batch 21 loss: 23626.6484375\n",
      "Batch 22 loss: 26202.875\n",
      "Batch 23 loss: 22004.318359375\n",
      "Batch 24 loss: 20292.236328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25 loss: 24661.19921875\n",
      "Batch 26 loss: 17966.6796875\n",
      "Batch 27 loss: 32622.859375\n",
      "Batch 28 loss: 29955.80078125\n",
      "Batch 29 loss: 33397.1015625\n",
      "Batch 30 loss: 30747.857421875\n",
      "Batch 31 loss: 26328.142578125\n",
      "Batch 32 loss: 34294.1015625\n",
      "Batch 33 loss: 28267.208984375\n",
      "Batch 34 loss: 25186.943359375\n",
      "Batch 35 loss: 34099.72265625\n",
      "Batch 36 loss: 39143.015625\n",
      "Batch 37 loss: 17188.345703125\n",
      "Batch 38 loss: 39268.62890625\n",
      "Batch 39 loss: 40309.84375\n",
      "Batch 40 loss: 27869.5078125\n",
      "Batch 41 loss: 27278.01171875\n",
      "Batch 42 loss: 23766.109375\n",
      "Batch 43 loss: 26007.978515625\n",
      "Batch 44 loss: 33931.23046875\n",
      "Batch 45 loss: 27239.48828125\n",
      "Batch 46 loss: 29408.72265625\n",
      "Batch 47 loss: 20771.681640625\n",
      "Batch 48 loss: 32726.875\n",
      "Batch 49 loss: 24448.98046875\n",
      "Batch 50 loss: 33615.140625\n",
      "Batch 51 loss: 48517.80078125\n",
      "Batch 52 loss: 22197.162109375\n",
      "Batch 53 loss: 28221.42578125\n",
      "Batch 54 loss: 32883.109375\n",
      "Batch 55 loss: 20651.68359375\n",
      "Batch 56 loss: 52052.328125\n",
      "Batch 57 loss: 19322.005859375\n",
      "Batch 58 loss: 36047.08203125\n",
      "Batch 59 loss: 29389.76171875\n",
      "Batch 60 loss: 42623.0703125\n",
      "Batch 61 loss: 30794.36328125\n",
      "Batch 62 loss: 24642.712890625\n",
      "Batch 63 loss: 23012.0390625\n",
      "Batch 64 loss: 35401.84765625\n",
      "Batch 65 loss: 33227.3046875\n",
      "Batch 66 loss: 47207.265625\n",
      "Batch 67 loss: 25102.400390625\n",
      "Batch 68 loss: 29563.720703125\n",
      "Batch 69 loss: 26137.33203125\n",
      "Batch 70 loss: 24428.0\n",
      "Batch 71 loss: 32829.26171875\n",
      "Batch 72 loss: 38185.44921875\n",
      "Batch 73 loss: 30182.54296875\n",
      "Batch 74 loss: 24097.75390625\n",
      "Batch 75 loss: 25711.919921875\n",
      "Batch 76 loss: 29735.1484375\n",
      "Batch 77 loss: 29357.23828125\n",
      "Batch 78 loss: 27670.677734375\n",
      "Batch 79 loss: 23991.62109375\n",
      "Batch 80 loss: 19254.8984375\n",
      "Batch 81 loss: 21569.029296875\n",
      "Batch 82 loss: 23609.267578125\n",
      "Batch 83 loss: 23205.05859375\n",
      "Batch 84 loss: 27537.6015625\n",
      "Batch 85 loss: 28539.859375\n",
      "Batch 86 loss: 32197.041015625\n",
      "Batch 87 loss: 30060.685546875\n",
      "Batch 88 loss: 30834.703125\n",
      "Batch 89 loss: 33582.0078125\n",
      "Batch 90 loss: 25579.98828125\n",
      "Batch 91 loss: 37933.3515625\n",
      "Batch 92 loss: 25342.0703125\n",
      "Batch 93 loss: 28839.146484375\n",
      "Batch 94 loss: 19677.908203125\n",
      "Batch 95 loss: 32460.357421875\n",
      "Batch 96 loss: 30553.474609375\n",
      "Batch 97 loss: 29160.43359375\n",
      "Batch 98 loss: 32012.6640625\n",
      "Batch 99 loss: 20324.9375\n",
      "Batch 100 loss: 27346.587890625\n",
      "Batch 101 loss: 21239.263671875\n",
      "Batch 102 loss: 29309.205078125\n",
      "Batch 103 loss: 49197.671875\n",
      "Batch 104 loss: 22075.974609375\n",
      "Batch 105 loss: 28572.90234375\n",
      "Batch 106 loss: 32456.87890625\n",
      "Batch 107 loss: 28630.06640625\n",
      "Batch 108 loss: 25251.609375\n",
      "Batch 109 loss: 30018.080078125\n",
      "Batch 110 loss: 27994.787109375\n",
      "Batch 111 loss: 26883.71875\n",
      "Batch 112 loss: 30547.5625\n",
      "Batch 113 loss: 21490.048828125\n",
      "Batch 114 loss: 31026.18359375\n",
      "Batch 115 loss: 36518.57421875\n",
      "Batch 116 loss: 39522.67578125\n",
      "Batch 117 loss: 42493.375\n",
      "Batch 118 loss: 29632.005859375\n",
      "Batch 119 loss: 16904.703125\n",
      "Batch 120 loss: 22467.76953125\n",
      "Batch 121 loss: 42289.15625\n",
      "Batch 122 loss: 28204.2265625\n",
      "Batch 123 loss: 19119.28125\n",
      "Batch 124 loss: 29790.0234375\n",
      "Batch 125 loss: 35689.27734375\n",
      "Batch 126 loss: 14227.173828125\n",
      "Batch 127 loss: 28802.20703125\n",
      "Batch 128 loss: 39046.7421875\n",
      "Batch 129 loss: 15681.958984375\n",
      "Batch 130 loss: 26967.619140625\n",
      "Batch 131 loss: 27497.560546875\n",
      "Batch 132 loss: 29738.654296875\n",
      "Batch 133 loss: 29623.314453125\n",
      "Batch 134 loss: 24583.53515625\n",
      "Batch 135 loss: 33484.1796875\n",
      "Batch 136 loss: 42092.19140625\n",
      "Batch 137 loss: 30209.251953125\n",
      "Batch 138 loss: 18870.521484375\n",
      "Batch 139 loss: 17421.8359375\n",
      "Batch 140 loss: 28940.302734375\n",
      "Batch 141 loss: 34101.59765625\n",
      "Batch 142 loss: 40741.08984375\n",
      "Batch 143 loss: 31119.47265625\n",
      "Batch 144 loss: 18372.291015625\n",
      "Batch 145 loss: 21310.02734375\n",
      "Batch 146 loss: 27902.162109375\n",
      "Batch 147 loss: 26399.966796875\n",
      "Batch 148 loss: 21840.49609375\n",
      "Batch 149 loss: 27607.373046875\n",
      "Batch 150 loss: 17471.314453125\n",
      "Batch 151 loss: 29719.587890625\n",
      "Batch 152 loss: 27071.244140625\n",
      "Batch 153 loss: 25052.728515625\n",
      "Batch 154 loss: 35919.1796875\n",
      "Batch 155 loss: 28720.03125\n",
      "Batch 156 loss: 23994.58203125\n",
      "Batch 157 loss: 38383.78125\n",
      "Batch 158 loss: 40269.359375\n",
      "Batch 159 loss: 31318.904296875\n",
      "Batch 160 loss: 40715.5234375\n",
      "Batch 161 loss: 23296.998046875\n",
      "Batch 162 loss: 25389.814453125\n",
      "Batch 163 loss: 50914.64453125\n",
      "Batch 164 loss: 28701.11328125\n",
      "Batch 165 loss: 31725.529296875\n",
      "Batch 166 loss: 33374.30078125\n",
      "Batch 167 loss: 20736.837890625\n",
      "Batch 168 loss: 23920.16796875\n",
      "Batch 169 loss: 32124.81640625\n",
      "Batch 170 loss: 26741.95703125\n",
      "Batch 171 loss: 32631.25\n",
      "Batch 172 loss: 40623.26171875\n",
      "Batch 173 loss: 25162.353515625\n",
      "Batch 174 loss: 26811.212890625\n",
      "Batch 175 loss: 34658.7578125\n",
      "Batch 176 loss: 32081.54296875\n",
      "Batch 177 loss: 34488.3671875\n",
      "Batch 178 loss: 24375.6328125\n",
      "Batch 179 loss: 22301.27734375\n",
      "Batch 180 loss: 23889.75\n",
      "Batch 181 loss: 28263.263671875\n",
      "Batch 182 loss: 16974.00390625\n",
      "Batch 183 loss: 30897.34375\n",
      "Batch 184 loss: 28696.208984375\n",
      "Batch 185 loss: 29135.486328125\n",
      "Batch 186 loss: 42765.8046875\n",
      "Batch 187 loss: 37969.40234375\n",
      "Batch 188 loss: 45189.15625\n",
      "Batch 189 loss: 25823.62890625\n",
      "Batch 190 loss: 20460.91796875\n",
      "Batch 191 loss: 40525.7890625\n",
      "Batch 192 loss: 28926.62890625\n",
      "Batch 193 loss: 43484.7734375\n",
      "Batch 194 loss: 21720.71875\n",
      "Batch 195 loss: 21063.79296875\n",
      "Batch 196 loss: 23646.6171875\n",
      "Batch 197 loss: 43847.9765625\n",
      "Batch 198 loss: 23163.826171875\n",
      "Batch 199 loss: 19758.62890625\n",
      "Batch 200 loss: 29746.04296875\n",
      "Batch 201 loss: 25581.837890625\n",
      "Batch 202 loss: 24175.025390625\n",
      "Batch 203 loss: 47922.66015625\n",
      "Batch 204 loss: 22586.060546875\n",
      "Batch 205 loss: 32472.185546875\n",
      "Batch 206 loss: 23801.587890625\n",
      "Batch 207 loss: 28927.16015625\n",
      "Batch 208 loss: 36078.73828125\n",
      "Batch 209 loss: 56491.37890625\n",
      "Batch 210 loss: 20041.267578125\n",
      "Batch 211 loss: 19105.634765625\n",
      "Batch 212 loss: 27029.068359375\n",
      "Batch 213 loss: 31825.60546875\n",
      "Batch 214 loss: 20558.677734375\n",
      "Batch 215 loss: 21763.84765625\n",
      "Batch 216 loss: 56207.71875\n",
      "Batch 217 loss: 24079.693359375\n",
      "Batch 218 loss: 26260.6796875\n",
      "Batch 219 loss: 27619.625\n",
      "Batch 220 loss: 31450.826171875\n",
      "Batch 221 loss: 29418.158203125\n",
      "Batch 222 loss: 22252.89453125\n",
      "Batch 223 loss: 19753.919921875\n",
      "Batch 224 loss: 28543.337890625\n",
      "Batch 225 loss: 29995.671875\n",
      "Batch 226 loss: 25414.724609375\n",
      "Batch 227 loss: 30086.69921875\n",
      "Batch 228 loss: 35484.01953125\n",
      "Batch 229 loss: 32139.666015625\n",
      "Batch 230 loss: 24844.58984375\n",
      "Batch 231 loss: 41989.125\n",
      "Batch 232 loss: 30966.611328125\n",
      "Batch 233 loss: 25701.484375\n",
      "Batch 234 loss: 28549.06640625\n",
      "Batch 235 loss: 28602.7421875\n",
      "Batch 236 loss: 25875.421875\n",
      "Batch 237 loss: 46308.640625\n",
      "Batch 238 loss: 22916.01171875\n",
      "Batch 239 loss: 25986.681640625\n",
      "Batch 240 loss: 30488.82421875\n",
      "Batch 241 loss: 40076.24609375\n",
      "Batch 242 loss: 31164.83984375\n",
      "Batch 243 loss: 24067.345703125\n",
      "Batch 244 loss: 37353.09375\n",
      "Batch 245 loss: 31960.384765625\n",
      "Batch 246 loss: 32599.419921875\n",
      "Batch 247 loss: 26844.783203125\n",
      "Batch 248 loss: 33414.6328125\n",
      "Batch 249 loss: 31815.619140625\n",
      "### Epoch 1\n",
      "Batch 0 loss: 36945.5078125\n",
      "Batch 1 loss: 35648.640625\n",
      "Batch 2 loss: 40617.33984375\n",
      "Batch 3 loss: 22485.529296875\n",
      "Batch 4 loss: 25673.673828125\n",
      "Batch 5 loss: 39978.7734375\n",
      "Batch 6 loss: 31025.431640625\n",
      "Batch 7 loss: 25966.046875\n",
      "Batch 8 loss: 30445.833984375\n",
      "Batch 9 loss: 23879.2890625\n",
      "Batch 10 loss: 27973.53125\n",
      "Batch 11 loss: 17637.998046875\n",
      "Batch 12 loss: 17971.208984375\n",
      "Batch 13 loss: 28890.064453125\n",
      "Batch 14 loss: 40438.9296875\n",
      "Batch 15 loss: 34163.765625\n",
      "Batch 16 loss: 13742.7734375\n",
      "Batch 17 loss: 29663.5703125\n",
      "Batch 18 loss: 21754.759765625\n",
      "Batch 19 loss: 23600.67578125\n",
      "Batch 20 loss: 29169.873046875\n",
      "Batch 21 loss: 32050.90625\n",
      "Batch 22 loss: 35298.4296875\n",
      "Batch 23 loss: 26447.75390625\n",
      "Batch 24 loss: 31645.623046875\n",
      "Batch 25 loss: 54485.39453125\n",
      "Batch 26 loss: 32826.87109375\n",
      "Batch 27 loss: 18293.94921875\n",
      "Batch 28 loss: 26331.015625\n",
      "Batch 29 loss: 33361.0859375\n",
      "Batch 30 loss: 26544.701171875\n",
      "Batch 31 loss: 28012.24609375\n",
      "Batch 32 loss: 49004.125\n",
      "Batch 33 loss: 43804.4765625\n",
      "Batch 34 loss: 22426.939453125\n",
      "Batch 35 loss: 33426.5234375\n",
      "Batch 36 loss: 27448.97265625\n",
      "Batch 37 loss: 15887.0927734375\n",
      "Batch 38 loss: 33454.84765625\n",
      "Batch 39 loss: 45062.5703125\n",
      "Batch 40 loss: 26268.3515625\n",
      "Batch 41 loss: 26354.783203125\n",
      "Batch 42 loss: 35346.40234375\n",
      "Batch 43 loss: 21591.6171875\n",
      "Batch 44 loss: 30442.4375\n",
      "Batch 45 loss: 31753.984375\n",
      "Batch 46 loss: 27915.001953125\n",
      "Batch 47 loss: 24418.158203125\n",
      "Batch 48 loss: 25421.576171875\n",
      "Batch 49 loss: 28973.92578125\n",
      "Batch 50 loss: 28878.298828125\n",
      "Batch 51 loss: 31850.515625\n",
      "Batch 52 loss: 17620.73046875\n",
      "Batch 53 loss: 18247.474609375\n",
      "Batch 54 loss: 32127.890625\n",
      "Batch 55 loss: 24529.865234375\n",
      "Batch 56 loss: 40466.953125\n",
      "Batch 57 loss: 43256.53515625\n",
      "Batch 58 loss: 23216.078125\n",
      "Batch 59 loss: 28618.7890625\n",
      "Batch 60 loss: 24327.552734375\n",
      "Batch 61 loss: 29716.462890625\n",
      "Batch 62 loss: 28810.421875\n",
      "Batch 63 loss: 37563.70703125\n",
      "Batch 64 loss: 22442.236328125\n",
      "Batch 65 loss: 35204.234375\n",
      "Batch 66 loss: 29688.513671875\n",
      "Batch 67 loss: 29975.435546875\n",
      "Batch 68 loss: 36431.58203125\n",
      "Batch 69 loss: 34117.6328125\n",
      "Batch 70 loss: 27613.81640625\n",
      "Batch 71 loss: 27087.861328125\n",
      "Batch 72 loss: 31395.05859375\n",
      "Batch 73 loss: 22301.0546875\n",
      "Batch 74 loss: 22943.173828125\n",
      "Batch 75 loss: 24814.4765625\n",
      "Batch 76 loss: 34738.73046875\n",
      "Batch 77 loss: 28666.40234375\n",
      "Batch 78 loss: 24916.919921875\n",
      "Batch 79 loss: 23026.623046875\n",
      "Batch 80 loss: 28883.12109375\n",
      "Batch 81 loss: 40790.68359375\n",
      "Batch 82 loss: 30327.5\n",
      "Batch 83 loss: 33240.7265625\n",
      "Batch 84 loss: 33544.171875\n",
      "Batch 85 loss: 28811.65625\n",
      "Batch 86 loss: 40910.609375\n",
      "Batch 87 loss: 21787.92578125\n",
      "Batch 88 loss: 36460.80078125\n",
      "Batch 89 loss: 36535.63671875\n",
      "Batch 90 loss: 34867.234375\n",
      "Batch 91 loss: 29963.326171875\n",
      "Batch 92 loss: 38598.4765625\n",
      "Batch 93 loss: 24176.080078125\n",
      "Batch 94 loss: 30155.642578125\n",
      "Batch 95 loss: 25216.240234375\n",
      "Batch 96 loss: 23513.771484375\n",
      "Batch 97 loss: 44324.0859375\n",
      "Batch 98 loss: 29862.82421875\n",
      "Batch 99 loss: 25353.416015625\n",
      "Batch 100 loss: 27258.5234375\n",
      "Batch 101 loss: 31403.234375\n",
      "Batch 102 loss: 32134.7265625\n",
      "Batch 103 loss: 31784.037109375\n",
      "Batch 104 loss: 60689.9921875\n",
      "Batch 105 loss: 24404.19921875\n",
      "Batch 106 loss: 38939.4765625\n",
      "Batch 107 loss: 23674.19921875\n",
      "Batch 108 loss: 27954.05859375\n",
      "Batch 109 loss: 36881.265625\n",
      "Batch 110 loss: 32376.35546875\n",
      "Batch 111 loss: 25621.55078125\n",
      "Batch 112 loss: 27479.43359375\n",
      "Batch 113 loss: 26707.89453125\n",
      "Batch 114 loss: 33262.3515625\n",
      "Batch 115 loss: 29694.447265625\n",
      "Batch 116 loss: 29054.564453125\n",
      "Batch 117 loss: 27621.046875\n",
      "Batch 118 loss: 29152.59375\n",
      "Batch 119 loss: 31459.92578125\n",
      "Batch 120 loss: 24418.703125\n",
      "Batch 121 loss: 20625.884765625\n",
      "Batch 122 loss: 33203.25390625\n",
      "Batch 123 loss: 15675.9052734375\n",
      "Batch 124 loss: 31366.0546875\n",
      "Batch 125 loss: 42639.55078125\n",
      "Batch 126 loss: 32341.171875\n",
      "Batch 127 loss: 32490.841796875\n",
      "Batch 128 loss: 30984.595703125\n",
      "Batch 129 loss: 34799.29296875\n",
      "Batch 130 loss: 26131.677734375\n",
      "Batch 131 loss: 33601.69140625\n",
      "Batch 132 loss: 24373.78515625\n",
      "Batch 133 loss: 42708.80859375\n",
      "Batch 134 loss: 19767.8671875\n",
      "Batch 135 loss: 42723.46484375\n",
      "Batch 136 loss: 28872.77734375\n",
      "Batch 137 loss: 25614.79296875\n",
      "Batch 138 loss: 26174.92578125\n",
      "Batch 139 loss: 19739.8984375\n",
      "Batch 140 loss: 17560.8515625\n",
      "Batch 141 loss: 31274.5\n",
      "Batch 142 loss: 30465.412109375\n",
      "Batch 143 loss: 31311.30859375\n",
      "Batch 144 loss: 25687.6875\n",
      "Batch 145 loss: 27148.37890625\n",
      "Batch 146 loss: 28630.61328125\n",
      "Batch 147 loss: 21044.853515625\n",
      "Batch 148 loss: 22895.56640625\n",
      "Batch 149 loss: 22039.06640625\n",
      "Batch 150 loss: 36527.2421875\n",
      "Batch 151 loss: 24134.150390625\n",
      "Batch 152 loss: 29728.4375\n",
      "Batch 153 loss: 29983.82421875\n",
      "Batch 154 loss: 24123.572265625\n",
      "Batch 155 loss: 42705.69921875\n",
      "Batch 156 loss: 30900.9609375\n",
      "Batch 157 loss: 27354.869140625\n",
      "Batch 158 loss: 22045.705078125\n",
      "Batch 159 loss: 40614.22265625\n",
      "Batch 160 loss: 39279.12109375\n",
      "Batch 161 loss: 36547.0546875\n",
      "Batch 162 loss: 23341.234375\n",
      "Batch 163 loss: 29160.029296875\n",
      "Batch 164 loss: 41072.65234375\n",
      "Batch 165 loss: 27582.642578125\n",
      "Batch 166 loss: 23332.681640625\n",
      "Batch 167 loss: 29983.078125\n",
      "Batch 168 loss: 26372.083984375\n",
      "Batch 169 loss: 33916.484375\n",
      "Batch 170 loss: 22829.998046875\n",
      "Batch 171 loss: 36848.9375\n",
      "Batch 172 loss: 28900.52734375\n",
      "Batch 173 loss: 25963.09765625\n",
      "Batch 174 loss: 27894.01171875\n",
      "Batch 175 loss: 25268.1328125\n",
      "Batch 176 loss: 27764.880859375\n",
      "Batch 177 loss: 36324.74609375\n",
      "Batch 178 loss: 20703.6171875\n",
      "Batch 179 loss: 20079.8828125\n",
      "Batch 180 loss: 30198.162109375\n",
      "Batch 181 loss: 29416.150390625\n",
      "Batch 182 loss: 21353.56640625\n",
      "Batch 183 loss: 34003.51171875\n",
      "Batch 184 loss: 31028.98046875\n",
      "Batch 185 loss: 33742.94921875\n",
      "Batch 186 loss: 25003.947265625\n",
      "Batch 187 loss: 24483.044921875\n",
      "Batch 188 loss: 35240.40625\n",
      "Batch 189 loss: 28383.65234375\n",
      "Batch 190 loss: 15722.146484375\n",
      "Batch 191 loss: 19233.0\n",
      "Batch 192 loss: 25762.154296875\n",
      "Batch 193 loss: 29022.8125\n",
      "Batch 194 loss: 23466.8671875\n",
      "Batch 195 loss: 22114.345703125\n",
      "Batch 196 loss: 43196.91015625\n",
      "Batch 197 loss: 21649.875\n",
      "Batch 198 loss: 21861.119140625\n",
      "Batch 199 loss: 28021.634765625\n",
      "Batch 200 loss: 26580.375\n",
      "Batch 201 loss: 18173.453125\n",
      "Batch 202 loss: 22082.595703125\n",
      "Batch 203 loss: 36177.9375\n",
      "Batch 204 loss: 27692.546875\n",
      "Batch 205 loss: 25460.6328125\n",
      "Batch 206 loss: 29487.119140625\n",
      "Batch 207 loss: 37044.62890625\n",
      "Batch 208 loss: 24969.763671875\n",
      "Batch 209 loss: 31473.38671875\n",
      "Batch 210 loss: 24862.236328125\n",
      "Batch 211 loss: 33704.08203125\n",
      "Batch 212 loss: 33732.375\n",
      "Batch 213 loss: 24638.880859375\n",
      "Batch 214 loss: 22329.353515625\n",
      "Batch 215 loss: 19786.93359375\n",
      "Batch 216 loss: 19081.345703125\n",
      "Batch 217 loss: 31725.02734375\n",
      "Batch 218 loss: 33372.171875\n",
      "Batch 219 loss: 28617.3515625\n",
      "Batch 220 loss: 40863.5234375\n",
      "Batch 221 loss: 24495.69921875\n",
      "Batch 222 loss: 29400.833984375\n",
      "Batch 223 loss: 36133.9765625\n",
      "Batch 224 loss: 33185.8828125\n",
      "Batch 225 loss: 22012.15625\n",
      "Batch 226 loss: 20517.5\n",
      "Batch 227 loss: 18373.740234375\n",
      "Batch 228 loss: 24867.298828125\n",
      "Batch 229 loss: 51833.02734375\n",
      "Batch 230 loss: 25002.23046875\n",
      "Batch 231 loss: 41987.140625\n",
      "Batch 232 loss: 24869.826171875\n",
      "Batch 233 loss: 46174.37890625\n",
      "Batch 234 loss: 24521.69140625\n",
      "Batch 235 loss: 38693.13671875\n",
      "Batch 236 loss: 24950.04296875\n",
      "Batch 237 loss: 31066.32421875\n",
      "Batch 238 loss: 40213.75\n",
      "Batch 239 loss: 20023.439453125\n",
      "Batch 240 loss: 33369.09765625\n",
      "Batch 241 loss: 27336.234375\n",
      "Batch 242 loss: 30811.13671875\n",
      "Batch 243 loss: 27737.984375\n",
      "Batch 244 loss: 28749.248046875\n",
      "Batch 245 loss: 30989.3671875\n",
      "Batch 246 loss: 40912.1953125\n",
      "Batch 247 loss: 22385.65625\n",
      "Batch 248 loss: 21307.32421875\n",
      "Batch 249 loss: 17147.421875\n",
      "### Epoch 2\n",
      "Batch 0 loss: 28610.5390625\n",
      "Batch 1 loss: 37564.02734375\n",
      "Batch 2 loss: 30055.671875\n",
      "Batch 3 loss: 41680.68359375\n",
      "Batch 4 loss: 32315.169921875\n",
      "Batch 5 loss: 33094.6484375\n",
      "Batch 6 loss: 25297.462890625\n",
      "Batch 7 loss: 27263.23828125\n",
      "Batch 8 loss: 23295.771484375\n",
      "Batch 9 loss: 36249.18359375\n",
      "Batch 10 loss: 33252.1328125\n",
      "Batch 11 loss: 25139.27734375\n",
      "Batch 12 loss: 30707.390625\n",
      "Batch 13 loss: 35131.78125\n",
      "Batch 14 loss: 13887.7900390625\n",
      "Batch 15 loss: 36298.3125\n",
      "Batch 16 loss: 31507.90625\n",
      "Batch 17 loss: 34489.4609375\n",
      "Batch 18 loss: 17506.185546875\n",
      "Batch 19 loss: 20124.158203125\n",
      "Batch 20 loss: 25429.458984375\n",
      "Batch 21 loss: 35653.828125\n",
      "Batch 22 loss: 27050.603515625\n",
      "Batch 23 loss: 28454.490234375\n",
      "Batch 24 loss: 32450.529296875\n",
      "Batch 25 loss: 31762.689453125\n",
      "Batch 26 loss: 22422.9765625\n",
      "Batch 27 loss: 33290.83984375\n",
      "Batch 28 loss: 23858.353515625\n",
      "Batch 29 loss: 26215.19921875\n",
      "Batch 30 loss: 25600.052734375\n",
      "Batch 31 loss: 35547.21875\n",
      "Batch 32 loss: 29920.37109375\n",
      "Batch 33 loss: 35548.1171875\n",
      "Batch 34 loss: 18198.169921875\n",
      "Batch 35 loss: 32582.453125\n",
      "Batch 36 loss: 19720.400390625\n",
      "Batch 37 loss: 36316.6015625\n",
      "Batch 38 loss: 30359.041015625\n",
      "Batch 39 loss: 22852.146484375\n",
      "Batch 40 loss: 29481.70703125\n",
      "Batch 41 loss: 51382.4453125\n",
      "Batch 42 loss: 20576.853515625\n",
      "Batch 43 loss: 31449.65234375\n",
      "Batch 44 loss: 14816.529296875\n",
      "Batch 45 loss: 30530.90625\n",
      "Batch 46 loss: 20472.9140625\n",
      "Batch 47 loss: 19723.357421875\n",
      "Batch 48 loss: 24670.111328125\n",
      "Batch 49 loss: 36812.1953125\n",
      "Batch 50 loss: 29517.00390625\n",
      "Batch 51 loss: 35419.1171875\n",
      "Batch 52 loss: 16982.083984375\n",
      "Batch 53 loss: 34521.6640625\n",
      "Batch 54 loss: 37803.05859375\n",
      "Batch 55 loss: 27432.904296875\n",
      "Batch 56 loss: 30531.84765625\n",
      "Batch 57 loss: 21471.853515625\n",
      "Batch 58 loss: 26294.25\n",
      "Batch 59 loss: 27730.51953125\n",
      "Batch 60 loss: 34148.625\n",
      "Batch 61 loss: 42426.83203125\n",
      "Batch 62 loss: 29654.52734375\n",
      "Batch 63 loss: 33381.39453125\n",
      "Batch 64 loss: 17687.814453125\n",
      "Batch 65 loss: 29798.193359375\n",
      "Batch 66 loss: 28761.8828125\n",
      "Batch 67 loss: 33302.90234375\n",
      "Batch 68 loss: 27126.80859375\n",
      "Batch 69 loss: 35513.77734375\n",
      "Batch 70 loss: 25236.0703125\n",
      "Batch 71 loss: 46449.40625\n",
      "Batch 72 loss: 32289.216796875\n",
      "Batch 73 loss: 24406.03125\n",
      "Batch 74 loss: 37738.66015625\n",
      "Batch 75 loss: 27736.80859375\n",
      "Batch 76 loss: 24183.9921875\n",
      "Batch 77 loss: 22023.14453125\n",
      "Batch 78 loss: 31558.658203125\n",
      "Batch 79 loss: 36896.9375\n",
      "Batch 80 loss: 25730.076171875\n",
      "Batch 81 loss: 34799.109375\n",
      "Batch 82 loss: 37478.89453125\n",
      "Batch 83 loss: 21059.68359375\n",
      "Batch 84 loss: 33608.32421875\n",
      "Batch 85 loss: 25568.34375\n",
      "Batch 86 loss: 32917.9453125\n",
      "Batch 87 loss: 22796.48046875\n",
      "Batch 88 loss: 28468.30859375\n",
      "Batch 89 loss: 38805.6953125\n",
      "Batch 90 loss: 19635.919921875\n",
      "Batch 91 loss: 35321.74609375\n",
      "Batch 92 loss: 31478.9375\n",
      "Batch 93 loss: 25671.44921875\n",
      "Batch 94 loss: 36763.4453125\n",
      "Batch 95 loss: 34020.5546875\n",
      "Batch 96 loss: 26807.080078125\n",
      "Batch 97 loss: 48056.7109375\n",
      "Batch 98 loss: 25699.5546875\n",
      "Batch 99 loss: 22856.091796875\n",
      "Batch 100 loss: 26263.345703125\n",
      "Batch 101 loss: 36934.13671875\n",
      "Batch 102 loss: 38447.17578125\n",
      "Batch 103 loss: 23368.5859375\n",
      "Batch 104 loss: 29838.62109375\n",
      "Batch 105 loss: 42374.44140625\n",
      "Batch 106 loss: 21515.171875\n",
      "Batch 107 loss: 43852.1640625\n",
      "Batch 108 loss: 23441.51171875\n",
      "Batch 109 loss: 28086.681640625\n",
      "Batch 110 loss: 35837.19140625\n",
      "Batch 111 loss: 21793.73046875\n",
      "Batch 112 loss: 25361.205078125\n",
      "Batch 113 loss: 28465.984375\n",
      "Batch 114 loss: 30672.6171875\n",
      "Batch 115 loss: 49699.7421875\n",
      "Batch 116 loss: 31409.09765625\n",
      "Batch 117 loss: 15666.05078125\n",
      "Batch 118 loss: 27148.220703125\n",
      "Batch 119 loss: 30396.876953125\n",
      "Batch 120 loss: 27837.41015625\n",
      "Batch 121 loss: 22426.2890625\n",
      "Batch 122 loss: 30115.373046875\n",
      "Batch 123 loss: 33029.63671875\n",
      "Batch 124 loss: 38104.328125\n",
      "Batch 125 loss: 22705.9375\n",
      "Batch 126 loss: 30232.126953125\n",
      "Batch 127 loss: 34661.234375\n",
      "Batch 128 loss: 15962.7705078125\n",
      "Batch 129 loss: 23206.53125\n",
      "Batch 130 loss: 29092.166015625\n",
      "Batch 131 loss: 27225.48046875\n",
      "Batch 132 loss: 20457.77734375\n",
      "Batch 133 loss: 34501.9609375\n",
      "Batch 134 loss: 32485.365234375\n",
      "Batch 135 loss: 39214.78125\n",
      "Batch 136 loss: 27685.251953125\n",
      "Batch 137 loss: 32138.111328125\n",
      "Batch 138 loss: 29186.458984375\n",
      "Batch 139 loss: 19765.220703125\n",
      "Batch 140 loss: 38178.97265625\n",
      "Batch 141 loss: 32298.255859375\n",
      "Batch 142 loss: 46495.41796875\n",
      "Batch 143 loss: 28182.208984375\n",
      "Batch 144 loss: 33605.61328125\n",
      "Batch 145 loss: 24966.052734375\n",
      "Batch 146 loss: 27624.08203125\n",
      "Batch 147 loss: 26304.220703125\n",
      "Batch 148 loss: 35915.43359375\n",
      "Batch 149 loss: 35181.74609375\n",
      "Batch 150 loss: 29792.083984375\n",
      "Batch 151 loss: 43013.48828125\n",
      "Batch 152 loss: 22662.501953125\n",
      "Batch 153 loss: 31405.533203125\n",
      "Batch 154 loss: 38048.5625\n",
      "Batch 155 loss: 18303.865234375\n",
      "Batch 156 loss: 29589.728515625\n",
      "Batch 157 loss: 39776.23828125\n",
      "Batch 158 loss: 31881.974609375\n",
      "Batch 159 loss: 28154.671875\n",
      "Batch 160 loss: 39169.5859375\n",
      "Batch 161 loss: 30927.8984375\n",
      "Batch 162 loss: 27184.1796875\n",
      "Batch 163 loss: 20300.38671875\n",
      "Batch 164 loss: 30636.4375\n",
      "Batch 165 loss: 28900.2109375\n",
      "Batch 166 loss: 24645.021484375\n",
      "Batch 167 loss: 18163.81640625\n",
      "Batch 168 loss: 29607.34375\n",
      "Batch 169 loss: 14937.291015625\n",
      "Batch 170 loss: 34353.22265625\n",
      "Batch 171 loss: 20309.173828125\n",
      "Batch 172 loss: 31334.35546875\n",
      "Batch 173 loss: 34016.85546875\n",
      "Batch 174 loss: 33407.11328125\n",
      "Batch 175 loss: 26752.3671875\n",
      "Batch 176 loss: 25528.337890625\n",
      "Batch 177 loss: 25681.68359375\n",
      "Batch 178 loss: 27554.09375\n",
      "Batch 179 loss: 23725.443359375\n",
      "Batch 180 loss: 25131.96875\n",
      "Batch 181 loss: 16261.1767578125\n",
      "Batch 182 loss: 40609.453125\n",
      "Batch 183 loss: 34396.0078125\n",
      "Batch 184 loss: 27799.08984375\n",
      "Batch 185 loss: 36573.921875\n",
      "Batch 186 loss: 28551.490234375\n",
      "Batch 187 loss: 22933.541015625\n",
      "Batch 188 loss: 19497.380859375\n",
      "Batch 189 loss: 22612.244140625\n",
      "Batch 190 loss: 33744.6875\n",
      "Batch 191 loss: 19297.794921875\n",
      "Batch 192 loss: 21855.189453125\n",
      "Batch 193 loss: 23552.5625\n",
      "Batch 194 loss: 22476.474609375\n",
      "Batch 195 loss: 33787.8125\n",
      "Batch 196 loss: 40675.1796875\n",
      "Batch 197 loss: 37007.09375\n",
      "Batch 198 loss: 38816.96875\n",
      "Batch 199 loss: 26876.01953125\n",
      "Batch 200 loss: 30587.84765625\n",
      "Batch 201 loss: 34232.94140625\n",
      "Batch 202 loss: 28179.38671875\n",
      "Batch 203 loss: 42361.140625\n",
      "Batch 204 loss: 29900.01171875\n",
      "Batch 205 loss: 25098.556640625\n",
      "Batch 206 loss: 25025.0546875\n",
      "Batch 207 loss: 31418.275390625\n",
      "Batch 208 loss: 28223.3125\n",
      "Batch 209 loss: 33708.81640625\n",
      "Batch 210 loss: 25411.98046875\n",
      "Batch 211 loss: 32457.21484375\n",
      "Batch 212 loss: 30338.228515625\n",
      "Batch 213 loss: 32980.68359375\n",
      "Batch 214 loss: 28585.859375\n",
      "Batch 215 loss: 27019.478515625\n",
      "Batch 216 loss: 30912.615234375\n",
      "Batch 217 loss: 27744.703125\n",
      "Batch 218 loss: 33242.48828125\n",
      "Batch 219 loss: 33919.2109375\n",
      "Batch 220 loss: 22298.517578125\n",
      "Batch 221 loss: 25019.369140625\n",
      "Batch 222 loss: 25362.376953125\n",
      "Batch 223 loss: 30213.80859375\n",
      "Batch 224 loss: 15969.61328125\n",
      "Batch 225 loss: 34241.53125\n",
      "Batch 226 loss: 40985.84375\n",
      "Batch 227 loss: 16958.66796875\n",
      "Batch 228 loss: 29070.744140625\n",
      "Batch 229 loss: 25455.271484375\n",
      "Batch 230 loss: 21809.615234375\n",
      "Batch 231 loss: 26920.173828125\n",
      "Batch 232 loss: 38590.13671875\n",
      "Batch 233 loss: 21802.337890625\n",
      "Batch 234 loss: 32910.62109375\n",
      "Batch 235 loss: 31056.697265625\n",
      "Batch 236 loss: 22829.314453125\n",
      "Batch 237 loss: 34507.31640625\n",
      "Batch 238 loss: 30394.712890625\n",
      "Batch 239 loss: 27407.060546875\n",
      "Batch 240 loss: 17202.19140625\n",
      "Batch 241 loss: 23318.529296875\n",
      "Batch 242 loss: 34578.0703125\n",
      "Batch 243 loss: 25387.93359375\n",
      "Batch 244 loss: 26268.55859375\n",
      "Batch 245 loss: 33636.2421875\n",
      "Batch 246 loss: 37244.875\n",
      "Batch 247 loss: 28843.697265625\n",
      "Batch 248 loss: 31362.4375\n",
      "Batch 249 loss: 25856.154296875\n",
      "### Epoch 3\n",
      "Batch 0 loss: 13560.70703125\n",
      "Batch 1 loss: 40261.1484375\n",
      "Batch 2 loss: 30745.15625\n",
      "Batch 3 loss: 22239.080078125\n",
      "Batch 4 loss: 41523.9296875\n",
      "Batch 5 loss: 26395.841796875\n",
      "Batch 6 loss: 37002.94921875\n",
      "Batch 7 loss: 33456.48046875\n",
      "Batch 8 loss: 25837.796875\n",
      "Batch 9 loss: 25911.6015625\n",
      "Batch 10 loss: 38136.74609375\n",
      "Batch 11 loss: 32774.74609375\n",
      "Batch 12 loss: 29400.072265625\n",
      "Batch 13 loss: 26469.3984375\n",
      "Batch 14 loss: 25680.859375\n",
      "Batch 15 loss: 36085.78515625\n",
      "Batch 16 loss: 42263.1484375\n",
      "Batch 17 loss: 36139.98046875\n",
      "Batch 18 loss: 45489.59375\n",
      "Batch 19 loss: 21227.109375\n",
      "Batch 20 loss: 51044.296875\n",
      "Batch 21 loss: 26665.21875\n",
      "Batch 22 loss: 33819.9921875\n",
      "Batch 23 loss: 28583.7421875\n",
      "Batch 24 loss: 12482.607421875\n",
      "Batch 25 loss: 21150.1953125\n",
      "Batch 26 loss: 28521.994140625\n",
      "Batch 27 loss: 23054.751953125\n",
      "Batch 28 loss: 21867.890625\n",
      "Batch 29 loss: 38215.49609375\n",
      "Batch 30 loss: 24397.533203125\n",
      "Batch 31 loss: 12412.2861328125\n",
      "Batch 32 loss: 27754.4375\n",
      "Batch 33 loss: 30741.810546875\n",
      "Batch 34 loss: 30577.837890625\n",
      "Batch 35 loss: 15974.8056640625\n",
      "Batch 36 loss: 22162.77734375\n",
      "Batch 37 loss: 25811.33984375\n",
      "Batch 38 loss: 32352.19921875\n",
      "Batch 39 loss: 33314.84765625\n",
      "Batch 40 loss: 21428.82421875\n",
      "Batch 41 loss: 42164.43359375\n",
      "Batch 42 loss: 29843.2109375\n",
      "Batch 43 loss: 32851.5703125\n",
      "Batch 44 loss: 24801.982421875\n",
      "Batch 45 loss: 22173.33203125\n",
      "Batch 46 loss: 50704.15625\n",
      "Batch 47 loss: 42984.15234375\n",
      "Batch 48 loss: 45062.32421875\n",
      "Batch 49 loss: 25092.6171875\n",
      "Batch 50 loss: 34004.69921875\n",
      "Batch 51 loss: 26939.5390625\n",
      "Batch 52 loss: 28878.56640625\n",
      "Batch 53 loss: 32933.12109375\n",
      "Batch 54 loss: 23072.509765625\n",
      "Batch 55 loss: 23328.6953125\n",
      "Batch 56 loss: 35821.20703125\n",
      "Batch 57 loss: 30533.61328125\n",
      "Batch 58 loss: 38736.8046875\n",
      "Batch 59 loss: 22850.171875\n",
      "Batch 60 loss: 29517.1953125\n",
      "Batch 61 loss: 21083.45703125\n",
      "Batch 62 loss: 28648.14453125\n",
      "Batch 63 loss: 49574.23046875\n",
      "Batch 64 loss: 25614.830078125\n",
      "Batch 65 loss: 19719.67578125\n",
      "Batch 66 loss: 35174.16796875\n",
      "Batch 67 loss: 23867.734375\n",
      "Batch 68 loss: 20096.998046875\n",
      "Batch 69 loss: 11803.2236328125\n",
      "Batch 70 loss: 28765.384765625\n",
      "Batch 71 loss: 34434.41015625\n",
      "Batch 72 loss: 30369.162109375\n",
      "Batch 73 loss: 26634.9765625\n",
      "Batch 74 loss: 30605.876953125\n",
      "Batch 75 loss: 25295.224609375\n",
      "Batch 76 loss: 22241.23046875\n",
      "Batch 77 loss: 20509.677734375\n",
      "Batch 78 loss: 25563.34765625\n",
      "Batch 79 loss: 16380.1875\n",
      "Batch 80 loss: 29944.951171875\n",
      "Batch 81 loss: 44823.95703125\n",
      "Batch 82 loss: 26076.791015625\n",
      "Batch 83 loss: 33577.52734375\n",
      "Batch 84 loss: 29090.6796875\n",
      "Batch 85 loss: 22464.44140625\n",
      "Batch 86 loss: 38898.7265625\n",
      "Batch 87 loss: 27565.244140625\n",
      "Batch 88 loss: 36361.21875\n",
      "Batch 89 loss: 28910.00390625\n",
      "Batch 90 loss: 27359.255859375\n",
      "Batch 91 loss: 19940.55859375\n",
      "Batch 92 loss: 29952.01171875\n",
      "Batch 93 loss: 33287.66796875\n",
      "Batch 94 loss: 30807.681640625\n",
      "Batch 95 loss: 33726.37890625\n",
      "Batch 96 loss: 34631.76171875\n",
      "Batch 97 loss: 26562.275390625\n",
      "Batch 98 loss: 33823.2734375\n",
      "Batch 99 loss: 30063.46875\n",
      "Batch 100 loss: 18463.26953125\n",
      "Batch 101 loss: 45258.84375\n",
      "Batch 102 loss: 33307.453125\n",
      "Batch 103 loss: 32738.181640625\n",
      "Batch 104 loss: 30230.21484375\n",
      "Batch 105 loss: 25688.931640625\n",
      "Batch 106 loss: 28466.18359375\n",
      "Batch 107 loss: 29332.396484375\n",
      "Batch 108 loss: 30714.826171875\n",
      "Batch 109 loss: 36980.09765625\n",
      "Batch 110 loss: 29018.505859375\n",
      "Batch 111 loss: 36877.6796875\n",
      "Batch 112 loss: 33204.94921875\n",
      "Batch 113 loss: 28158.015625\n",
      "Batch 114 loss: 28596.5546875\n",
      "Batch 115 loss: 27403.017578125\n",
      "Batch 116 loss: 28849.294921875\n",
      "Batch 117 loss: 32762.388671875\n",
      "Batch 118 loss: 24920.5625\n",
      "Batch 119 loss: 33367.4609375\n",
      "Batch 120 loss: 23225.7421875\n",
      "Batch 121 loss: 22179.33203125\n",
      "Batch 122 loss: 21716.001953125\n",
      "Batch 123 loss: 31694.84765625\n",
      "Batch 124 loss: 22670.37109375\n",
      "Batch 125 loss: 26559.705078125\n",
      "Batch 126 loss: 29434.90625\n",
      "Batch 127 loss: 29057.81640625\n",
      "Batch 128 loss: 20886.53515625\n",
      "Batch 129 loss: 21866.7734375\n",
      "Batch 130 loss: 21692.505859375\n",
      "Batch 131 loss: 26904.705078125\n",
      "Batch 132 loss: 27342.775390625\n",
      "Batch 133 loss: 30265.9765625\n",
      "Batch 134 loss: 20168.61328125\n",
      "Batch 135 loss: 47142.59375\n",
      "Batch 136 loss: 31173.83203125\n",
      "Batch 137 loss: 24962.857421875\n",
      "Batch 138 loss: 29630.509765625\n",
      "Batch 139 loss: 28774.5625\n",
      "Batch 140 loss: 37125.91015625\n",
      "Batch 141 loss: 32994.0\n",
      "Batch 142 loss: 25120.748046875\n",
      "Batch 143 loss: 29438.3984375\n",
      "Batch 144 loss: 25164.5390625\n",
      "Batch 145 loss: 25304.494140625\n",
      "Batch 146 loss: 24937.578125\n",
      "Batch 147 loss: 19933.11328125\n",
      "Batch 148 loss: 22608.12109375\n",
      "Batch 149 loss: 26376.6640625\n",
      "Batch 150 loss: 34235.6796875\n",
      "Batch 151 loss: 21855.234375\n",
      "Batch 152 loss: 17611.94921875\n",
      "Batch 153 loss: 27279.7578125\n",
      "Batch 154 loss: 19841.47265625\n",
      "Batch 155 loss: 33718.203125\n",
      "Batch 156 loss: 42855.97265625\n",
      "Batch 157 loss: 42029.71875\n",
      "Batch 158 loss: 25662.328125\n",
      "Batch 159 loss: 21990.251953125\n",
      "Batch 160 loss: 20678.55078125\n",
      "Batch 161 loss: 31671.30859375\n",
      "Batch 162 loss: 43645.6875\n",
      "Batch 163 loss: 31428.353515625\n",
      "Batch 164 loss: 29379.505859375\n",
      "Batch 165 loss: 30312.0234375\n",
      "Batch 166 loss: 20379.498046875\n",
      "Batch 167 loss: 29216.705078125\n",
      "Batch 168 loss: 23246.3671875\n",
      "Batch 169 loss: 25591.77734375\n",
      "Batch 170 loss: 36405.51953125\n",
      "Batch 171 loss: 30158.197265625\n",
      "Batch 172 loss: 25554.201171875\n",
      "Batch 173 loss: 34298.61328125\n",
      "Batch 174 loss: 30991.0703125\n",
      "Batch 175 loss: 21839.349609375\n",
      "Batch 176 loss: 22787.63671875\n",
      "Batch 177 loss: 25339.65625\n",
      "Batch 178 loss: 31061.919921875\n",
      "Batch 179 loss: 29135.810546875\n",
      "Batch 180 loss: 34035.09765625\n",
      "Batch 181 loss: 48120.25390625\n",
      "Batch 182 loss: 20837.431640625\n",
      "Batch 183 loss: 25232.765625\n",
      "Batch 184 loss: 33817.62890625\n",
      "Batch 185 loss: 35106.484375\n",
      "Batch 186 loss: 39713.99609375\n",
      "Batch 187 loss: 28014.728515625\n",
      "Batch 188 loss: 26262.0625\n",
      "Batch 189 loss: 34836.30078125\n",
      "Batch 190 loss: 21879.9375\n",
      "Batch 191 loss: 25374.5\n",
      "Batch 192 loss: 16828.31640625\n",
      "Batch 193 loss: 35339.6015625\n",
      "Batch 194 loss: 29653.9609375\n",
      "Batch 195 loss: 30126.060546875\n",
      "Batch 196 loss: 29877.40625\n",
      "Batch 197 loss: 24982.740234375\n",
      "Batch 198 loss: 33441.1875\n",
      "Batch 199 loss: 38465.91796875\n",
      "Batch 200 loss: 42829.91015625\n",
      "Batch 201 loss: 46052.63671875\n",
      "Batch 202 loss: 28978.11328125\n",
      "Batch 203 loss: 21753.265625\n",
      "Batch 204 loss: 37761.8828125\n",
      "Batch 205 loss: 20962.130859375\n",
      "Batch 206 loss: 26911.109375\n",
      "Batch 207 loss: 29558.08203125\n",
      "Batch 208 loss: 22827.87109375\n",
      "Batch 209 loss: 40435.265625\n",
      "Batch 210 loss: 35843.046875\n",
      "Batch 211 loss: 26590.486328125\n",
      "Batch 212 loss: 28748.138671875\n",
      "Batch 213 loss: 19672.103515625\n",
      "Batch 214 loss: 27592.5546875\n",
      "Batch 215 loss: 44342.46875\n",
      "Batch 216 loss: 31829.08203125\n",
      "Batch 217 loss: 36820.09375\n",
      "Batch 218 loss: 29778.0234375\n",
      "Batch 219 loss: 17594.388671875\n",
      "Batch 220 loss: 40037.1171875\n",
      "Batch 221 loss: 39256.16796875\n",
      "Batch 222 loss: 24337.943359375\n",
      "Batch 223 loss: 41427.44140625\n",
      "Batch 224 loss: 19662.435546875\n",
      "Batch 225 loss: 41849.23046875\n",
      "Batch 226 loss: 28190.35546875\n",
      "Batch 227 loss: 20775.798828125\n",
      "Batch 228 loss: 28579.74609375\n",
      "Batch 229 loss: 23961.73046875\n",
      "Batch 230 loss: 38778.6171875\n",
      "Batch 231 loss: 28854.681640625\n",
      "Batch 232 loss: 18680.87109375\n",
      "Batch 233 loss: 35503.140625\n",
      "Batch 234 loss: 33683.25\n",
      "Batch 235 loss: 30903.19921875\n",
      "Batch 236 loss: 25779.953125\n",
      "Batch 237 loss: 19151.072265625\n",
      "Batch 238 loss: 20765.900390625\n",
      "Batch 239 loss: 31509.923828125\n",
      "Batch 240 loss: 36699.84765625\n",
      "Batch 241 loss: 32583.373046875\n",
      "Batch 242 loss: 29527.375\n",
      "Batch 243 loss: 20485.400390625\n",
      "Batch 244 loss: 31470.31640625\n",
      "Batch 245 loss: 23886.103515625\n",
      "Batch 246 loss: 37994.0078125\n",
      "Batch 247 loss: 20954.814453125\n",
      "Batch 248 loss: 33680.25\n",
      "Batch 249 loss: 34739.5390625\n",
      "### Epoch 4\n",
      "Batch 0 loss: 43926.0\n",
      "Batch 1 loss: 22606.302734375\n",
      "Batch 2 loss: 30965.609375\n",
      "Batch 3 loss: 22105.32421875\n",
      "Batch 4 loss: 39004.8515625\n",
      "Batch 5 loss: 28599.08203125\n",
      "Batch 6 loss: 17998.333984375\n",
      "Batch 7 loss: 26370.404296875\n",
      "Batch 8 loss: 29030.154296875\n",
      "Batch 9 loss: 43485.7890625\n",
      "Batch 10 loss: 22060.91015625\n",
      "Batch 11 loss: 37303.79296875\n",
      "Batch 12 loss: 32758.048828125\n",
      "Batch 13 loss: 36614.70703125\n",
      "Batch 14 loss: 29207.80078125\n",
      "Batch 15 loss: 48455.796875\n",
      "Batch 16 loss: 29170.3359375\n",
      "Batch 17 loss: 14377.4970703125\n",
      "Batch 18 loss: 26626.28125\n",
      "Batch 19 loss: 23609.982421875\n",
      "Batch 20 loss: 31990.7265625\n",
      "Batch 21 loss: 35728.89453125\n",
      "Batch 22 loss: 31864.74609375\n",
      "Batch 23 loss: 31075.88671875\n",
      "Batch 24 loss: 30986.8671875\n",
      "Batch 25 loss: 25203.3515625\n",
      "Batch 26 loss: 25614.6640625\n",
      "Batch 27 loss: 35792.8046875\n",
      "Batch 28 loss: 41230.0703125\n",
      "Batch 29 loss: 29271.2578125\n",
      "Batch 30 loss: 16060.4794921875\n",
      "Batch 31 loss: 28118.322265625\n",
      "Batch 32 loss: 17114.1328125\n",
      "Batch 33 loss: 19635.748046875\n",
      "Batch 34 loss: 29798.978515625\n",
      "Batch 35 loss: 22845.58984375\n",
      "Batch 36 loss: 32774.9140625\n",
      "Batch 37 loss: 14325.65234375\n",
      "Batch 38 loss: 22791.701171875\n",
      "Batch 39 loss: 25396.193359375\n",
      "Batch 40 loss: 17901.83203125\n",
      "Batch 41 loss: 22871.921875\n",
      "Batch 42 loss: 21330.69140625\n",
      "Batch 43 loss: 27825.345703125\n",
      "Batch 44 loss: 35671.578125\n",
      "Batch 45 loss: 31033.03125\n",
      "Batch 46 loss: 30922.03125\n",
      "Batch 47 loss: 29861.513671875\n",
      "Batch 48 loss: 27654.91015625\n",
      "Batch 49 loss: 30549.564453125\n",
      "Batch 50 loss: 17733.10546875\n",
      "Batch 51 loss: 34437.37890625\n",
      "Batch 52 loss: 28110.38671875\n",
      "Batch 53 loss: 26948.822265625\n",
      "Batch 54 loss: 13381.453125\n",
      "Batch 55 loss: 28232.595703125\n",
      "Batch 56 loss: 31062.390625\n",
      "Batch 57 loss: 27212.703125\n",
      "Batch 58 loss: 25590.10546875\n",
      "Batch 59 loss: 22324.01171875\n",
      "Batch 60 loss: 23973.724609375\n",
      "Batch 61 loss: 23039.796875\n",
      "Batch 62 loss: 34014.0\n",
      "Batch 63 loss: 32715.9296875\n",
      "Batch 64 loss: 22575.26171875\n",
      "Batch 65 loss: 36072.2421875\n",
      "Batch 66 loss: 32896.19140625\n",
      "Batch 67 loss: 24698.93359375\n",
      "Batch 68 loss: 26436.89453125\n",
      "Batch 69 loss: 32897.58203125\n",
      "Batch 70 loss: 28271.822265625\n",
      "Batch 71 loss: 24602.267578125\n",
      "Batch 72 loss: 28271.2890625\n",
      "Batch 73 loss: 19016.4140625\n",
      "Batch 74 loss: 28203.767578125\n",
      "Batch 75 loss: 25097.669921875\n",
      "Batch 76 loss: 29568.828125\n",
      "Batch 77 loss: 28153.548828125\n",
      "Batch 78 loss: 30457.3046875\n",
      "Batch 79 loss: 39480.40625\n",
      "Batch 80 loss: 25047.85546875\n",
      "Batch 81 loss: 30678.66796875\n",
      "Batch 82 loss: 35541.6328125\n",
      "Batch 83 loss: 19800.8046875\n",
      "Batch 84 loss: 31755.408203125\n",
      "Batch 85 loss: 32239.109375\n",
      "Batch 86 loss: 19262.708984375\n",
      "Batch 87 loss: 28853.4453125\n",
      "Batch 88 loss: 34433.30859375\n",
      "Batch 89 loss: 27195.998046875\n",
      "Batch 90 loss: 33421.09375\n",
      "Batch 91 loss: 16611.08984375\n",
      "Batch 92 loss: 42620.37109375\n",
      "Batch 93 loss: 33684.921875\n",
      "Batch 94 loss: 16367.1162109375\n",
      "Batch 95 loss: 32362.92578125\n",
      "Batch 96 loss: 32705.771484375\n",
      "Batch 97 loss: 38735.64453125\n",
      "Batch 98 loss: 32959.86328125\n",
      "Batch 99 loss: 37447.56640625\n",
      "Batch 100 loss: 22977.875\n",
      "Batch 101 loss: 27256.6015625\n",
      "Batch 102 loss: 33968.11328125\n",
      "Batch 103 loss: 32176.853515625\n",
      "Batch 104 loss: 30469.765625\n",
      "Batch 105 loss: 35900.06640625\n",
      "Batch 106 loss: 21747.037109375\n",
      "Batch 107 loss: 44850.75\n",
      "Batch 108 loss: 38385.734375\n",
      "Batch 109 loss: 36140.19921875\n",
      "Batch 110 loss: 30385.275390625\n",
      "Batch 111 loss: 26503.33203125\n",
      "Batch 112 loss: 33410.77734375\n",
      "Batch 113 loss: 38607.6796875\n",
      "Batch 114 loss: 33158.6328125\n",
      "Batch 115 loss: 27731.76171875\n",
      "Batch 116 loss: 19039.56640625\n",
      "Batch 117 loss: 24009.279296875\n",
      "Batch 118 loss: 27781.72265625\n",
      "Batch 119 loss: 16901.1640625\n",
      "Batch 120 loss: 29062.34765625\n",
      "Batch 121 loss: 35170.63671875\n",
      "Batch 122 loss: 24886.3984375\n",
      "Batch 123 loss: 26280.298828125\n",
      "Batch 124 loss: 28868.12890625\n",
      "Batch 125 loss: 35274.9765625\n",
      "Batch 126 loss: 34883.99609375\n",
      "Batch 127 loss: 32568.896484375\n",
      "Batch 128 loss: 35364.52734375\n",
      "Batch 129 loss: 21549.478515625\n",
      "Batch 130 loss: 24184.712890625\n",
      "Batch 131 loss: 34499.18359375\n",
      "Batch 132 loss: 29039.0703125\n",
      "Batch 133 loss: 32636.111328125\n",
      "Batch 134 loss: 30207.5546875\n",
      "Batch 135 loss: 28899.63671875\n",
      "Batch 136 loss: 22480.138671875\n",
      "Batch 137 loss: 18174.3125\n",
      "Batch 138 loss: 31790.775390625\n",
      "Batch 139 loss: 17931.197265625\n",
      "Batch 140 loss: 35982.625\n",
      "Batch 141 loss: 21752.53515625\n",
      "Batch 142 loss: 37610.58984375\n",
      "Batch 143 loss: 23747.95703125\n",
      "Batch 144 loss: 27946.53515625\n",
      "Batch 145 loss: 33537.40625\n",
      "Batch 146 loss: 31320.001953125\n",
      "Batch 147 loss: 40460.44140625\n",
      "Batch 148 loss: 27166.994140625\n",
      "Batch 149 loss: 26620.76953125\n",
      "Batch 150 loss: 33124.0625\n",
      "Batch 151 loss: 35550.03125\n",
      "Batch 152 loss: 28644.587890625\n",
      "Batch 153 loss: 31124.2890625\n",
      "Batch 154 loss: 35992.78125\n",
      "Batch 155 loss: 33068.5625\n",
      "Batch 156 loss: 36966.1328125\n",
      "Batch 157 loss: 18113.97265625\n",
      "Batch 158 loss: 29356.751953125\n",
      "Batch 159 loss: 26740.162109375\n",
      "Batch 160 loss: 24042.146484375\n",
      "Batch 161 loss: 20662.603515625\n",
      "Batch 162 loss: 36840.890625\n",
      "Batch 163 loss: 28624.234375\n",
      "Batch 164 loss: 33444.87890625\n",
      "Batch 165 loss: 30817.203125\n",
      "Batch 166 loss: 25739.404296875\n",
      "Batch 167 loss: 32824.16015625\n",
      "Batch 168 loss: 29882.595703125\n",
      "Batch 169 loss: 34173.03125\n",
      "Batch 170 loss: 32755.05078125\n",
      "Batch 171 loss: 27364.21484375\n",
      "Batch 172 loss: 28900.1796875\n",
      "Batch 173 loss: 31693.27734375\n",
      "Batch 174 loss: 28027.34765625\n",
      "Batch 175 loss: 19835.921875\n",
      "Batch 176 loss: 21925.662109375\n",
      "Batch 177 loss: 34476.49609375\n",
      "Batch 178 loss: 33532.765625\n",
      "Batch 179 loss: 37923.19140625\n",
      "Batch 180 loss: 33200.375\n",
      "Batch 181 loss: 34673.703125\n",
      "Batch 182 loss: 20636.27734375\n",
      "Batch 183 loss: 34582.3359375\n",
      "Batch 184 loss: 28459.609375\n",
      "Batch 185 loss: 32157.681640625\n",
      "Batch 186 loss: 36793.0078125\n",
      "Batch 187 loss: 25978.025390625\n",
      "Batch 188 loss: 31878.4921875\n",
      "Batch 189 loss: 26557.890625\n",
      "Batch 190 loss: 35040.5859375\n",
      "Batch 191 loss: 32852.16015625\n",
      "Batch 192 loss: 32877.8046875\n",
      "Batch 193 loss: 33885.0234375\n",
      "Batch 194 loss: 33400.4453125\n",
      "Batch 195 loss: 17822.4453125\n",
      "Batch 196 loss: 40172.4140625\n",
      "Batch 197 loss: 29381.171875\n",
      "Batch 198 loss: 34707.26953125\n",
      "Batch 199 loss: 41256.5234375\n",
      "Batch 200 loss: 28928.65234375\n",
      "Batch 201 loss: 16758.7890625\n",
      "Batch 202 loss: 19122.15625\n",
      "Batch 203 loss: 30431.26953125\n",
      "Batch 204 loss: 44053.125\n",
      "Batch 205 loss: 25299.36328125\n",
      "Batch 206 loss: 21771.70703125\n",
      "Batch 207 loss: 28233.71484375\n",
      "Batch 208 loss: 13964.4580078125\n",
      "Batch 209 loss: 23635.03125\n",
      "Batch 210 loss: 34189.23828125\n",
      "Batch 211 loss: 39571.90234375\n",
      "Batch 212 loss: 38946.48828125\n",
      "Batch 213 loss: 29029.8359375\n",
      "Batch 214 loss: 24209.62890625\n",
      "Batch 215 loss: 38779.546875\n",
      "Batch 216 loss: 36408.78125\n",
      "Batch 217 loss: 15605.099609375\n",
      "Batch 218 loss: 30444.85546875\n",
      "Batch 219 loss: 35551.72265625\n",
      "Batch 220 loss: 25911.7421875\n",
      "Batch 221 loss: 28237.078125\n",
      "Batch 222 loss: 16544.3125\n",
      "Batch 223 loss: 30022.69921875\n",
      "Batch 224 loss: 39085.546875\n",
      "Batch 225 loss: 24203.666015625\n",
      "Batch 226 loss: 33039.8828125\n",
      "Batch 227 loss: 21179.08984375\n",
      "Batch 228 loss: 32777.2109375\n",
      "Batch 229 loss: 36080.10546875\n",
      "Batch 230 loss: 30134.78125\n",
      "Batch 231 loss: 24442.345703125\n",
      "Batch 232 loss: 42369.2890625\n",
      "Batch 233 loss: 47617.49609375\n",
      "Batch 234 loss: 28794.01171875\n",
      "Batch 235 loss: 33492.0703125\n",
      "Batch 236 loss: 33250.49609375\n",
      "Batch 237 loss: 25960.58984375\n",
      "Batch 238 loss: 29321.73828125\n",
      "Batch 239 loss: 25978.94140625\n",
      "Batch 240 loss: 24558.671875\n",
      "Batch 241 loss: 30049.07421875\n",
      "Batch 242 loss: 32873.9921875\n",
      "Batch 243 loss: 25114.443359375\n",
      "Batch 244 loss: 28539.25\n",
      "Batch 245 loss: 26670.15234375\n",
      "Batch 246 loss: 27201.181640625\n",
      "Batch 247 loss: 34460.01171875\n",
      "Batch 248 loss: 36473.1484375\n",
      "Batch 249 loss: 28969.337890625\n",
      "### Epoch 5\n",
      "Batch 0 loss: 31469.9453125\n",
      "Batch 1 loss: 26626.853515625\n",
      "Batch 2 loss: 19348.23828125\n",
      "Batch 3 loss: 27866.494140625\n",
      "Batch 4 loss: 20634.587890625\n",
      "Batch 5 loss: 23667.958984375\n",
      "Batch 6 loss: 35899.80859375\n",
      "Batch 7 loss: 20711.83984375\n",
      "Batch 8 loss: 36816.76171875\n",
      "Batch 9 loss: 33411.80859375\n",
      "Batch 10 loss: 27749.501953125\n",
      "Batch 11 loss: 36426.984375\n",
      "Batch 12 loss: 45013.625\n",
      "Batch 13 loss: 37488.38671875\n",
      "Batch 14 loss: 9912.8349609375\n",
      "Batch 15 loss: 23524.865234375\n",
      "Batch 16 loss: 28255.736328125\n",
      "Batch 17 loss: 18837.421875\n",
      "Batch 18 loss: 33107.07421875\n",
      "Batch 19 loss: 24413.20703125\n",
      "Batch 20 loss: 37464.78515625\n",
      "Batch 21 loss: 34495.265625\n",
      "Batch 22 loss: 18330.69921875\n",
      "Batch 23 loss: 29205.15625\n",
      "Batch 24 loss: 25084.548828125\n",
      "Batch 25 loss: 32975.91015625\n",
      "Batch 26 loss: 25312.0\n",
      "Batch 27 loss: 40882.78515625\n",
      "Batch 28 loss: 24801.501953125\n",
      "Batch 29 loss: 32749.841796875\n",
      "Batch 30 loss: 27296.0625\n",
      "Batch 31 loss: 49129.9453125\n",
      "Batch 32 loss: 25787.919921875\n",
      "Batch 33 loss: 29089.837890625\n",
      "Batch 34 loss: 21730.25\n",
      "Batch 35 loss: 29987.521484375\n",
      "Batch 36 loss: 20298.9765625\n",
      "Batch 37 loss: 30465.666015625\n",
      "Batch 38 loss: 36270.09375\n",
      "Batch 39 loss: 29023.56640625\n",
      "Batch 40 loss: 28698.28125\n",
      "Batch 41 loss: 32073.1875\n",
      "Batch 42 loss: 31154.419921875\n",
      "Batch 43 loss: 20961.240234375\n",
      "Batch 44 loss: 29567.10546875\n",
      "Batch 45 loss: 39255.17578125\n",
      "Batch 46 loss: 27577.90625\n",
      "Batch 47 loss: 26775.076171875\n",
      "Batch 48 loss: 43050.3828125\n",
      "Batch 49 loss: 23410.26171875\n",
      "Batch 50 loss: 19593.529296875\n",
      "Batch 51 loss: 29247.607421875\n",
      "Batch 52 loss: 28509.40234375\n",
      "Batch 53 loss: 16066.484375\n",
      "Batch 54 loss: 32924.28125\n",
      "Batch 55 loss: 33833.609375\n",
      "Batch 56 loss: 26933.091796875\n",
      "Batch 57 loss: 33492.9765625\n",
      "Batch 58 loss: 31316.44140625\n",
      "Batch 59 loss: 15270.314453125\n",
      "Batch 60 loss: 24043.08203125\n",
      "Batch 61 loss: 38707.0625\n",
      "Batch 62 loss: 25118.12109375\n",
      "Batch 63 loss: 37649.390625\n",
      "Batch 64 loss: 27540.6171875\n",
      "Batch 65 loss: 26016.01171875\n",
      "Batch 66 loss: 20141.943359375\n",
      "Batch 67 loss: 33932.23046875\n",
      "Batch 68 loss: 31581.955078125\n",
      "Batch 69 loss: 27118.79296875\n",
      "Batch 70 loss: 27267.427734375\n",
      "Batch 71 loss: 32801.328125\n",
      "Batch 72 loss: 44851.77734375\n",
      "Batch 73 loss: 36386.671875\n",
      "Batch 74 loss: 29027.30859375\n",
      "Batch 75 loss: 28320.4140625\n",
      "Batch 76 loss: 32361.173828125\n",
      "Batch 77 loss: 26208.453125\n",
      "Batch 78 loss: 28329.390625\n",
      "Batch 79 loss: 31118.166015625\n",
      "Batch 80 loss: 23034.0703125\n",
      "Batch 81 loss: 27980.484375\n",
      "Batch 82 loss: 31079.05078125\n",
      "Batch 83 loss: 38699.7578125\n",
      "Batch 84 loss: 27808.03125\n",
      "Batch 85 loss: 29742.0625\n",
      "Batch 86 loss: 16026.03515625\n",
      "Batch 87 loss: 19048.466796875\n",
      "Batch 88 loss: 18235.65625\n",
      "Batch 89 loss: 25707.9140625\n",
      "Batch 90 loss: 38503.63671875\n",
      "Batch 91 loss: 32016.533203125\n",
      "Batch 92 loss: 21534.75390625\n",
      "Batch 93 loss: 25568.33203125\n",
      "Batch 94 loss: 25139.271484375\n",
      "Batch 95 loss: 26451.416015625\n",
      "Batch 96 loss: 17739.947265625\n",
      "Batch 97 loss: 18745.806640625\n",
      "Batch 98 loss: 24252.673828125\n",
      "Batch 99 loss: 41377.09375\n",
      "Batch 100 loss: 30931.357421875\n",
      "Batch 101 loss: 32266.3671875\n",
      "Batch 102 loss: 30869.513671875\n",
      "Batch 103 loss: 30783.4921875\n",
      "Batch 104 loss: 20732.24609375\n",
      "Batch 105 loss: 17441.798828125\n",
      "Batch 106 loss: 33389.12109375\n",
      "Batch 107 loss: 33453.17578125\n",
      "Batch 108 loss: 27810.26953125\n",
      "Batch 109 loss: 29241.716796875\n",
      "Batch 110 loss: 35354.234375\n",
      "Batch 111 loss: 39195.59765625\n",
      "Batch 112 loss: 34028.28515625\n",
      "Batch 113 loss: 22296.76171875\n",
      "Batch 114 loss: 30555.818359375\n",
      "Batch 115 loss: 30297.3828125\n",
      "Batch 116 loss: 27211.427734375\n",
      "Batch 117 loss: 26631.318359375\n",
      "Batch 118 loss: 29630.90234375\n",
      "Batch 119 loss: 21825.58203125\n",
      "Batch 120 loss: 27662.740234375\n",
      "Batch 121 loss: 34280.578125\n",
      "Batch 122 loss: 33880.8203125\n",
      "Batch 123 loss: 24461.91796875\n",
      "Batch 124 loss: 30124.66015625\n",
      "Batch 125 loss: 36728.44921875\n",
      "Batch 126 loss: 18582.642578125\n",
      "Batch 127 loss: 15030.728515625\n",
      "Batch 128 loss: 23149.77734375\n",
      "Batch 129 loss: 36911.01171875\n",
      "Batch 130 loss: 23800.740234375\n",
      "Batch 131 loss: 28124.001953125\n",
      "Batch 132 loss: 25768.6875\n",
      "Batch 133 loss: 37803.52734375\n",
      "Batch 134 loss: 24809.53515625\n",
      "Batch 135 loss: 33455.34375\n",
      "Batch 136 loss: 32357.953125\n",
      "Batch 137 loss: 38405.8515625\n",
      "Batch 138 loss: 40402.7421875\n",
      "Batch 139 loss: 33398.69140625\n",
      "Batch 140 loss: 16531.494140625\n",
      "Batch 141 loss: 30919.853515625\n",
      "Batch 142 loss: 38166.98828125\n",
      "Batch 143 loss: 27824.71875\n",
      "Batch 144 loss: 15823.81640625\n",
      "Batch 145 loss: 52950.1640625\n",
      "Batch 146 loss: 23881.15234375\n",
      "Batch 147 loss: 37898.609375\n",
      "Batch 148 loss: 19922.83984375\n",
      "Batch 149 loss: 27277.431640625\n",
      "Batch 150 loss: 31027.87890625\n",
      "Batch 151 loss: 20391.974609375\n",
      "Batch 152 loss: 29782.896484375\n",
      "Batch 153 loss: 11945.1083984375\n",
      "Batch 154 loss: 40588.02734375\n",
      "Batch 155 loss: 27935.150390625\n",
      "Batch 156 loss: 21881.24609375\n",
      "Batch 157 loss: 28744.978515625\n",
      "Batch 158 loss: 34442.484375\n",
      "Batch 159 loss: 34494.515625\n",
      "Batch 160 loss: 37193.2421875\n",
      "Batch 161 loss: 20004.9296875\n",
      "Batch 162 loss: 32761.64453125\n",
      "Batch 163 loss: 32460.44921875\n",
      "Batch 164 loss: 16678.3203125\n",
      "Batch 165 loss: 37175.49609375\n",
      "Batch 166 loss: 24588.779296875\n",
      "Batch 167 loss: 29952.173828125\n",
      "Batch 168 loss: 28293.888671875\n",
      "Batch 169 loss: 33635.3515625\n",
      "Batch 170 loss: 33129.796875\n",
      "Batch 171 loss: 28823.798828125\n",
      "Batch 172 loss: 34974.5546875\n",
      "Batch 173 loss: 25628.29296875\n",
      "Batch 174 loss: 32327.392578125\n",
      "Batch 175 loss: 25599.69921875\n",
      "Batch 176 loss: 41697.3203125\n",
      "Batch 177 loss: 25464.513671875\n",
      "Batch 178 loss: 35240.59375\n",
      "Batch 179 loss: 34077.875\n",
      "Batch 180 loss: 40083.32421875\n",
      "Batch 181 loss: 32754.583984375\n",
      "Batch 182 loss: 34706.26171875\n",
      "Batch 183 loss: 40534.78125\n",
      "Batch 184 loss: 36209.39453125\n",
      "Batch 185 loss: 25131.525390625\n",
      "Batch 186 loss: 15631.7421875\n",
      "Batch 187 loss: 27590.037109375\n",
      "Batch 188 loss: 37010.31640625\n",
      "Batch 189 loss: 19615.59765625\n",
      "Batch 190 loss: 26023.84375\n",
      "Batch 191 loss: 34770.12109375\n",
      "Batch 192 loss: 31642.572265625\n",
      "Batch 193 loss: 30688.48046875\n",
      "Batch 194 loss: 34598.08203125\n",
      "Batch 195 loss: 19855.15625\n",
      "Batch 196 loss: 23896.16796875\n",
      "Batch 197 loss: 41056.75390625\n",
      "Batch 198 loss: 40754.67578125\n",
      "Batch 199 loss: 29071.353515625\n",
      "Batch 200 loss: 28486.1171875\n",
      "Batch 201 loss: 35935.14453125\n",
      "Batch 202 loss: 21764.828125\n",
      "Batch 203 loss: 24257.109375\n",
      "Batch 204 loss: 21821.671875\n",
      "Batch 205 loss: 26770.677734375\n",
      "Batch 206 loss: 21025.158203125\n",
      "Batch 207 loss: 33146.07421875\n",
      "Batch 208 loss: 41602.96875\n",
      "Batch 209 loss: 38729.24609375\n",
      "Batch 210 loss: 25907.30859375\n",
      "Batch 211 loss: 17059.82421875\n",
      "Batch 212 loss: 40236.91796875\n",
      "Batch 213 loss: 20612.552734375\n",
      "Batch 214 loss: 47993.0234375\n",
      "Batch 215 loss: 36071.4140625\n",
      "Batch 216 loss: 33707.765625\n",
      "Batch 217 loss: 26705.208984375\n",
      "Batch 218 loss: 24922.669921875\n",
      "Batch 219 loss: 32681.646484375\n",
      "Batch 220 loss: 20943.80859375\n",
      "Batch 221 loss: 30836.578125\n",
      "Batch 222 loss: 27640.91015625\n",
      "Batch 223 loss: 30139.994140625\n",
      "Batch 224 loss: 31635.703125\n",
      "Batch 225 loss: 44543.9296875\n",
      "Batch 226 loss: 15541.5\n",
      "Batch 227 loss: 23745.83984375\n",
      "Batch 228 loss: 22508.91015625\n",
      "Batch 229 loss: 21781.279296875\n",
      "Batch 230 loss: 50305.05078125\n",
      "Batch 231 loss: 38874.69921875\n",
      "Batch 232 loss: 38627.7265625\n",
      "Batch 233 loss: 26132.212890625\n",
      "Batch 234 loss: 44462.4453125\n",
      "Batch 235 loss: 24002.060546875\n",
      "Batch 236 loss: 20458.3046875\n",
      "Batch 237 loss: 42488.65234375\n",
      "Batch 238 loss: 26051.947265625\n",
      "Batch 239 loss: 25621.259765625\n",
      "Batch 240 loss: 24892.884765625\n",
      "Batch 241 loss: 34816.63671875\n",
      "Batch 242 loss: 24049.32421875\n",
      "Batch 243 loss: 29596.693359375\n",
      "Batch 244 loss: 36245.0\n",
      "Batch 245 loss: 18496.26171875\n",
      "Batch 246 loss: 17060.130859375\n",
      "Batch 247 loss: 24351.337890625\n",
      "Batch 248 loss: 47895.609375\n",
      "Batch 249 loss: 34311.25\n",
      "### Epoch 6\n",
      "Batch 0 loss: 35079.08203125\n",
      "Batch 1 loss: 27897.380859375\n",
      "Batch 2 loss: 24835.421875\n",
      "Batch 3 loss: 31247.7265625\n",
      "Batch 4 loss: 23754.203125\n",
      "Batch 5 loss: 35770.44140625\n",
      "Batch 6 loss: 32630.826171875\n",
      "Batch 7 loss: 33294.515625\n",
      "Batch 8 loss: 24467.029296875\n",
      "Batch 9 loss: 32739.326171875\n",
      "Batch 10 loss: 33160.40234375\n",
      "Batch 11 loss: 35341.375\n",
      "Batch 12 loss: 38878.6640625\n",
      "Batch 13 loss: 27254.884765625\n",
      "Batch 14 loss: 22681.345703125\n",
      "Batch 15 loss: 23794.212890625\n",
      "Batch 16 loss: 19053.572265625\n",
      "Batch 17 loss: 42653.1953125\n",
      "Batch 18 loss: 23478.693359375\n",
      "Batch 19 loss: 21554.388671875\n",
      "Batch 20 loss: 38933.66015625\n",
      "Batch 21 loss: 18298.7109375\n",
      "Batch 22 loss: 20468.689453125\n",
      "Batch 23 loss: 40049.1796875\n",
      "Batch 24 loss: 24311.37890625\n",
      "Batch 25 loss: 41188.47265625\n",
      "Batch 26 loss: 38237.37109375\n",
      "Batch 27 loss: 35924.890625\n",
      "Batch 28 loss: 26079.439453125\n",
      "Batch 29 loss: 16983.724609375\n",
      "Batch 30 loss: 22199.8515625\n",
      "Batch 31 loss: 35843.953125\n",
      "Batch 32 loss: 34174.921875\n",
      "Batch 33 loss: 33149.71484375\n",
      "Batch 34 loss: 28646.431640625\n",
      "Batch 35 loss: 34407.07421875\n",
      "Batch 36 loss: 42173.1328125\n",
      "Batch 37 loss: 32761.310546875\n",
      "Batch 38 loss: 40060.27734375\n",
      "Batch 39 loss: 28907.4453125\n",
      "Batch 40 loss: 30320.443359375\n",
      "Batch 41 loss: 23165.69140625\n",
      "Batch 42 loss: 25662.646484375\n",
      "Batch 43 loss: 32897.89453125\n",
      "Batch 44 loss: 21859.91015625\n",
      "Batch 45 loss: 29815.2578125\n",
      "Batch 46 loss: 31540.189453125\n",
      "Batch 47 loss: 23796.60546875\n",
      "Batch 48 loss: 32313.560546875\n",
      "Batch 49 loss: 41515.609375\n",
      "Batch 50 loss: 27262.2734375\n",
      "Batch 51 loss: 32942.875\n",
      "Batch 52 loss: 40798.02734375\n",
      "Batch 53 loss: 31163.572265625\n",
      "Batch 54 loss: 40843.63671875\n",
      "Batch 55 loss: 21412.060546875\n",
      "Batch 56 loss: 40529.0\n",
      "Batch 57 loss: 15724.8486328125\n",
      "Batch 58 loss: 35781.30078125\n",
      "Batch 59 loss: 27311.76171875\n",
      "Batch 60 loss: 18773.75\n",
      "Batch 61 loss: 21100.76171875\n",
      "Batch 62 loss: 24578.978515625\n",
      "Batch 63 loss: 18100.265625\n",
      "Batch 64 loss: 28285.0\n",
      "Batch 65 loss: 30170.734375\n",
      "Batch 66 loss: 37856.671875\n",
      "Batch 67 loss: 27678.462890625\n",
      "Batch 68 loss: 22739.9453125\n",
      "Batch 69 loss: 33946.46484375\n",
      "Batch 70 loss: 21883.015625\n",
      "Batch 71 loss: 25627.181640625\n",
      "Batch 72 loss: 16271.4208984375\n",
      "Batch 73 loss: 22486.373046875\n",
      "Batch 74 loss: 31218.263671875\n",
      "Batch 75 loss: 24174.615234375\n",
      "Batch 76 loss: 19483.037109375\n",
      "Batch 77 loss: 35941.57421875\n",
      "Batch 78 loss: 18927.55859375\n",
      "Batch 79 loss: 32520.501953125\n",
      "Batch 80 loss: 21209.46875\n",
      "Batch 81 loss: 22570.861328125\n",
      "Batch 82 loss: 34802.578125\n",
      "Batch 83 loss: 26027.15625\n",
      "Batch 84 loss: 24126.51171875\n",
      "Batch 85 loss: 30343.484375\n",
      "Batch 86 loss: 29558.45703125\n",
      "Batch 87 loss: 33359.9296875\n",
      "Batch 88 loss: 32121.62890625\n",
      "Batch 89 loss: 29958.33984375\n",
      "Batch 90 loss: 37271.8203125\n",
      "Batch 91 loss: 29718.900390625\n",
      "Batch 92 loss: 19787.095703125\n",
      "Batch 93 loss: 24357.7890625\n",
      "Batch 94 loss: 32014.751953125\n",
      "Batch 95 loss: 33359.484375\n",
      "Batch 96 loss: 25325.541015625\n",
      "Batch 97 loss: 32664.0390625\n",
      "Batch 98 loss: 29110.23828125\n",
      "Batch 99 loss: 31546.125\n",
      "Batch 100 loss: 38343.9609375\n",
      "Batch 101 loss: 16019.88671875\n",
      "Batch 102 loss: 27071.6875\n",
      "Batch 103 loss: 31201.162109375\n",
      "Batch 104 loss: 25152.87109375\n",
      "Batch 105 loss: 25768.41796875\n",
      "Batch 106 loss: 21922.94921875\n",
      "Batch 107 loss: 42009.09375\n",
      "Batch 108 loss: 29214.6875\n",
      "Batch 109 loss: 49055.18359375\n",
      "Batch 110 loss: 31925.9609375\n",
      "Batch 111 loss: 17688.380859375\n",
      "Batch 112 loss: 39881.1640625\n",
      "Batch 113 loss: 27499.771484375\n",
      "Batch 114 loss: 26222.048828125\n",
      "Batch 115 loss: 25036.287109375\n",
      "Batch 116 loss: 31540.482421875\n",
      "Batch 117 loss: 24881.60546875\n",
      "Batch 118 loss: 25218.271484375\n",
      "Batch 119 loss: 31432.0\n",
      "Batch 120 loss: 23337.494140625\n",
      "Batch 121 loss: 21973.859375\n",
      "Batch 122 loss: 34372.69921875\n",
      "Batch 123 loss: 32760.080078125\n",
      "Batch 124 loss: 20318.87890625\n",
      "Batch 125 loss: 35112.3203125\n",
      "Batch 126 loss: 29318.86328125\n",
      "Batch 127 loss: 34150.0703125\n",
      "Batch 128 loss: 28772.578125\n",
      "Batch 129 loss: 19015.83203125\n",
      "Batch 130 loss: 27503.1640625\n",
      "Batch 131 loss: 34447.06640625\n",
      "Batch 132 loss: 26490.45703125\n",
      "Batch 133 loss: 27321.283203125\n",
      "Batch 134 loss: 29194.173828125\n",
      "Batch 135 loss: 35793.57421875\n",
      "Batch 136 loss: 31258.65625\n",
      "Batch 137 loss: 26980.0546875\n",
      "Batch 138 loss: 30420.99609375\n",
      "Batch 139 loss: 30161.33203125\n",
      "Batch 140 loss: 28006.205078125\n",
      "Batch 141 loss: 27634.951171875\n",
      "Batch 142 loss: 30647.41796875\n",
      "Batch 143 loss: 26624.7109375\n",
      "Batch 144 loss: 24928.7734375\n",
      "Batch 145 loss: 31369.556640625\n",
      "Batch 146 loss: 30407.802734375\n",
      "Batch 147 loss: 33764.3046875\n",
      "Batch 148 loss: 32756.228515625\n",
      "Batch 149 loss: 20335.365234375\n",
      "Batch 150 loss: 32823.27734375\n",
      "Batch 151 loss: 18786.1640625\n",
      "Batch 152 loss: 24708.82421875\n",
      "Batch 153 loss: 22931.345703125\n",
      "Batch 154 loss: 29099.564453125\n",
      "Batch 155 loss: 32316.1015625\n",
      "Batch 156 loss: 34815.49609375\n",
      "Batch 157 loss: 25103.90234375\n",
      "Batch 158 loss: 45148.0625\n",
      "Batch 159 loss: 28423.578125\n",
      "Batch 160 loss: 32459.509765625\n",
      "Batch 161 loss: 24185.146484375\n",
      "Batch 162 loss: 36283.08203125\n",
      "Batch 163 loss: 33560.7109375\n",
      "Batch 164 loss: 28485.396484375\n",
      "Batch 165 loss: 27708.505859375\n",
      "Batch 166 loss: 29871.98828125\n",
      "Batch 167 loss: 31630.603515625\n",
      "Batch 168 loss: 26519.3984375\n",
      "Batch 169 loss: 28016.130859375\n",
      "Batch 170 loss: 37197.40625\n",
      "Batch 171 loss: 25551.279296875\n",
      "Batch 172 loss: 29116.90625\n",
      "Batch 173 loss: 24310.50390625\n",
      "Batch 174 loss: 19788.78125\n",
      "Batch 175 loss: 27014.0234375\n",
      "Batch 176 loss: 33895.15234375\n",
      "Batch 177 loss: 24778.53125\n",
      "Batch 178 loss: 22390.35546875\n",
      "Batch 179 loss: 27636.6171875\n",
      "Batch 180 loss: 26732.662109375\n",
      "Batch 181 loss: 13327.974609375\n",
      "Batch 182 loss: 28821.7109375\n",
      "Batch 183 loss: 17451.17578125\n",
      "Batch 184 loss: 48874.3203125\n",
      "Batch 185 loss: 18366.42578125\n",
      "Batch 186 loss: 35835.8984375\n",
      "Batch 187 loss: 31201.6953125\n",
      "Batch 188 loss: 32777.48828125\n",
      "Batch 189 loss: 28487.515625\n",
      "Batch 190 loss: 19960.515625\n",
      "Batch 191 loss: 23516.9140625\n",
      "Batch 192 loss: 39808.98828125\n",
      "Batch 193 loss: 37155.11328125\n",
      "Batch 194 loss: 32899.0078125\n",
      "Batch 195 loss: 29739.369140625\n",
      "Batch 196 loss: 52023.12890625\n",
      "Batch 197 loss: 27458.68359375\n",
      "Batch 198 loss: 26928.3984375\n",
      "Batch 199 loss: 34039.9453125\n",
      "Batch 200 loss: 19800.07421875\n",
      "Batch 201 loss: 29880.431640625\n",
      "Batch 202 loss: 25691.015625\n",
      "Batch 203 loss: 33186.10546875\n",
      "Batch 204 loss: 27343.2890625\n",
      "Batch 205 loss: 28625.908203125\n",
      "Batch 206 loss: 36195.66796875\n",
      "Batch 207 loss: 46242.4453125\n",
      "Batch 208 loss: 33853.89453125\n",
      "Batch 209 loss: 26631.80078125\n",
      "Batch 210 loss: 29139.703125\n",
      "Batch 211 loss: 26883.2890625\n",
      "Batch 212 loss: 35461.07421875\n",
      "Batch 213 loss: 26106.314453125\n",
      "Batch 214 loss: 40018.05078125\n",
      "Batch 215 loss: 30218.794921875\n",
      "Batch 216 loss: 30366.51171875\n",
      "Batch 217 loss: 29202.552734375\n",
      "Batch 218 loss: 41189.4921875\n",
      "Batch 219 loss: 45423.5625\n",
      "Batch 220 loss: 23269.82421875\n",
      "Batch 221 loss: 26723.796875\n",
      "Batch 222 loss: 39048.79296875\n",
      "Batch 223 loss: 35354.84765625\n",
      "Batch 224 loss: 18775.3828125\n",
      "Batch 225 loss: 27120.890625\n",
      "Batch 226 loss: 26788.3984375\n",
      "Batch 227 loss: 18298.103515625\n",
      "Batch 228 loss: 18847.572265625\n",
      "Batch 229 loss: 33575.765625\n",
      "Batch 230 loss: 18627.904296875\n",
      "Batch 231 loss: 27979.517578125\n",
      "Batch 232 loss: 34902.0625\n",
      "Batch 233 loss: 24447.80859375\n",
      "Batch 234 loss: 21141.228515625\n",
      "Batch 235 loss: 25848.642578125\n",
      "Batch 236 loss: 38586.109375\n",
      "Batch 237 loss: 26133.251953125\n",
      "Batch 238 loss: 40272.42578125\n",
      "Batch 239 loss: 41598.8359375\n",
      "Batch 240 loss: 29135.916015625\n",
      "Batch 241 loss: 22207.744140625\n",
      "Batch 242 loss: 21263.255859375\n",
      "Batch 243 loss: 36271.46875\n",
      "Batch 244 loss: 38108.875\n",
      "Batch 245 loss: 34975.37890625\n",
      "Batch 246 loss: 25484.734375\n",
      "Batch 247 loss: 34508.24609375\n",
      "Batch 248 loss: 21329.09375\n",
      "Batch 249 loss: 38600.57421875\n",
      "### Epoch 7\n",
      "Batch 0 loss: 29248.240234375\n",
      "Batch 1 loss: 19882.099609375\n",
      "Batch 2 loss: 26388.498046875\n",
      "Batch 3 loss: 21841.626953125\n",
      "Batch 4 loss: 27665.767578125\n",
      "Batch 5 loss: 28996.24609375\n",
      "Batch 6 loss: 31681.306640625\n",
      "Batch 7 loss: 26940.2421875\n",
      "Batch 8 loss: 27924.16015625\n",
      "Batch 9 loss: 24423.4140625\n",
      "Batch 10 loss: 24776.751953125\n",
      "Batch 11 loss: 23726.75390625\n",
      "Batch 12 loss: 29049.9140625\n",
      "Batch 13 loss: 18861.177734375\n",
      "Batch 14 loss: 32932.7734375\n",
      "Batch 15 loss: 19748.28125\n",
      "Batch 16 loss: 25541.248046875\n",
      "Batch 17 loss: 30856.404296875\n",
      "Batch 18 loss: 32201.71484375\n",
      "Batch 19 loss: 26476.26171875\n",
      "Batch 20 loss: 30215.44140625\n",
      "Batch 21 loss: 40359.78125\n",
      "Batch 22 loss: 19258.01953125\n",
      "Batch 23 loss: 22425.51953125\n",
      "Batch 24 loss: 34865.09375\n",
      "Batch 25 loss: 45989.234375\n",
      "Batch 26 loss: 20804.595703125\n",
      "Batch 27 loss: 16023.791015625\n",
      "Batch 28 loss: 21829.900390625\n",
      "Batch 29 loss: 18931.935546875\n",
      "Batch 30 loss: 25997.8203125\n",
      "Batch 31 loss: 25397.34765625\n",
      "Batch 32 loss: 25676.072265625\n",
      "Batch 33 loss: 13960.1875\n",
      "Batch 34 loss: 33739.08203125\n",
      "Batch 35 loss: 29545.615234375\n",
      "Batch 36 loss: 33643.52734375\n",
      "Batch 37 loss: 45183.6484375\n",
      "Batch 38 loss: 36249.75\n",
      "Batch 39 loss: 22132.763671875\n",
      "Batch 40 loss: 26493.37890625\n",
      "Batch 41 loss: 30501.1640625\n",
      "Batch 42 loss: 40485.12890625\n",
      "Batch 43 loss: 25740.658203125\n",
      "Batch 44 loss: 40604.6015625\n",
      "Batch 45 loss: 45374.875\n",
      "Batch 46 loss: 34171.375\n",
      "Batch 47 loss: 25426.970703125\n",
      "Batch 48 loss: 35394.2890625\n",
      "Batch 49 loss: 27008.33203125\n",
      "Batch 50 loss: 22135.486328125\n",
      "Batch 51 loss: 26238.5546875\n",
      "Batch 52 loss: 23886.4296875\n",
      "Batch 53 loss: 28070.44140625\n",
      "Batch 54 loss: 26038.904296875\n",
      "Batch 55 loss: 37516.29296875\n",
      "Batch 56 loss: 24863.380859375\n",
      "Batch 57 loss: 45875.203125\n",
      "Batch 58 loss: 26710.880859375\n",
      "Batch 59 loss: 24204.47265625\n",
      "Batch 60 loss: 27544.36328125\n",
      "Batch 61 loss: 19848.61328125\n",
      "Batch 62 loss: 22341.1484375\n",
      "Batch 63 loss: 24625.626953125\n",
      "Batch 64 loss: 17427.2890625\n",
      "Batch 65 loss: 19892.005859375\n",
      "Batch 66 loss: 55524.34765625\n",
      "Batch 67 loss: 19624.044921875\n",
      "Batch 68 loss: 30356.43359375\n",
      "Batch 69 loss: 26964.6796875\n",
      "Batch 70 loss: 19800.384765625\n",
      "Batch 71 loss: 32751.513671875\n",
      "Batch 72 loss: 36527.0625\n",
      "Batch 73 loss: 21284.6953125\n",
      "Batch 74 loss: 33321.9296875\n",
      "Batch 75 loss: 34126.0703125\n",
      "Batch 76 loss: 33810.5390625\n",
      "Batch 77 loss: 14084.9482421875\n",
      "Batch 78 loss: 41701.2890625\n",
      "Batch 79 loss: 36670.25390625\n",
      "Batch 80 loss: 35089.87890625\n",
      "Batch 81 loss: 18446.6796875\n",
      "Batch 82 loss: 33154.1875\n",
      "Batch 83 loss: 28155.310546875\n",
      "Batch 84 loss: 45658.515625\n",
      "Batch 85 loss: 36684.515625\n",
      "Batch 86 loss: 27485.423828125\n",
      "Batch 87 loss: 27791.8828125\n",
      "Batch 88 loss: 30860.982421875\n",
      "Batch 89 loss: 34389.1640625\n",
      "Batch 90 loss: 29106.6484375\n",
      "Batch 91 loss: 19773.6640625\n",
      "Batch 92 loss: 24090.716796875\n",
      "Batch 93 loss: 29670.52734375\n",
      "Batch 94 loss: 39063.31640625\n",
      "Batch 95 loss: 34421.0390625\n",
      "Batch 96 loss: 27309.96875\n",
      "Batch 97 loss: 17791.51171875\n",
      "Batch 98 loss: 35248.90625\n",
      "Batch 99 loss: 20680.453125\n",
      "Batch 100 loss: 26387.763671875\n",
      "Batch 101 loss: 32995.80078125\n",
      "Batch 102 loss: 25758.6328125\n",
      "Batch 103 loss: 32050.279296875\n",
      "Batch 104 loss: 41939.265625\n",
      "Batch 105 loss: 28892.541015625\n",
      "Batch 106 loss: 32938.9140625\n",
      "Batch 107 loss: 43694.90625\n",
      "Batch 108 loss: 31710.40625\n",
      "Batch 109 loss: 22171.83984375\n",
      "Batch 110 loss: 36008.87890625\n",
      "Batch 111 loss: 28147.61328125\n",
      "Batch 112 loss: 22686.12890625\n",
      "Batch 113 loss: 20256.78125\n",
      "Batch 114 loss: 37193.19921875\n",
      "Batch 115 loss: 26316.390625\n",
      "Batch 116 loss: 26286.673828125\n",
      "Batch 117 loss: 30998.904296875\n",
      "Batch 118 loss: 30130.9140625\n",
      "Batch 119 loss: 26354.2421875\n",
      "Batch 120 loss: 19437.033203125\n",
      "Batch 121 loss: 39812.1640625\n",
      "Batch 122 loss: 27016.10546875\n",
      "Batch 123 loss: 27660.6640625\n",
      "Batch 124 loss: 29454.140625\n",
      "Batch 125 loss: 26558.56640625\n",
      "Batch 126 loss: 23875.04296875\n",
      "Batch 127 loss: 21374.521484375\n",
      "Batch 128 loss: 39653.9375\n",
      "Batch 129 loss: 30258.8125\n",
      "Batch 130 loss: 43650.76171875\n",
      "Batch 131 loss: 23530.201171875\n",
      "Batch 132 loss: 37530.8984375\n",
      "Batch 133 loss: 27630.78125\n",
      "Batch 134 loss: 33129.046875\n",
      "Batch 135 loss: 30595.689453125\n",
      "Batch 136 loss: 49258.03125\n",
      "Batch 137 loss: 24754.544921875\n",
      "Batch 138 loss: 31276.61328125\n",
      "Batch 139 loss: 24933.3828125\n",
      "Batch 140 loss: 16502.56640625\n",
      "Batch 141 loss: 15502.595703125\n",
      "Batch 142 loss: 30999.693359375\n",
      "Batch 143 loss: 27616.37890625\n",
      "Batch 144 loss: 30235.28125\n",
      "Batch 145 loss: 48054.92578125\n",
      "Batch 146 loss: 36951.40625\n",
      "Batch 147 loss: 22575.833984375\n",
      "Batch 148 loss: 28046.0703125\n",
      "Batch 149 loss: 24963.904296875\n",
      "Batch 150 loss: 19622.04296875\n",
      "Batch 151 loss: 17316.564453125\n",
      "Batch 152 loss: 26475.23046875\n",
      "Batch 153 loss: 26564.72265625\n",
      "Batch 154 loss: 43191.18359375\n",
      "Batch 155 loss: 22528.396484375\n",
      "Batch 156 loss: 38458.8046875\n",
      "Batch 157 loss: 18063.154296875\n",
      "Batch 158 loss: 29296.94921875\n",
      "Batch 159 loss: 35818.51171875\n",
      "Batch 160 loss: 26410.919921875\n",
      "Batch 161 loss: 31450.328125\n",
      "Batch 162 loss: 18921.42578125\n",
      "Batch 163 loss: 35214.234375\n",
      "Batch 164 loss: 25328.408203125\n",
      "Batch 165 loss: 29051.90234375\n",
      "Batch 166 loss: 35667.13671875\n",
      "Batch 167 loss: 38195.6796875\n",
      "Batch 168 loss: 24565.6953125\n",
      "Batch 169 loss: 27542.99609375\n",
      "Batch 170 loss: 38291.9765625\n",
      "Batch 171 loss: 26588.09765625\n",
      "Batch 172 loss: 38448.07421875\n",
      "Batch 173 loss: 26571.712890625\n",
      "Batch 174 loss: 27424.98828125\n",
      "Batch 175 loss: 37261.484375\n",
      "Batch 176 loss: 27113.0859375\n",
      "Batch 177 loss: 32245.6640625\n",
      "Batch 178 loss: 36325.74609375\n",
      "Batch 179 loss: 29645.130859375\n",
      "Batch 180 loss: 33596.3046875\n",
      "Batch 181 loss: 32541.08203125\n",
      "Batch 182 loss: 15144.7099609375\n",
      "Batch 183 loss: 17064.2734375\n",
      "Batch 184 loss: 20495.359375\n",
      "Batch 185 loss: 42193.21484375\n",
      "Batch 186 loss: 27029.31640625\n",
      "Batch 187 loss: 33144.6328125\n",
      "Batch 188 loss: 21505.80859375\n",
      "Batch 189 loss: 19943.83203125\n",
      "Batch 190 loss: 20498.267578125\n",
      "Batch 191 loss: 28578.994140625\n",
      "Batch 192 loss: 36049.8984375\n",
      "Batch 193 loss: 22472.833984375\n",
      "Batch 194 loss: 23948.943359375\n",
      "Batch 195 loss: 29231.78515625\n",
      "Batch 196 loss: 31510.740234375\n",
      "Batch 197 loss: 28473.212890625\n",
      "Batch 198 loss: 37877.53125\n",
      "Batch 199 loss: 34954.3125\n",
      "Batch 200 loss: 36048.26953125\n",
      "Batch 201 loss: 22476.8671875\n",
      "Batch 202 loss: 25979.583984375\n",
      "Batch 203 loss: 32428.59375\n",
      "Batch 204 loss: 39947.609375\n",
      "Batch 205 loss: 45937.53125\n",
      "Batch 206 loss: 22906.814453125\n",
      "Batch 207 loss: 27520.12890625\n",
      "Batch 208 loss: 24690.966796875\n",
      "Batch 209 loss: 26893.951171875\n",
      "Batch 210 loss: 31908.259765625\n",
      "Batch 211 loss: 42501.3203125\n",
      "Batch 212 loss: 39515.80859375\n",
      "Batch 213 loss: 44326.953125\n",
      "Batch 214 loss: 37874.36328125\n",
      "Batch 215 loss: 39014.54296875\n",
      "Batch 216 loss: 35228.140625\n",
      "Batch 217 loss: 21669.14453125\n",
      "Batch 218 loss: 31650.658203125\n",
      "Batch 219 loss: 25641.615234375\n",
      "Batch 220 loss: 30539.259765625\n",
      "Batch 221 loss: 26268.98046875\n",
      "Batch 222 loss: 26161.759765625\n",
      "Batch 223 loss: 28428.46875\n",
      "Batch 224 loss: 32959.52734375\n",
      "Batch 225 loss: 24827.908203125\n",
      "Batch 226 loss: 39448.79296875\n",
      "Batch 227 loss: 25152.125\n",
      "Batch 228 loss: 18751.107421875\n",
      "Batch 229 loss: 32374.421875\n",
      "Batch 230 loss: 38385.44921875\n",
      "Batch 231 loss: 35282.38671875\n",
      "Batch 232 loss: 26086.578125\n",
      "Batch 233 loss: 46012.109375\n",
      "Batch 234 loss: 32008.953125\n",
      "Batch 235 loss: 40026.19921875\n",
      "Batch 236 loss: 31190.509765625\n",
      "Batch 237 loss: 16747.431640625\n",
      "Batch 238 loss: 20365.453125\n",
      "Batch 239 loss: 43646.2421875\n",
      "Batch 240 loss: 23131.296875\n",
      "Batch 241 loss: 32079.681640625\n",
      "Batch 242 loss: 19848.681640625\n",
      "Batch 243 loss: 37974.96484375\n",
      "Batch 244 loss: 22888.44921875\n",
      "Batch 245 loss: 27669.318359375\n",
      "Batch 246 loss: 20191.560546875\n",
      "Batch 247 loss: 37004.20703125\n",
      "Batch 248 loss: 29899.5390625\n",
      "Batch 249 loss: 20388.873046875\n",
      "### Epoch 8\n",
      "Batch 0 loss: 24783.42578125\n",
      "Batch 1 loss: 35770.546875\n",
      "Batch 2 loss: 37465.6328125\n",
      "Batch 3 loss: 31379.44140625\n",
      "Batch 4 loss: 36606.5703125\n",
      "Batch 5 loss: 27897.3046875\n",
      "Batch 6 loss: 14693.8251953125\n",
      "Batch 7 loss: 47883.546875\n",
      "Batch 8 loss: 27746.1796875\n",
      "Batch 9 loss: 32194.501953125\n",
      "Batch 10 loss: 26308.994140625\n",
      "Batch 11 loss: 22249.978515625\n",
      "Batch 12 loss: 36406.86328125\n",
      "Batch 13 loss: 19711.255859375\n",
      "Batch 14 loss: 29746.458984375\n",
      "Batch 15 loss: 49054.921875\n",
      "Batch 16 loss: 22190.16015625\n",
      "Batch 17 loss: 30520.177734375\n",
      "Batch 18 loss: 21885.80078125\n",
      "Batch 19 loss: 39506.49609375\n",
      "Batch 20 loss: 29494.0390625\n",
      "Batch 21 loss: 31128.96484375\n",
      "Batch 22 loss: 30868.03515625\n",
      "Batch 23 loss: 37906.33984375\n",
      "Batch 24 loss: 30369.400390625\n",
      "Batch 25 loss: 21588.208984375\n",
      "Batch 26 loss: 33826.703125\n",
      "Batch 27 loss: 26473.990234375\n",
      "Batch 28 loss: 24234.984375\n",
      "Batch 29 loss: 29005.046875\n",
      "Batch 30 loss: 15468.904296875\n",
      "Batch 31 loss: 32456.376953125\n",
      "Batch 32 loss: 35497.3515625\n",
      "Batch 33 loss: 32557.4375\n",
      "Batch 34 loss: 30903.689453125\n",
      "Batch 35 loss: 16701.65234375\n",
      "Batch 36 loss: 26153.611328125\n",
      "Batch 37 loss: 33973.8515625\n",
      "Batch 38 loss: 22314.0625\n",
      "Batch 39 loss: 42821.6484375\n",
      "Batch 40 loss: 25679.5703125\n",
      "Batch 41 loss: 23528.03515625\n",
      "Batch 42 loss: 35195.2578125\n",
      "Batch 43 loss: 26491.525390625\n",
      "Batch 44 loss: 27376.9609375\n",
      "Batch 45 loss: 29814.5234375\n",
      "Batch 46 loss: 20702.16796875\n",
      "Batch 47 loss: 37012.83203125\n",
      "Batch 48 loss: 26341.259765625\n",
      "Batch 49 loss: 28909.283203125\n",
      "Batch 50 loss: 21951.751953125\n",
      "Batch 51 loss: 41928.546875\n",
      "Batch 52 loss: 33349.25\n",
      "Batch 53 loss: 25302.1640625\n",
      "Batch 54 loss: 31852.267578125\n",
      "Batch 55 loss: 16331.4453125\n",
      "Batch 56 loss: 29757.5859375\n",
      "Batch 57 loss: 34398.19921875\n",
      "Batch 58 loss: 36842.0703125\n",
      "Batch 59 loss: 26475.255859375\n",
      "Batch 60 loss: 26651.822265625\n",
      "Batch 61 loss: 46461.44140625\n",
      "Batch 62 loss: 21934.53515625\n",
      "Batch 63 loss: 13356.5048828125\n",
      "Batch 64 loss: 37771.32421875\n",
      "Batch 65 loss: 42055.53515625\n",
      "Batch 66 loss: 26059.984375\n",
      "Batch 67 loss: 42293.9765625\n",
      "Batch 68 loss: 23289.501953125\n",
      "Batch 69 loss: 29762.98046875\n",
      "Batch 70 loss: 33953.80859375\n",
      "Batch 71 loss: 12522.1484375\n",
      "Batch 72 loss: 33291.21484375\n",
      "Batch 73 loss: 17532.779296875\n",
      "Batch 74 loss: 36494.359375\n",
      "Batch 75 loss: 18601.041015625\n",
      "Batch 76 loss: 32376.80078125\n",
      "Batch 77 loss: 23176.18359375\n",
      "Batch 78 loss: 27584.70703125\n",
      "Batch 79 loss: 23532.134765625\n",
      "Batch 80 loss: 31146.6015625\n",
      "Batch 81 loss: 25582.65625\n",
      "Batch 82 loss: 41900.8125\n",
      "Batch 83 loss: 46179.18359375\n",
      "Batch 84 loss: 25780.833984375\n",
      "Batch 85 loss: 22881.849609375\n",
      "Batch 86 loss: 23683.4296875\n",
      "Batch 87 loss: 19727.29296875\n",
      "Batch 88 loss: 32254.19921875\n",
      "Batch 89 loss: 30977.0703125\n",
      "Batch 90 loss: 23538.80078125\n",
      "Batch 91 loss: 26572.578125\n",
      "Batch 92 loss: 18491.140625\n",
      "Batch 93 loss: 37634.8984375\n",
      "Batch 94 loss: 30262.97265625\n",
      "Batch 95 loss: 25495.3046875\n",
      "Batch 96 loss: 49427.82421875\n",
      "Batch 97 loss: 21597.904296875\n",
      "Batch 98 loss: 37867.4609375\n",
      "Batch 99 loss: 34534.37890625\n",
      "Batch 100 loss: 38933.30859375\n",
      "Batch 101 loss: 15404.033203125\n",
      "Batch 102 loss: 36147.65625\n",
      "Batch 103 loss: 23622.69921875\n",
      "Batch 104 loss: 22493.529296875\n",
      "Batch 105 loss: 31230.125\n",
      "Batch 106 loss: 28668.03515625\n",
      "Batch 107 loss: 29859.6328125\n",
      "Batch 108 loss: 29253.548828125\n",
      "Batch 109 loss: 30028.25\n",
      "Batch 110 loss: 25054.912109375\n",
      "Batch 111 loss: 36899.125\n",
      "Batch 112 loss: 28872.32421875\n",
      "Batch 113 loss: 35859.8125\n",
      "Batch 114 loss: 29602.98828125\n",
      "Batch 115 loss: 27907.541015625\n",
      "Batch 116 loss: 28445.408203125\n",
      "Batch 117 loss: 20206.609375\n",
      "Batch 118 loss: 24134.888671875\n",
      "Batch 119 loss: 14433.537109375\n",
      "Batch 120 loss: 20198.71484375\n",
      "Batch 121 loss: 33849.78515625\n",
      "Batch 122 loss: 22538.6875\n",
      "Batch 123 loss: 30793.9765625\n",
      "Batch 124 loss: 16101.2255859375\n",
      "Batch 125 loss: 29090.25390625\n",
      "Batch 126 loss: 45296.4140625\n",
      "Batch 127 loss: 27445.890625\n",
      "Batch 128 loss: 24628.265625\n",
      "Batch 129 loss: 25659.619140625\n",
      "Batch 130 loss: 32967.69921875\n",
      "Batch 131 loss: 32136.875\n",
      "Batch 132 loss: 33621.375\n",
      "Batch 133 loss: 22884.06640625\n",
      "Batch 134 loss: 37430.2265625\n",
      "Batch 135 loss: 26216.662109375\n",
      "Batch 136 loss: 26841.892578125\n",
      "Batch 137 loss: 17238.365234375\n",
      "Batch 138 loss: 25950.5234375\n",
      "Batch 139 loss: 27527.41796875\n",
      "Batch 140 loss: 40074.67578125\n",
      "Batch 141 loss: 29467.39453125\n",
      "Batch 142 loss: 38856.125\n",
      "Batch 143 loss: 15979.7080078125\n",
      "Batch 144 loss: 37394.734375\n",
      "Batch 145 loss: 34162.15234375\n",
      "Batch 146 loss: 29058.58203125\n",
      "Batch 147 loss: 33768.609375\n",
      "Batch 148 loss: 21756.88671875\n",
      "Batch 149 loss: 35471.9296875\n",
      "Batch 150 loss: 15481.275390625\n",
      "Batch 151 loss: 31140.134765625\n",
      "Batch 152 loss: 36209.87890625\n",
      "Batch 153 loss: 24186.791015625\n",
      "Batch 154 loss: 48479.0234375\n",
      "Batch 155 loss: 22419.509765625\n",
      "Batch 156 loss: 29913.16015625\n",
      "Batch 157 loss: 39953.484375\n",
      "Batch 158 loss: 20271.12109375\n",
      "Batch 159 loss: 33662.48828125\n",
      "Batch 160 loss: 32291.314453125\n",
      "Batch 161 loss: 36525.0625\n",
      "Batch 162 loss: 39713.46484375\n",
      "Batch 163 loss: 26963.4140625\n",
      "Batch 164 loss: 34888.125\n",
      "Batch 165 loss: 42181.32421875\n",
      "Batch 166 loss: 24585.841796875\n",
      "Batch 167 loss: 25488.85546875\n",
      "Batch 168 loss: 19927.736328125\n",
      "Batch 169 loss: 27239.64453125\n",
      "Batch 170 loss: 34716.4375\n",
      "Batch 171 loss: 25616.90234375\n",
      "Batch 172 loss: 27376.1015625\n",
      "Batch 173 loss: 30839.69921875\n",
      "Batch 174 loss: 25204.91015625\n",
      "Batch 175 loss: 40711.36328125\n",
      "Batch 176 loss: 32086.603515625\n",
      "Batch 177 loss: 35001.50390625\n",
      "Batch 178 loss: 25635.6640625\n",
      "Batch 179 loss: 31989.041015625\n",
      "Batch 180 loss: 33855.19140625\n",
      "Batch 181 loss: 22823.69140625\n",
      "Batch 182 loss: 28681.794921875\n",
      "Batch 183 loss: 34216.6953125\n",
      "Batch 184 loss: 41298.96484375\n",
      "Batch 185 loss: 38502.05078125\n",
      "Batch 186 loss: 39759.6875\n",
      "Batch 187 loss: 32887.94921875\n",
      "Batch 188 loss: 15834.001953125\n",
      "Batch 189 loss: 47349.92578125\n",
      "Batch 190 loss: 27320.107421875\n",
      "Batch 191 loss: 24884.0390625\n",
      "Batch 192 loss: 18256.75\n",
      "Batch 193 loss: 24691.91015625\n",
      "Batch 194 loss: 26344.158203125\n",
      "Batch 195 loss: 38952.171875\n",
      "Batch 196 loss: 45818.37890625\n",
      "Batch 197 loss: 28268.33984375\n",
      "Batch 198 loss: 32601.796875\n",
      "Batch 199 loss: 41533.328125\n",
      "Batch 200 loss: 22512.845703125\n",
      "Batch 201 loss: 22532.11328125\n",
      "Batch 202 loss: 23324.625\n",
      "Batch 203 loss: 20067.13671875\n",
      "Batch 204 loss: 37997.375\n",
      "Batch 205 loss: 24861.009765625\n",
      "Batch 206 loss: 25080.087890625\n",
      "Batch 207 loss: 18034.53515625\n",
      "Batch 208 loss: 36187.5234375\n",
      "Batch 209 loss: 38562.74609375\n",
      "Batch 210 loss: 31262.974609375\n",
      "Batch 211 loss: 24530.30078125\n",
      "Batch 212 loss: 28961.3046875\n",
      "Batch 213 loss: 38087.5390625\n",
      "Batch 214 loss: 16043.287109375\n",
      "Batch 215 loss: 27892.625\n",
      "Batch 216 loss: 18970.326171875\n",
      "Batch 217 loss: 29806.501953125\n",
      "Batch 218 loss: 22315.2265625\n",
      "Batch 219 loss: 31770.41015625\n",
      "Batch 220 loss: 34671.64453125\n",
      "Batch 221 loss: 30404.376953125\n",
      "Batch 222 loss: 30210.4609375\n",
      "Batch 223 loss: 25081.92578125\n",
      "Batch 224 loss: 33592.03125\n",
      "Batch 225 loss: 22438.9296875\n",
      "Batch 226 loss: 40179.55078125\n",
      "Batch 227 loss: 24606.90625\n",
      "Batch 228 loss: 15489.3134765625\n",
      "Batch 229 loss: 33496.5546875\n",
      "Batch 230 loss: 16808.3984375\n",
      "Batch 231 loss: 39404.28125\n",
      "Batch 232 loss: 26523.421875\n",
      "Batch 233 loss: 24841.13671875\n",
      "Batch 234 loss: 33249.54296875\n",
      "Batch 235 loss: 26109.537109375\n",
      "Batch 236 loss: 34796.85546875\n",
      "Batch 237 loss: 21672.703125\n",
      "Batch 238 loss: 43379.7265625\n",
      "Batch 239 loss: 21548.716796875\n",
      "Batch 240 loss: 26800.154296875\n",
      "Batch 241 loss: 39639.0078125\n",
      "Batch 242 loss: 35621.5703125\n",
      "Batch 243 loss: 23740.84765625\n",
      "Batch 244 loss: 26585.583984375\n",
      "Batch 245 loss: 40563.84765625\n",
      "Batch 246 loss: 27654.95703125\n",
      "Batch 247 loss: 21508.43359375\n",
      "Batch 248 loss: 24442.841796875\n",
      "Batch 249 loss: 21933.88671875\n",
      "### Epoch 9\n",
      "Batch 0 loss: 25207.751953125\n",
      "Batch 1 loss: 20455.59765625\n",
      "Batch 2 loss: 26177.8125\n",
      "Batch 3 loss: 29546.328125\n",
      "Batch 4 loss: 25114.677734375\n",
      "Batch 5 loss: 25540.517578125\n",
      "Batch 6 loss: 32842.94140625\n",
      "Batch 7 loss: 33088.6953125\n",
      "Batch 8 loss: 27234.087890625\n",
      "Batch 9 loss: 38672.15234375\n",
      "Batch 10 loss: 27012.478515625\n",
      "Batch 11 loss: 28400.1484375\n",
      "Batch 12 loss: 27464.892578125\n",
      "Batch 13 loss: 23809.587890625\n",
      "Batch 14 loss: 23680.03515625\n",
      "Batch 15 loss: 28432.54296875\n",
      "Batch 16 loss: 25311.638671875\n",
      "Batch 17 loss: 34259.90234375\n",
      "Batch 18 loss: 19697.517578125\n",
      "Batch 19 loss: 13923.7451171875\n",
      "Batch 20 loss: 24700.134765625\n",
      "Batch 21 loss: 27040.72265625\n",
      "Batch 22 loss: 31153.419921875\n",
      "Batch 23 loss: 23858.642578125\n",
      "Batch 24 loss: 25349.123046875\n",
      "Batch 25 loss: 25230.859375\n",
      "Batch 26 loss: 32503.4921875\n",
      "Batch 27 loss: 26057.880859375\n",
      "Batch 28 loss: 35832.8125\n",
      "Batch 29 loss: 35711.671875\n",
      "Batch 30 loss: 28713.6484375\n",
      "Batch 31 loss: 30784.203125\n",
      "Batch 32 loss: 44076.89453125\n",
      "Batch 33 loss: 22866.767578125\n",
      "Batch 34 loss: 31659.31640625\n",
      "Batch 35 loss: 30629.4140625\n",
      "Batch 36 loss: 36983.9296875\n",
      "Batch 37 loss: 32937.4375\n",
      "Batch 38 loss: 24960.3671875\n",
      "Batch 39 loss: 30614.6953125\n",
      "Batch 40 loss: 34072.68359375\n",
      "Batch 41 loss: 26237.23828125\n",
      "Batch 42 loss: 29745.728515625\n",
      "Batch 43 loss: 42471.875\n",
      "Batch 44 loss: 40563.609375\n",
      "Batch 45 loss: 43319.59375\n",
      "Batch 46 loss: 27147.8671875\n",
      "Batch 47 loss: 17923.095703125\n",
      "Batch 48 loss: 41865.4453125\n",
      "Batch 49 loss: 38277.765625\n",
      "Batch 50 loss: 26992.29296875\n",
      "Batch 51 loss: 29058.97265625\n",
      "Batch 52 loss: 21867.5\n",
      "Batch 53 loss: 32569.37890625\n",
      "Batch 54 loss: 28284.578125\n",
      "Batch 55 loss: 33311.8984375\n",
      "Batch 56 loss: 18935.2265625\n",
      "Batch 57 loss: 27658.89453125\n",
      "Batch 58 loss: 36726.30859375\n",
      "Batch 59 loss: 24525.05859375\n",
      "Batch 60 loss: 21343.4296875\n",
      "Batch 61 loss: 39646.53515625\n",
      "Batch 62 loss: 18333.646484375\n",
      "Batch 63 loss: 32304.986328125\n",
      "Batch 64 loss: 29952.275390625\n",
      "Batch 65 loss: 25178.3984375\n",
      "Batch 66 loss: 23373.552734375\n",
      "Batch 67 loss: 35033.1015625\n",
      "Batch 68 loss: 26283.03515625\n",
      "Batch 69 loss: 18530.734375\n",
      "Batch 70 loss: 24192.162109375\n",
      "Batch 71 loss: 22458.76171875\n",
      "Batch 72 loss: 21305.1953125\n",
      "Batch 73 loss: 22029.029296875\n",
      "Batch 74 loss: 31851.724609375\n",
      "Batch 75 loss: 30609.62109375\n",
      "Batch 76 loss: 25621.94140625\n",
      "Batch 77 loss: 13036.3017578125\n",
      "Batch 78 loss: 32496.228515625\n",
      "Batch 79 loss: 34389.140625\n",
      "Batch 80 loss: 31433.849609375\n",
      "Batch 81 loss: 33423.6796875\n",
      "Batch 82 loss: 40110.19921875\n",
      "Batch 83 loss: 20281.73828125\n",
      "Batch 84 loss: 25779.904296875\n",
      "Batch 85 loss: 23174.2265625\n",
      "Batch 86 loss: 23142.2890625\n",
      "Batch 87 loss: 41267.62109375\n",
      "Batch 88 loss: 20073.93359375\n",
      "Batch 89 loss: 17027.900390625\n",
      "Batch 90 loss: 24239.873046875\n",
      "Batch 91 loss: 28345.921875\n",
      "Batch 92 loss: 30776.955078125\n",
      "Batch 93 loss: 33161.85546875\n",
      "Batch 94 loss: 25723.080078125\n",
      "Batch 95 loss: 50196.8125\n",
      "Batch 96 loss: 30806.6171875\n",
      "Batch 97 loss: 36573.96875\n",
      "Batch 98 loss: 32079.0703125\n",
      "Batch 99 loss: 29841.224609375\n",
      "Batch 100 loss: 26176.439453125\n",
      "Batch 101 loss: 31447.681640625\n",
      "Batch 102 loss: 31757.376953125\n",
      "Batch 103 loss: 23193.888671875\n",
      "Batch 104 loss: 26101.541015625\n",
      "Batch 105 loss: 45155.6875\n",
      "Batch 106 loss: 40135.5\n",
      "Batch 107 loss: 31394.626953125\n",
      "Batch 108 loss: 27775.72265625\n",
      "Batch 109 loss: 32218.3671875\n",
      "Batch 110 loss: 16662.09375\n",
      "Batch 111 loss: 30835.8828125\n",
      "Batch 112 loss: 31661.755859375\n",
      "Batch 113 loss: 32849.51953125\n",
      "Batch 114 loss: 20113.048828125\n",
      "Batch 115 loss: 27231.728515625\n",
      "Batch 116 loss: 38599.1328125\n",
      "Batch 117 loss: 30072.294921875\n",
      "Batch 118 loss: 44340.0859375\n",
      "Batch 119 loss: 23595.900390625\n",
      "Batch 120 loss: 28340.69140625\n",
      "Batch 121 loss: 28320.849609375\n",
      "Batch 122 loss: 30313.36328125\n",
      "Batch 123 loss: 46500.33203125\n",
      "Batch 124 loss: 25083.76171875\n",
      "Batch 125 loss: 23287.23046875\n",
      "Batch 126 loss: 28147.59765625\n",
      "Batch 127 loss: 20555.1953125\n",
      "Batch 128 loss: 32826.95703125\n",
      "Batch 129 loss: 27931.828125\n",
      "Batch 130 loss: 23747.60546875\n",
      "Batch 131 loss: 41809.4296875\n",
      "Batch 132 loss: 28031.41796875\n",
      "Batch 133 loss: 29217.28515625\n",
      "Batch 134 loss: 22268.373046875\n",
      "Batch 135 loss: 30676.37890625\n",
      "Batch 136 loss: 42241.125\n",
      "Batch 137 loss: 21502.212890625\n",
      "Batch 138 loss: 30142.314453125\n",
      "Batch 139 loss: 33948.34375\n",
      "Batch 140 loss: 39117.45703125\n",
      "Batch 141 loss: 24492.97265625\n",
      "Batch 142 loss: 31326.50390625\n",
      "Batch 143 loss: 21043.953125\n",
      "Batch 144 loss: 40073.22265625\n",
      "Batch 145 loss: 14992.251953125\n",
      "Batch 146 loss: 47005.8046875\n",
      "Batch 147 loss: 25605.677734375\n",
      "Batch 148 loss: 39557.96875\n",
      "Batch 149 loss: 23400.40234375\n",
      "Batch 150 loss: 20679.96484375\n",
      "Batch 151 loss: 38201.5390625\n",
      "Batch 152 loss: 37509.515625\n",
      "Batch 153 loss: 19806.748046875\n",
      "Batch 154 loss: 31754.580078125\n",
      "Batch 155 loss: 30916.935546875\n",
      "Batch 156 loss: 23894.169921875\n",
      "Batch 157 loss: 41620.12890625\n",
      "Batch 158 loss: 23747.14453125\n",
      "Batch 159 loss: 29426.138671875\n",
      "Batch 160 loss: 21494.765625\n",
      "Batch 161 loss: 31555.693359375\n",
      "Batch 162 loss: 18199.798828125\n",
      "Batch 163 loss: 31802.111328125\n",
      "Batch 164 loss: 36142.1328125\n",
      "Batch 165 loss: 27691.77734375\n",
      "Batch 166 loss: 34076.91015625\n",
      "Batch 167 loss: 18758.697265625\n",
      "Batch 168 loss: 30535.41015625\n",
      "Batch 169 loss: 33099.8046875\n",
      "Batch 170 loss: 19899.650390625\n",
      "Batch 171 loss: 26996.2890625\n",
      "Batch 172 loss: 36548.640625\n",
      "Batch 173 loss: 29738.20703125\n",
      "Batch 174 loss: 29626.525390625\n",
      "Batch 175 loss: 31510.71875\n",
      "Batch 176 loss: 29366.150390625\n",
      "Batch 177 loss: 42164.6875\n",
      "Batch 178 loss: 27899.5078125\n",
      "Batch 179 loss: 25302.18359375\n",
      "Batch 180 loss: 22141.15234375\n",
      "Batch 181 loss: 30495.70703125\n",
      "Batch 182 loss: 22244.91015625\n",
      "Batch 183 loss: 27721.5546875\n",
      "Batch 184 loss: 41181.2734375\n",
      "Batch 185 loss: 42519.4921875\n",
      "Batch 186 loss: 36469.640625\n",
      "Batch 187 loss: 21082.345703125\n",
      "Batch 188 loss: 36329.89453125\n",
      "Batch 189 loss: 23204.640625\n",
      "Batch 190 loss: 36517.4765625\n",
      "Batch 191 loss: 36897.05859375\n",
      "Batch 192 loss: 31169.8671875\n",
      "Batch 193 loss: 30974.171875\n",
      "Batch 194 loss: 16862.328125\n",
      "Batch 195 loss: 37193.375\n",
      "Batch 196 loss: 20236.794921875\n",
      "Batch 197 loss: 36975.4765625\n",
      "Batch 198 loss: 15550.5712890625\n",
      "Batch 199 loss: 33478.3125\n",
      "Batch 200 loss: 37932.32421875\n",
      "Batch 201 loss: 27328.45703125\n",
      "Batch 202 loss: 34197.13671875\n",
      "Batch 203 loss: 29277.845703125\n",
      "Batch 204 loss: 20643.740234375\n",
      "Batch 205 loss: 24411.12109375\n",
      "Batch 206 loss: 35262.09765625\n",
      "Batch 207 loss: 33340.40234375\n",
      "Batch 208 loss: 33795.7890625\n",
      "Batch 209 loss: 23711.94921875\n",
      "Batch 210 loss: 24629.921875\n",
      "Batch 211 loss: 32595.5703125\n",
      "Batch 212 loss: 28161.71875\n",
      "Batch 213 loss: 28274.6328125\n",
      "Batch 214 loss: 33401.20703125\n",
      "Batch 215 loss: 24095.923828125\n",
      "Batch 216 loss: 30703.6328125\n",
      "Batch 217 loss: 15097.87109375\n",
      "Batch 218 loss: 31622.43359375\n",
      "Batch 219 loss: 27424.90625\n",
      "Batch 220 loss: 37037.16015625\n",
      "Batch 221 loss: 36820.53125\n",
      "Batch 222 loss: 41184.89453125\n",
      "Batch 223 loss: 17757.3203125\n",
      "Batch 224 loss: 34566.8203125\n",
      "Batch 225 loss: 23900.0703125\n",
      "Batch 226 loss: 16135.3837890625\n",
      "Batch 227 loss: 30335.09375\n",
      "Batch 228 loss: 19648.578125\n",
      "Batch 229 loss: 30593.91015625\n",
      "Batch 230 loss: 24585.654296875\n",
      "Batch 231 loss: 38045.21875\n",
      "Batch 232 loss: 20506.80078125\n",
      "Batch 233 loss: 25314.107421875\n",
      "Batch 234 loss: 31124.177734375\n",
      "Batch 235 loss: 29737.884765625\n",
      "Batch 236 loss: 42438.12109375\n",
      "Batch 237 loss: 32517.3515625\n",
      "Batch 238 loss: 40681.2109375\n",
      "Batch 239 loss: 24794.490234375\n",
      "Batch 240 loss: 25976.93359375\n",
      "Batch 241 loss: 34918.8046875\n",
      "Batch 242 loss: 24709.3125\n",
      "Batch 243 loss: 27377.802734375\n",
      "Batch 244 loss: 31557.64453125\n",
      "Batch 245 loss: 31014.3671875\n",
      "Batch 246 loss: 36378.734375\n",
      "Batch 247 loss: 27852.55859375\n",
      "Batch 248 loss: 29441.064453125\n",
      "Batch 249 loss: 33351.92578125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "294113.74183984374"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(model, train_dl, optimizer, criterion, epochs=10):\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"### Epoch {epoch}\")\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.float())\n",
    "            loss = criterion(output, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Batch {i} loss: {loss.item()}\")\n",
    "            \n",
    "    return running_loss / len(train_dl)\n",
    "                            \n",
    "\n",
    "train(model, train_dl, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_test, output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input_data'"
     ]
    }
   ],
   "source": [
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, output.detach().numpy())\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "class GroupLayer(nn.Module):\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\n",
    "                \"Total feature size must be divisible by group feature size\")\n",
    "\n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0\n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "\n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size).double()\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1).double()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(input_data.shape)\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "\n",
    "        outs = []\n",
    "        for i, x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    TorchModel,\n",
    "    module__group_feat_size=group_feat_size,\n",
    "    module__total_feat_size=total_feat_size,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=2000, n_features=260, noise=0.01)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: group_feat_size, total_feat_size.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m25136.4242\u001b[0m    \u001b[32m28184.4529\u001b[0m  0.2992\n",
      "      2    \u001b[36m25131.6008\u001b[0m    \u001b[32m28180.9656\u001b[0m  0.4549\n",
      "      3    \u001b[36m25123.8036\u001b[0m    \u001b[32m28172.6188\u001b[0m  0.2994\n",
      "      4    \u001b[36m25107.4279\u001b[0m    \u001b[32m28155.9001\u001b[0m  0.3518\n",
      "      5    \u001b[36m25076.7573\u001b[0m    \u001b[32m28121.9955\u001b[0m  0.3075\n",
      "      6    \u001b[36m25019.4746\u001b[0m    \u001b[32m28054.9637\u001b[0m  0.3289\n",
      "      7    \u001b[36m24920.8820\u001b[0m    \u001b[32m27951.3269\u001b[0m  0.3097\n",
      "      8    \u001b[36m24765.2670\u001b[0m    \u001b[32m27787.6675\u001b[0m  0.3423\n",
      "      9    \u001b[36m24534.5411\u001b[0m    \u001b[32m27549.5329\u001b[0m  0.4599\n",
      "     10    \u001b[36m24206.8254\u001b[0m    \u001b[32m27207.2966\u001b[0m  0.3357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=TorchModel(\n",
       "    (group_layers): ModuleList(\n",
       "      (0-25): 26 x GroupLayer(\n",
       "        (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
       "    (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make prediction \n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "\n",
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n",
    "# Bring in CCLE data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in original ccle data\n",
    "loading_code = 'generic-gdsc-1-FGFR_0939-LN_IC50-fgfr4_ccle_dynamic_features-true-Row'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "feature_data_numpy = feature_data.to_numpy()\n",
    "label_data_numpy = label_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 260)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data_numpy.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
