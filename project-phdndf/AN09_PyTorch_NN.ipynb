{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(torch.__version__)\n",
    "    # Setup device agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0923,  0.0333, -0.0900,  ..., -0.0675, -0.0047,  0.0873],\n",
      "        [-0.0146, -0.0310,  0.0644,  ...,  0.0143, -0.0487, -0.0433],\n",
      "        [-0.0833,  0.0484, -0.0755,  ...,  0.0413,  0.0275, -0.0353],\n",
      "        ...,\n",
      "        [-0.0843, -0.0177,  0.0054,  ..., -0.0752, -0.0066, -0.0202],\n",
      "        [ 0.0113,  0.0908, -0.0784,  ...,  0.0843,  0.0939, -0.0249],\n",
      "        [-0.0149,  0.0009, -0.0201,  ...,  0.0959,  0.0997, -0.0040]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0065,  0.0285, -0.0186, -0.0276,  0.0414, -0.0571,  0.0835, -0.0327,\n",
      "        -0.0767,  0.0320,  0.0786,  0.0022, -0.0944,  0.0383, -0.0579,  0.0107,\n",
      "         0.0165,  0.0876, -0.0749, -0.0954,  0.0553,  0.0333,  0.0116, -0.0388,\n",
      "        -0.0466,  0.0496, -0.0604, -0.0510, -0.0307,  0.0805,  0.0273, -0.0366,\n",
      "         0.0488, -0.0048, -0.0095,  0.0846,  0.0077,  0.0451, -0.0942, -0.0123,\n",
      "         0.0894, -0.0108,  0.0421,  0.0059, -0.0109, -0.0938,  0.0227,  0.0959,\n",
      "         0.0439, -0.0074, -0.0113,  0.0886, -0.0099, -0.0736, -0.0067,  0.0333,\n",
      "         0.0474, -0.0674, -0.0005, -0.0380,  0.0221,  0.0944,  0.0061,  0.0129,\n",
      "         0.0282,  0.0736,  0.0483,  0.0361, -0.0163,  0.0705, -0.0882, -0.0803,\n",
      "        -0.0367,  0.0692, -0.0664,  0.0128, -0.0488, -0.0021, -0.0514,  0.0896,\n",
      "         0.0664,  0.0851, -0.0942,  0.0957, -0.0391, -0.0415,  0.0475, -0.0413,\n",
      "        -0.0944,  0.0641, -0.0029, -0.0021, -0.0186,  0.0124, -0.0679,  0.0741,\n",
      "         0.0488, -0.0468,  0.0001, -0.0744, -0.0209,  0.0491,  0.0781, -0.0394,\n",
      "        -0.0197,  0.0356,  0.0376,  0.0202, -0.0240,  0.0849, -0.0258, -0.0890,\n",
      "        -0.0531, -0.0605,  0.0754, -0.0144,  0.0261, -0.0032, -0.0857, -0.0694,\n",
      "        -0.0583,  0.0877,  0.0875, -0.0691, -0.0448,  0.0343,  0.0807, -0.0583,\n",
      "        -0.0997, -0.0725, -0.0586, -0.0738,  0.0892, -0.0246,  0.0044, -0.0166,\n",
      "        -0.0735,  0.0649,  0.0691,  0.0693, -0.1000, -0.0093, -0.0851,  0.0157,\n",
      "        -0.0436, -0.0134,  0.0611, -0.0156, -0.0511,  0.0265,  0.0383, -0.0047,\n",
      "        -0.0412, -0.0235, -0.0791, -0.0331,  0.0397,  0.0282, -0.0495, -0.0807,\n",
      "        -0.0372,  0.0725,  0.0662, -0.0355, -0.0201,  0.0939,  0.0548, -0.0739,\n",
      "        -0.0918, -0.0443, -0.0388,  0.0834,  0.0396,  0.0450, -0.0421,  0.0298,\n",
      "         0.0917, -0.0616, -0.0215, -0.0132,  0.0903,  0.0391,  0.0978, -0.0673,\n",
      "        -0.0140,  0.0991, -0.0108,  0.0978,  0.0270, -0.0219, -0.0621,  0.0410,\n",
      "        -0.0458,  0.0098,  0.0103,  0.0590, -0.0866,  0.0098,  0.0970,  0.0503],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0619, -0.0488,  0.0156,  ..., -0.0425, -0.0406, -0.0373],\n",
      "        [ 0.0458, -0.0581,  0.0096,  ..., -0.0682,  0.0608,  0.0660],\n",
      "        [ 0.0013, -0.0269,  0.0050,  ..., -0.0194,  0.0006, -0.0248],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0609, -0.0061,  ..., -0.0688, -0.0017, -0.0290],\n",
      "        [-0.0050, -0.0236, -0.0066,  ...,  0.0423,  0.0455,  0.0581],\n",
      "        [-0.0412,  0.0126, -0.0221,  ..., -0.0625, -0.0021, -0.0375]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0449, -0.0521,  0.0150,  0.0083,  0.0345,  0.0433,  0.0438, -0.0519,\n",
      "         0.0149,  0.0668], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0619, -0.0488,  0.0156,  ..., -0.0425, -0.0406, -0.0373],\n",
      "        [ 0.0458, -0.0581,  0.0096,  ..., -0.0682,  0.0608,  0.0660],\n",
      "        [ 0.0013, -0.0269,  0.0050,  ..., -0.0194,  0.0006, -0.0248],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0609, -0.0061,  ..., -0.0688, -0.0017, -0.0290],\n",
      "        [-0.0050, -0.0236, -0.0066,  ...,  0.0423,  0.0455,  0.0581],\n",
      "        [-0.0412,  0.0126, -0.0221,  ..., -0.0625, -0.0021, -0.0375]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0449, -0.0521,  0.0150,  0.0083,  0.0345,  0.0433,  0.0438, -0.0519,\n",
      "         0.0149,  0.0668], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "\n",
    "\n",
    "class MySmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySmallModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.network1 = MySmallModel()\n",
    "        self.network2 = MySmallModel()\n",
    "        self.network3 = MySmallModel()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc_out = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = F.relu(self.network1(x1))\n",
    "        x2 = F.relu(self.network2(x2))\n",
    "        x3 = F.relu(self.network3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "N = 10\n",
    "x1, x2, x3 = torch.randn(N, 5), torch.randn(N, 5), torch.randn(N, 5)\n",
    "\n",
    "output = model(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterisation of Feature Size and Group Feature Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1208],\n",
      "        [0.1936],\n",
      "        [0.1864],\n",
      "        [0.2696],\n",
      "        [0.0661],\n",
      "        [0.2202],\n",
      "        [0.1334],\n",
      "        [0.1326],\n",
      "        [0.2051],\n",
      "        [0.1475]], grad_fn=<AddmmBackward0>)\n",
      "TorchModel(\n",
      "  (group_layers): ModuleList(\n",
      "    (0-25): 26 x GroupLayer(\n",
      "      (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
      "  (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GroupLayer(nn.Module):\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x \n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\"Total feature size must be divisible by group feature size\")\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0 \n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "            \n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size)\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(input_data.shape)\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "        \n",
    "        outs = []\n",
    "        for i,x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n",
    "N = 10\n",
    "x = torch.randn(N, total_feat_size)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimisation Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to generate some regression data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=10000, n_features=260, noise=0.1)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_dl = DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = TorchModel(group_feat_size, total_feat_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch 0\n",
      "Batch 0 loss: 43265.95334051589\n",
      "Batch 1 loss: 43769.70668765815\n",
      "Batch 2 loss: 51063.13610698151\n",
      "Batch 3 loss: 31887.078673868826\n",
      "Batch 4 loss: 27212.87292946202\n",
      "Batch 5 loss: 21852.101070121345\n",
      "Batch 6 loss: 39404.01740497344\n",
      "Batch 7 loss: 33454.19728695883\n",
      "Batch 8 loss: 29582.177416267456\n",
      "Batch 9 loss: 53554.742290062364\n",
      "Batch 10 loss: 27230.351765344778\n",
      "Batch 11 loss: 42616.03873473134\n",
      "Batch 12 loss: 23140.837882220338\n",
      "Batch 13 loss: 27290.00810295282\n",
      "Batch 14 loss: 46852.48649854868\n",
      "Batch 15 loss: 33112.50350914469\n",
      "Batch 16 loss: 28583.38737521213\n",
      "Batch 17 loss: 32850.4925280198\n",
      "Batch 18 loss: 36527.66924541115\n",
      "Batch 19 loss: 33230.5215310286\n",
      "Batch 20 loss: 41223.033694636484\n",
      "Batch 21 loss: 54027.38169258549\n",
      "Batch 22 loss: 38549.625888154056\n",
      "Batch 23 loss: 38897.8306342196\n",
      "Batch 24 loss: 36914.090484084416\n",
      "Batch 25 loss: 40052.6211180996\n",
      "Batch 26 loss: 39344.606266426286\n",
      "Batch 27 loss: 38337.501599303745\n",
      "Batch 28 loss: 38634.78469499729\n",
      "Batch 29 loss: 29934.59573211414\n",
      "Batch 30 loss: 39885.0338233439\n",
      "Batch 31 loss: 45298.00827684257\n",
      "Batch 32 loss: 35171.054821334445\n",
      "Batch 33 loss: 31442.218630653853\n",
      "Batch 34 loss: 39412.98205521723\n",
      "Batch 35 loss: 57678.829858123936\n",
      "Batch 36 loss: 35129.247025937875\n",
      "Batch 37 loss: 47776.775613572376\n",
      "Batch 38 loss: 24931.872710256546\n",
      "Batch 39 loss: 29396.398007797216\n",
      "Batch 40 loss: 38928.72246141473\n",
      "Batch 41 loss: 43201.35124883234\n",
      "Batch 42 loss: 41656.99224835873\n",
      "Batch 43 loss: 25831.197747938422\n",
      "Batch 44 loss: 43622.783752114425\n",
      "Batch 45 loss: 50076.88904445262\n",
      "Batch 46 loss: 31379.696091308644\n",
      "Batch 47 loss: 34403.2647007078\n",
      "Batch 48 loss: 36398.445483170086\n",
      "Batch 49 loss: 31807.975055097904\n",
      "Batch 50 loss: 51994.66232120457\n",
      "Batch 51 loss: 43709.341997522555\n",
      "Batch 52 loss: 62437.46319752621\n",
      "Batch 53 loss: 47270.72785604869\n",
      "Batch 54 loss: 29183.109206116897\n",
      "Batch 55 loss: 31824.659665735962\n",
      "Batch 56 loss: 46736.58178295912\n",
      "Batch 57 loss: 27096.406769964145\n",
      "Batch 58 loss: 46424.821816530166\n",
      "Batch 59 loss: 56438.557746984094\n",
      "Batch 60 loss: 45111.19968301404\n",
      "Batch 61 loss: 29363.336846582573\n",
      "Batch 62 loss: 37052.44720996295\n",
      "Batch 63 loss: 35966.516794502764\n",
      "Batch 64 loss: 32852.95028087151\n",
      "Batch 65 loss: 41057.27516142832\n",
      "Batch 66 loss: 42583.440201383266\n",
      "Batch 67 loss: 26565.815690062507\n",
      "Batch 68 loss: 55100.860290699165\n",
      "Batch 69 loss: 36882.32085267212\n",
      "Batch 70 loss: 37797.77276649678\n",
      "Batch 71 loss: 34183.97813042096\n",
      "Batch 72 loss: 37956.623837573745\n",
      "Batch 73 loss: 34487.18751910577\n",
      "Batch 74 loss: 29029.53038000498\n",
      "Batch 75 loss: 55718.176728212406\n",
      "Batch 76 loss: 41971.514830058244\n",
      "Batch 77 loss: 35578.7009802774\n",
      "Batch 78 loss: 26459.97510174693\n",
      "Batch 79 loss: 29692.23131578833\n",
      "Batch 80 loss: 47281.114062434404\n",
      "Batch 81 loss: 28704.529937097872\n",
      "Batch 82 loss: 34031.36028557717\n",
      "Batch 83 loss: 26692.875954337323\n",
      "Batch 84 loss: 34070.103905107\n",
      "Batch 85 loss: 33930.36652475186\n",
      "Batch 86 loss: 29828.4417494827\n",
      "Batch 87 loss: 26644.54997634876\n",
      "Batch 88 loss: 32895.46396116952\n",
      "Batch 89 loss: 52776.157410331034\n",
      "Batch 90 loss: 35169.02636024072\n",
      "Batch 91 loss: 24124.992131030813\n",
      "Batch 92 loss: 46173.36533008095\n",
      "Batch 93 loss: 40771.30047526735\n",
      "Batch 94 loss: 44272.2422744965\n",
      "Batch 95 loss: 49639.50428632525\n",
      "Batch 96 loss: 25575.3195803952\n",
      "Batch 97 loss: 39348.22311766523\n",
      "Batch 98 loss: 74414.06041860812\n",
      "Batch 99 loss: 28508.52631301182\n",
      "Batch 100 loss: 33032.09863677718\n",
      "Batch 101 loss: 36241.148217469316\n",
      "Batch 102 loss: 58940.81559163963\n",
      "Batch 103 loss: 43694.52410496971\n",
      "Batch 104 loss: 37456.40521668897\n",
      "Batch 105 loss: 37115.18818220115\n",
      "Batch 106 loss: 33304.08401708574\n",
      "Batch 107 loss: 33386.92000368402\n",
      "Batch 108 loss: 42545.92763178791\n",
      "Batch 109 loss: 36796.46032652473\n",
      "Batch 110 loss: 43404.265669534216\n",
      "Batch 111 loss: 39325.179335487024\n",
      "Batch 112 loss: 34151.59254755804\n",
      "Batch 113 loss: 33073.45540940756\n",
      "Batch 114 loss: 30283.136930890647\n",
      "Batch 115 loss: 24316.80186915477\n",
      "Batch 116 loss: 40370.154645853545\n",
      "Batch 117 loss: 31973.23013964436\n",
      "Batch 118 loss: 42791.574930099996\n",
      "Batch 119 loss: 48793.73181191441\n",
      "Batch 120 loss: 37164.23045154997\n",
      "Batch 121 loss: 31549.962973120324\n",
      "Batch 122 loss: 33829.82928781566\n",
      "Batch 123 loss: 61332.70925977604\n",
      "Batch 124 loss: 31061.441452813917\n",
      "Batch 125 loss: 44352.9824926318\n",
      "Batch 126 loss: 37046.31176694785\n",
      "Batch 127 loss: 31500.97543651513\n",
      "Batch 128 loss: 34764.26740960523\n",
      "Batch 129 loss: 42074.219793059\n",
      "Batch 130 loss: 27746.248859592248\n",
      "Batch 131 loss: 25933.30169269506\n",
      "Batch 132 loss: 40581.08912278386\n",
      "Batch 133 loss: 41984.12713874027\n",
      "Batch 134 loss: 43766.63930391154\n",
      "Batch 135 loss: 40072.00633472928\n",
      "Batch 136 loss: 31756.4247979114\n",
      "Batch 137 loss: 29436.542589581084\n",
      "Batch 138 loss: 26587.012191431786\n",
      "Batch 139 loss: 49486.240989718404\n",
      "Batch 140 loss: 28649.292783273337\n",
      "Batch 141 loss: 43493.92132274434\n",
      "Batch 142 loss: 36633.791127162614\n",
      "Batch 143 loss: 28171.98307714937\n",
      "Batch 144 loss: 45785.61202807227\n",
      "Batch 145 loss: 34192.60042652812\n",
      "Batch 146 loss: 19516.230504032163\n",
      "Batch 147 loss: 35581.23603476213\n",
      "Batch 148 loss: 35846.466256962885\n",
      "Batch 149 loss: 39931.86926038006\n",
      "Batch 150 loss: 37921.39197200841\n",
      "Batch 151 loss: 30176.46324982569\n",
      "Batch 152 loss: 37833.77857673979\n",
      "Batch 153 loss: 33947.4005386833\n",
      "Batch 154 loss: 26196.58493788709\n",
      "Batch 155 loss: 32317.772286378546\n",
      "Batch 156 loss: 28827.533009232124\n",
      "Batch 157 loss: 41943.91938139314\n",
      "Batch 158 loss: 29551.748153741748\n",
      "Batch 159 loss: 38341.544312772894\n",
      "Batch 160 loss: 62262.826766330996\n",
      "Batch 161 loss: 44335.80364616448\n",
      "Batch 162 loss: 29248.253540156154\n",
      "Batch 163 loss: 39619.271060299725\n",
      "Batch 164 loss: 26561.462767206318\n",
      "Batch 165 loss: 52187.68648184987\n",
      "Batch 166 loss: 30303.271530030455\n",
      "Batch 167 loss: 15531.009781003651\n",
      "Batch 168 loss: 50333.560209125135\n",
      "Batch 169 loss: 40460.41853426077\n",
      "Batch 170 loss: 42264.9260947537\n",
      "Batch 171 loss: 21606.742264855355\n",
      "Batch 172 loss: 33903.63259368996\n",
      "Batch 173 loss: 31005.59953145114\n",
      "Batch 174 loss: 43939.21586699053\n",
      "Batch 175 loss: 41034.13538486922\n",
      "Batch 176 loss: 36822.01475833957\n",
      "Batch 177 loss: 43936.94869009643\n",
      "Batch 178 loss: 31030.199589782096\n",
      "Batch 179 loss: 38387.15369807885\n",
      "Batch 180 loss: 35186.76550694827\n",
      "Batch 181 loss: 26630.616640547967\n",
      "Batch 182 loss: 55860.32742955999\n",
      "Batch 183 loss: 73036.83906281169\n",
      "Batch 184 loss: 37941.896991764326\n",
      "Batch 185 loss: 29366.05900424961\n",
      "Batch 186 loss: 31647.189172096667\n",
      "Batch 187 loss: 46755.70115046804\n",
      "Batch 188 loss: 55454.70136470009\n",
      "Batch 189 loss: 51483.20336025582\n",
      "Batch 190 loss: 44012.32055135595\n",
      "Batch 191 loss: 44726.25045271854\n",
      "Batch 192 loss: 33617.15250435774\n",
      "Batch 193 loss: 47683.3343412966\n",
      "Batch 194 loss: 40519.61784542926\n",
      "Batch 195 loss: 39891.694190889844\n",
      "Batch 196 loss: 42583.19720232839\n",
      "Batch 197 loss: 40642.6452583033\n",
      "Batch 198 loss: 33275.38095765815\n",
      "Batch 199 loss: 40490.87665104923\n",
      "Batch 200 loss: 47016.76269157571\n",
      "Batch 201 loss: 60405.182789140024\n",
      "Batch 202 loss: 52327.81930501149\n",
      "Batch 203 loss: 59944.32147510263\n",
      "Batch 204 loss: 31524.72653293086\n",
      "Batch 205 loss: 55221.93079703543\n",
      "Batch 206 loss: 24491.6272242496\n",
      "Batch 207 loss: 40964.87002502222\n",
      "Batch 208 loss: 35352.51390275739\n",
      "Batch 209 loss: 39845.94956731022\n",
      "Batch 210 loss: 32841.20017528348\n",
      "Batch 211 loss: 40818.70875606888\n",
      "Batch 212 loss: 28359.30578028609\n",
      "Batch 213 loss: 36727.67886529748\n",
      "Batch 214 loss: 28414.530206426654\n",
      "Batch 215 loss: 40376.9069007059\n",
      "Batch 216 loss: 23900.116829441467\n",
      "Batch 217 loss: 24932.740140852333\n",
      "Batch 218 loss: 37116.21613860405\n",
      "Batch 219 loss: 34858.82832884779\n",
      "Batch 220 loss: 52838.09713314922\n",
      "Batch 221 loss: 22324.640007117807\n",
      "Batch 222 loss: 42505.00798347805\n",
      "Batch 223 loss: 30103.6546886511\n",
      "Batch 224 loss: 41677.752738225376\n",
      "Batch 225 loss: 72730.04594239315\n",
      "Batch 226 loss: 27315.63287789697\n",
      "Batch 227 loss: 27282.166664120505\n",
      "Batch 228 loss: 37967.6964131363\n",
      "Batch 229 loss: 28028.038061858817\n",
      "Batch 230 loss: 29254.502609307034\n",
      "Batch 231 loss: 22949.446899127463\n",
      "Batch 232 loss: 41466.979320034654\n",
      "Batch 233 loss: 29392.734594611233\n",
      "Batch 234 loss: 43518.65376493037\n",
      "Batch 235 loss: 50731.79172374438\n",
      "Batch 236 loss: 45066.10853202225\n",
      "Batch 237 loss: 29143.396311641856\n",
      "Batch 238 loss: 44841.316130115934\n",
      "Batch 239 loss: 26438.57961645128\n",
      "Batch 240 loss: 42677.97906975623\n",
      "Batch 241 loss: 45908.71402552505\n",
      "Batch 242 loss: 38418.58192340621\n",
      "Batch 243 loss: 36756.75690177371\n",
      "Batch 244 loss: 38982.24504433912\n",
      "Batch 245 loss: 37156.163243475734\n",
      "Batch 246 loss: 40587.6695494022\n",
      "Batch 247 loss: 47534.37326325372\n",
      "Batch 248 loss: 37434.00557805463\n",
      "Batch 249 loss: 30186.75512987439\n",
      "### Epoch 1\n",
      "Batch 0 loss: 49784.81093462845\n",
      "Batch 1 loss: 39675.064380279946\n",
      "Batch 2 loss: 35476.3733171723\n",
      "Batch 3 loss: 38291.543757824424\n",
      "Batch 4 loss: 55342.792699630816\n",
      "Batch 5 loss: 32575.77679090795\n",
      "Batch 6 loss: 28181.081952703687\n",
      "Batch 7 loss: 53713.98855474875\n",
      "Batch 8 loss: 40273.8185695606\n",
      "Batch 9 loss: 27953.972911807406\n",
      "Batch 10 loss: 22212.759806124366\n",
      "Batch 11 loss: 37421.986282114376\n",
      "Batch 12 loss: 43617.59843511848\n",
      "Batch 13 loss: 28328.920004393294\n",
      "Batch 14 loss: 46231.69644457777\n",
      "Batch 15 loss: 28093.56681833848\n",
      "Batch 16 loss: 38593.87557929492\n",
      "Batch 17 loss: 31234.918148211254\n",
      "Batch 18 loss: 28023.22743390176\n",
      "Batch 19 loss: 39793.68996316647\n",
      "Batch 20 loss: 38847.12435515797\n",
      "Batch 21 loss: 43238.79868690063\n",
      "Batch 22 loss: 32811.66894066767\n",
      "Batch 23 loss: 23769.7638385625\n",
      "Batch 24 loss: 26657.90686134337\n",
      "Batch 25 loss: 41182.13657085577\n",
      "Batch 26 loss: 26153.048817890878\n",
      "Batch 27 loss: 33728.360275466985\n",
      "Batch 28 loss: 38020.81226860249\n",
      "Batch 29 loss: 40829.463030265986\n",
      "Batch 30 loss: 29133.136807461236\n",
      "Batch 31 loss: 35008.095684011234\n",
      "Batch 32 loss: 33456.57222929539\n",
      "Batch 33 loss: 28287.73473644045\n",
      "Batch 34 loss: 42580.72304061652\n",
      "Batch 35 loss: 25618.560126839835\n",
      "Batch 36 loss: 16700.7960076147\n",
      "Batch 37 loss: 31376.923475071773\n",
      "Batch 38 loss: 36785.78754624433\n",
      "Batch 39 loss: 40129.50676083333\n",
      "Batch 40 loss: 39256.262108064126\n",
      "Batch 41 loss: 50246.80045753012\n",
      "Batch 42 loss: 35891.04030816246\n",
      "Batch 43 loss: 39255.889507910964\n",
      "Batch 44 loss: 35659.295856968136\n",
      "Batch 45 loss: 39127.530204993396\n",
      "Batch 46 loss: 56140.96404157328\n",
      "Batch 47 loss: 33835.21462505702\n",
      "Batch 48 loss: 47682.70719354123\n",
      "Batch 49 loss: 49286.071154221296\n",
      "Batch 50 loss: 51130.754655586534\n",
      "Batch 51 loss: 50065.92946997187\n",
      "Batch 52 loss: 42093.4768036555\n",
      "Batch 53 loss: 34747.030241712804\n",
      "Batch 54 loss: 46111.64352508557\n",
      "Batch 55 loss: 43906.90514370574\n",
      "Batch 56 loss: 41208.389166756046\n",
      "Batch 57 loss: 23238.10207255218\n",
      "Batch 58 loss: 27336.579458727676\n",
      "Batch 59 loss: 35188.25878977444\n",
      "Batch 60 loss: 33584.63146027396\n",
      "Batch 61 loss: 33048.16742982653\n",
      "Batch 62 loss: 39500.22336711702\n",
      "Batch 63 loss: 38648.74845912719\n",
      "Batch 64 loss: 34918.02975745848\n",
      "Batch 65 loss: 25209.97160924309\n",
      "Batch 66 loss: 40544.53689973836\n",
      "Batch 67 loss: 33224.0456511418\n",
      "Batch 68 loss: 31120.32077278108\n",
      "Batch 69 loss: 46551.16685495703\n",
      "Batch 70 loss: 49663.73985302515\n",
      "Batch 71 loss: 31217.31973280704\n",
      "Batch 72 loss: 43171.39234260142\n",
      "Batch 73 loss: 46090.69109234753\n",
      "Batch 74 loss: 23346.112383284693\n",
      "Batch 75 loss: 59869.70610280339\n",
      "Batch 76 loss: 37380.17632017074\n",
      "Batch 77 loss: 48385.097963513625\n",
      "Batch 78 loss: 28689.498392819627\n",
      "Batch 79 loss: 33989.74498422818\n",
      "Batch 80 loss: 36876.6899381284\n",
      "Batch 81 loss: 25071.19207969239\n",
      "Batch 82 loss: 62583.11018899941\n",
      "Batch 83 loss: 38546.94679888576\n",
      "Batch 84 loss: 43475.522603171485\n",
      "Batch 85 loss: 31097.379115251744\n",
      "Batch 86 loss: 39465.11483422342\n",
      "Batch 87 loss: 37956.41608119369\n",
      "Batch 88 loss: 44695.553912014264\n",
      "Batch 89 loss: 43868.348869945636\n",
      "Batch 90 loss: 33836.03655375556\n",
      "Batch 91 loss: 32353.886728862068\n",
      "Batch 92 loss: 41955.50971198143\n",
      "Batch 93 loss: 27982.354780040823\n",
      "Batch 94 loss: 34885.057592638055\n",
      "Batch 95 loss: 38499.07947348991\n",
      "Batch 96 loss: 50501.145440292734\n",
      "Batch 97 loss: 30407.800806607116\n",
      "Batch 98 loss: 45847.309607801435\n",
      "Batch 99 loss: 34405.93143221022\n",
      "Batch 100 loss: 35002.09813588485\n",
      "Batch 101 loss: 39029.098427334065\n",
      "Batch 102 loss: 34890.71239024025\n",
      "Batch 103 loss: 41614.08464096283\n",
      "Batch 104 loss: 51183.64335039065\n",
      "Batch 105 loss: 43439.54218496004\n",
      "Batch 106 loss: 48484.376391161735\n",
      "Batch 107 loss: 29299.78329143246\n",
      "Batch 108 loss: 54647.418056056566\n",
      "Batch 109 loss: 42556.724876698005\n",
      "Batch 110 loss: 45373.82772351669\n",
      "Batch 111 loss: 26736.056591591427\n",
      "Batch 112 loss: 53170.67832681435\n",
      "Batch 113 loss: 39076.75852194489\n",
      "Batch 114 loss: 44614.55783965385\n",
      "Batch 115 loss: 28182.231015392375\n",
      "Batch 116 loss: 43336.82946284154\n",
      "Batch 117 loss: 30121.961356153035\n",
      "Batch 118 loss: 49091.23418266066\n",
      "Batch 119 loss: 33146.58327527438\n",
      "Batch 120 loss: 60272.54495847792\n",
      "Batch 121 loss: 28352.46177117739\n",
      "Batch 122 loss: 35287.2623008317\n",
      "Batch 123 loss: 29423.449332825418\n",
      "Batch 124 loss: 45411.33550074282\n",
      "Batch 125 loss: 30561.576906582188\n",
      "Batch 126 loss: 40315.69578871537\n",
      "Batch 127 loss: 38008.03597803378\n",
      "Batch 128 loss: 49421.40402467237\n",
      "Batch 129 loss: 31501.668748881923\n",
      "Batch 130 loss: 42418.60075260651\n",
      "Batch 131 loss: 51050.041019098106\n",
      "Batch 132 loss: 35458.31726648495\n",
      "Batch 133 loss: 45005.73250700085\n",
      "Batch 134 loss: 33603.08172295219\n",
      "Batch 135 loss: 30609.715452537064\n",
      "Batch 136 loss: 43244.71135814975\n",
      "Batch 137 loss: 44010.974647690135\n",
      "Batch 138 loss: 48087.429168118026\n",
      "Batch 139 loss: 40235.111245883316\n",
      "Batch 140 loss: 28830.883753119528\n",
      "Batch 141 loss: 20046.112459547305\n",
      "Batch 142 loss: 28729.95390390726\n",
      "Batch 143 loss: 30632.759897148186\n",
      "Batch 144 loss: 35840.85726907653\n",
      "Batch 145 loss: 40787.418728604665\n",
      "Batch 146 loss: 44635.32480511409\n",
      "Batch 147 loss: 22861.661717365732\n",
      "Batch 148 loss: 20548.24266589153\n",
      "Batch 149 loss: 36027.795151844766\n",
      "Batch 150 loss: 38031.71478755485\n",
      "Batch 151 loss: 36515.954359942174\n",
      "Batch 152 loss: 36778.37692644051\n",
      "Batch 153 loss: 38084.241944480156\n",
      "Batch 154 loss: 22165.73636569334\n",
      "Batch 155 loss: 27936.65992833209\n",
      "Batch 156 loss: 36462.60743168843\n",
      "Batch 157 loss: 32029.655313354462\n",
      "Batch 158 loss: 47052.30305263055\n",
      "Batch 159 loss: 28580.496777329834\n",
      "Batch 160 loss: 43886.09518778539\n",
      "Batch 161 loss: 28466.44962158485\n",
      "Batch 162 loss: 33370.31794496534\n",
      "Batch 163 loss: 45692.76621991273\n",
      "Batch 164 loss: 34246.028013198986\n",
      "Batch 165 loss: 48311.530080394834\n",
      "Batch 166 loss: 45780.02544462354\n",
      "Batch 167 loss: 36762.432719814504\n",
      "Batch 168 loss: 25535.15737781111\n",
      "Batch 169 loss: 54910.57435625988\n",
      "Batch 170 loss: 25994.895222109684\n",
      "Batch 171 loss: 36501.777751690715\n",
      "Batch 172 loss: 44269.44106716695\n",
      "Batch 173 loss: 50728.65190076806\n",
      "Batch 174 loss: 43628.35277017667\n",
      "Batch 175 loss: 47279.70704335253\n",
      "Batch 176 loss: 42521.534962500824\n",
      "Batch 177 loss: 35868.53690616878\n",
      "Batch 178 loss: 24796.730493328454\n",
      "Batch 179 loss: 42706.13039268847\n",
      "Batch 180 loss: 30119.51698962486\n",
      "Batch 181 loss: 20428.188216628332\n",
      "Batch 182 loss: 33509.86790971091\n",
      "Batch 183 loss: 26587.211231892303\n",
      "Batch 184 loss: 25627.741012189712\n",
      "Batch 185 loss: 45820.334821727796\n",
      "Batch 186 loss: 27452.839231503942\n",
      "Batch 187 loss: 35420.34673894509\n",
      "Batch 188 loss: 36205.16332120643\n",
      "Batch 189 loss: 39319.71292868853\n",
      "Batch 190 loss: 26536.882973198262\n",
      "Batch 191 loss: 35205.91051097184\n",
      "Batch 192 loss: 48643.371671877954\n",
      "Batch 193 loss: 41459.6132942461\n",
      "Batch 194 loss: 45542.735053586686\n",
      "Batch 195 loss: 40122.31420158147\n",
      "Batch 196 loss: 41628.47162739948\n",
      "Batch 197 loss: 37997.61111790188\n",
      "Batch 198 loss: 34320.31912573563\n",
      "Batch 199 loss: 39090.070812906735\n",
      "Batch 200 loss: 32598.519037541555\n",
      "Batch 201 loss: 47666.86317761202\n",
      "Batch 202 loss: 36729.527012562496\n",
      "Batch 203 loss: 34592.70175860356\n",
      "Batch 204 loss: 48048.041797570055\n",
      "Batch 205 loss: 30888.558970250495\n",
      "Batch 206 loss: 40272.800642053575\n",
      "Batch 207 loss: 27326.95339530165\n",
      "Batch 208 loss: 29866.299386611656\n",
      "Batch 209 loss: 39049.840790446375\n",
      "Batch 210 loss: 47868.61783745541\n",
      "Batch 211 loss: 51681.130532383846\n",
      "Batch 212 loss: 59761.758116437166\n",
      "Batch 213 loss: 27868.972789290678\n",
      "Batch 214 loss: 32443.42298112261\n",
      "Batch 215 loss: 49264.97401076133\n",
      "Batch 216 loss: 42080.812000368\n",
      "Batch 217 loss: 51247.633481609424\n",
      "Batch 218 loss: 28347.951714108338\n",
      "Batch 219 loss: 54257.84331776606\n",
      "Batch 220 loss: 45953.42122528622\n",
      "Batch 221 loss: 24592.89149424589\n",
      "Batch 222 loss: 47278.37799306121\n",
      "Batch 223 loss: 28114.66838635309\n",
      "Batch 224 loss: 34501.77005608638\n",
      "Batch 225 loss: 36311.61842549022\n",
      "Batch 226 loss: 29093.026866840315\n",
      "Batch 227 loss: 42836.06490784835\n",
      "Batch 228 loss: 34957.56397057337\n",
      "Batch 229 loss: 28571.170873999075\n",
      "Batch 230 loss: 51849.18533576299\n",
      "Batch 231 loss: 33542.829420916794\n",
      "Batch 232 loss: 49598.35676226628\n",
      "Batch 233 loss: 55508.15367604824\n",
      "Batch 234 loss: 51817.849192590416\n",
      "Batch 235 loss: 51538.56302027744\n",
      "Batch 236 loss: 40604.153704287535\n",
      "Batch 237 loss: 31576.235632271968\n",
      "Batch 238 loss: 48054.858247020624\n",
      "Batch 239 loss: 27620.39661474648\n",
      "Batch 240 loss: 46915.9285514312\n",
      "Batch 241 loss: 39205.71008547314\n",
      "Batch 242 loss: 34314.495740596896\n",
      "Batch 243 loss: 44556.848842294545\n",
      "Batch 244 loss: 40065.756781581076\n",
      "Batch 245 loss: 32142.64815878197\n",
      "Batch 246 loss: 33057.53545449822\n",
      "Batch 247 loss: 27802.782530822722\n",
      "Batch 248 loss: 28076.730630782797\n",
      "Batch 249 loss: 54015.36103479955\n",
      "### Epoch 2\n",
      "Batch 0 loss: 52387.55810196414\n",
      "Batch 1 loss: 45956.429448151706\n",
      "Batch 2 loss: 34456.429048836784\n",
      "Batch 3 loss: 25603.753454425696\n",
      "Batch 4 loss: 25422.312172535363\n",
      "Batch 5 loss: 39796.24417437416\n",
      "Batch 6 loss: 27267.73935045953\n",
      "Batch 7 loss: 71814.94432292464\n",
      "Batch 8 loss: 32514.010962145556\n",
      "Batch 9 loss: 41390.97808777684\n",
      "Batch 10 loss: 34015.69498643015\n",
      "Batch 11 loss: 43986.191296947596\n",
      "Batch 12 loss: 45376.94766753943\n",
      "Batch 13 loss: 41785.68740708554\n",
      "Batch 14 loss: 40652.53442921711\n",
      "Batch 15 loss: 27651.62611158068\n",
      "Batch 16 loss: 34191.4398351495\n",
      "Batch 17 loss: 41686.42947731056\n",
      "Batch 18 loss: 63793.38250129977\n",
      "Batch 19 loss: 39498.04621635255\n",
      "Batch 20 loss: 35506.47980065976\n",
      "Batch 21 loss: 26676.97147632105\n",
      "Batch 22 loss: 42240.820860245534\n",
      "Batch 23 loss: 13469.744363691134\n",
      "Batch 24 loss: 34509.77840451793\n",
      "Batch 25 loss: 34987.23667294425\n",
      "Batch 26 loss: 32854.30643481126\n",
      "Batch 27 loss: 29627.26919860114\n",
      "Batch 28 loss: 30944.113888397427\n",
      "Batch 29 loss: 37522.88138349673\n",
      "Batch 30 loss: 33094.47981973607\n",
      "Batch 31 loss: 20361.839492279953\n",
      "Batch 32 loss: 24601.04862481285\n",
      "Batch 33 loss: 39129.71473951169\n",
      "Batch 34 loss: 70123.01711903632\n",
      "Batch 35 loss: 52117.11452787844\n",
      "Batch 36 loss: 29631.54499568696\n",
      "Batch 37 loss: 41040.90030054811\n",
      "Batch 38 loss: 43050.046315901505\n",
      "Batch 39 loss: 24851.733466586018\n",
      "Batch 40 loss: 53983.31235518203\n",
      "Batch 41 loss: 25692.777897484095\n",
      "Batch 42 loss: 24738.563971858246\n",
      "Batch 43 loss: 45905.5927890897\n",
      "Batch 44 loss: 41152.22964446028\n",
      "Batch 45 loss: 38150.617547339876\n",
      "Batch 46 loss: 56289.15117681713\n",
      "Batch 47 loss: 28640.686758513133\n",
      "Batch 48 loss: 47611.05409205828\n",
      "Batch 49 loss: 45313.37839037037\n",
      "Batch 50 loss: 34338.13201043246\n",
      "Batch 51 loss: 50050.09799502302\n",
      "Batch 52 loss: 29348.79849439243\n",
      "Batch 53 loss: 53863.122007223195\n",
      "Batch 54 loss: 41008.02757749701\n",
      "Batch 55 loss: 30412.704692531144\n",
      "Batch 56 loss: 36320.62639179163\n",
      "Batch 57 loss: 20359.247304069402\n",
      "Batch 58 loss: 43478.42269451867\n",
      "Batch 59 loss: 41307.692262446195\n",
      "Batch 60 loss: 32987.45885076659\n",
      "Batch 61 loss: 30261.217598092524\n",
      "Batch 62 loss: 63505.168530662166\n",
      "Batch 63 loss: 35198.75713892026\n",
      "Batch 64 loss: 46314.871202199676\n",
      "Batch 65 loss: 31151.350602929466\n",
      "Batch 66 loss: 28296.686009889847\n",
      "Batch 67 loss: 42871.893519633515\n",
      "Batch 68 loss: 37635.50629471714\n",
      "Batch 69 loss: 48347.76186964867\n",
      "Batch 70 loss: 54488.35014425337\n",
      "Batch 71 loss: 23563.314813279914\n",
      "Batch 72 loss: 54595.03318862058\n",
      "Batch 73 loss: 31320.5131376575\n",
      "Batch 74 loss: 26934.813165956766\n",
      "Batch 75 loss: 45892.22210907021\n",
      "Batch 76 loss: 41194.3766937483\n",
      "Batch 77 loss: 42140.54599208779\n",
      "Batch 78 loss: 53195.100448522215\n",
      "Batch 79 loss: 24380.66816721531\n",
      "Batch 80 loss: 51879.297341588965\n",
      "Batch 81 loss: 23975.66793387706\n",
      "Batch 82 loss: 33980.37471606546\n",
      "Batch 83 loss: 43952.02331253239\n",
      "Batch 84 loss: 32052.803854328286\n",
      "Batch 85 loss: 33811.16819837932\n",
      "Batch 86 loss: 24532.209980258416\n",
      "Batch 87 loss: 28270.54422185883\n",
      "Batch 88 loss: 43876.54998295474\n",
      "Batch 89 loss: 36296.74146071082\n",
      "Batch 90 loss: 70208.97029648474\n",
      "Batch 91 loss: 39424.430379971425\n",
      "Batch 92 loss: 47676.9762124657\n",
      "Batch 93 loss: 25663.183434282477\n",
      "Batch 94 loss: 44711.28165909203\n",
      "Batch 95 loss: 40343.752887303\n",
      "Batch 96 loss: 30648.286907034304\n",
      "Batch 97 loss: 31666.836527386215\n",
      "Batch 98 loss: 43259.32095321281\n",
      "Batch 99 loss: 30656.690434635784\n",
      "Batch 100 loss: 34410.83639584143\n",
      "Batch 101 loss: 39667.16396337235\n",
      "Batch 102 loss: 49278.5189981867\n",
      "Batch 103 loss: 35263.559906552764\n",
      "Batch 104 loss: 43184.87470671192\n",
      "Batch 105 loss: 34752.66044579986\n",
      "Batch 106 loss: 30939.073889639578\n",
      "Batch 107 loss: 35270.41054713946\n",
      "Batch 108 loss: 36917.707052690195\n",
      "Batch 109 loss: 44936.452312642396\n",
      "Batch 110 loss: 23410.79166193478\n",
      "Batch 111 loss: 42424.63736082796\n",
      "Batch 112 loss: 47378.50492192726\n",
      "Batch 113 loss: 39860.373656402226\n",
      "Batch 114 loss: 29790.789353074608\n",
      "Batch 115 loss: 40774.49472338607\n",
      "Batch 116 loss: 30108.19347804659\n",
      "Batch 117 loss: 52722.586348109166\n",
      "Batch 118 loss: 26744.83755151923\n",
      "Batch 119 loss: 47277.37370266966\n",
      "Batch 120 loss: 31502.295604230116\n",
      "Batch 121 loss: 34804.7994080959\n",
      "Batch 122 loss: 33837.72125938257\n",
      "Batch 123 loss: 36634.933990924146\n",
      "Batch 124 loss: 33431.829370087886\n",
      "Batch 125 loss: 25033.413619954285\n",
      "Batch 126 loss: 47350.68500480928\n",
      "Batch 127 loss: 59485.64088649131\n",
      "Batch 128 loss: 39790.26698012368\n",
      "Batch 129 loss: 24824.30361074621\n",
      "Batch 130 loss: 30120.93938058306\n",
      "Batch 131 loss: 31541.552894760443\n",
      "Batch 132 loss: 43807.63440135561\n",
      "Batch 133 loss: 45176.90558269572\n",
      "Batch 134 loss: 39414.09322089591\n",
      "Batch 135 loss: 39795.338966403244\n",
      "Batch 136 loss: 31222.67187066403\n",
      "Batch 137 loss: 34776.23753161318\n",
      "Batch 138 loss: 31072.08088612133\n",
      "Batch 139 loss: 44008.2205700559\n",
      "Batch 140 loss: 32457.756480793607\n",
      "Batch 141 loss: 38714.880046866776\n",
      "Batch 142 loss: 32760.543594293893\n",
      "Batch 143 loss: 37290.90255502032\n",
      "Batch 144 loss: 46530.37353378265\n",
      "Batch 145 loss: 22758.313128376045\n",
      "Batch 146 loss: 33139.87419568356\n",
      "Batch 147 loss: 38852.83681977374\n",
      "Batch 148 loss: 38355.427532389454\n",
      "Batch 149 loss: 27150.88813432243\n",
      "Batch 150 loss: 60978.65381938711\n",
      "Batch 151 loss: 32780.8403218709\n",
      "Batch 152 loss: 20049.655807523664\n",
      "Batch 153 loss: 33502.376222623185\n",
      "Batch 154 loss: 36728.75706649218\n",
      "Batch 155 loss: 45840.582189188324\n",
      "Batch 156 loss: 23200.605149186256\n",
      "Batch 157 loss: 39571.928908421345\n",
      "Batch 158 loss: 42603.42575880223\n",
      "Batch 159 loss: 24750.38483423013\n",
      "Batch 160 loss: 41066.898608571\n",
      "Batch 161 loss: 20968.360225321452\n",
      "Batch 162 loss: 55425.75758267054\n",
      "Batch 163 loss: 33640.631127228975\n",
      "Batch 164 loss: 32353.254041427943\n",
      "Batch 165 loss: 47848.10779683622\n",
      "Batch 166 loss: 42800.121404998594\n",
      "Batch 167 loss: 32703.30807003892\n",
      "Batch 168 loss: 32919.61639693617\n",
      "Batch 169 loss: 34267.79478382754\n",
      "Batch 170 loss: 21615.778883301948\n",
      "Batch 171 loss: 27932.513409203857\n",
      "Batch 172 loss: 27393.811252945285\n",
      "Batch 173 loss: 32958.25299480291\n",
      "Batch 174 loss: 35662.55684575076\n",
      "Batch 175 loss: 49696.99764446248\n",
      "Batch 176 loss: 57123.39064384898\n",
      "Batch 177 loss: 41017.163141074365\n",
      "Batch 178 loss: 38161.87463771209\n",
      "Batch 179 loss: 40326.04287939209\n",
      "Batch 180 loss: 26331.232417582134\n",
      "Batch 181 loss: 24911.712410314936\n",
      "Batch 182 loss: 25013.410833771308\n",
      "Batch 183 loss: 37472.22298602165\n",
      "Batch 184 loss: 48979.98638762291\n",
      "Batch 185 loss: 35987.95046342182\n",
      "Batch 186 loss: 58156.710033355535\n",
      "Batch 187 loss: 28080.09911340716\n",
      "Batch 188 loss: 45937.00453848857\n",
      "Batch 189 loss: 35785.12847473593\n",
      "Batch 190 loss: 34092.55462185593\n",
      "Batch 191 loss: 37674.96662039394\n",
      "Batch 192 loss: 39403.80819077522\n",
      "Batch 193 loss: 39415.7747050232\n",
      "Batch 194 loss: 23379.905306142136\n",
      "Batch 195 loss: 39105.09378478509\n",
      "Batch 196 loss: 39257.302985121845\n",
      "Batch 197 loss: 42107.346053129964\n",
      "Batch 198 loss: 47872.266449769886\n",
      "Batch 199 loss: 30023.228845150177\n",
      "Batch 200 loss: 32502.20447442826\n",
      "Batch 201 loss: 53375.85397159284\n",
      "Batch 202 loss: 36696.22220292165\n",
      "Batch 203 loss: 44186.59812822632\n",
      "Batch 204 loss: 33979.130160702356\n",
      "Batch 205 loss: 46869.4130962327\n",
      "Batch 206 loss: 40723.65449410364\n",
      "Batch 207 loss: 51023.523651996686\n",
      "Batch 208 loss: 41096.92371972648\n",
      "Batch 209 loss: 40842.73304417985\n",
      "Batch 210 loss: 52460.00689085271\n",
      "Batch 211 loss: 47478.12196804745\n",
      "Batch 212 loss: 33709.698032253094\n",
      "Batch 213 loss: 52729.927037833535\n",
      "Batch 214 loss: 30881.920165154035\n",
      "Batch 215 loss: 24744.972065996724\n",
      "Batch 216 loss: 33026.629532100465\n",
      "Batch 217 loss: 30790.325075498236\n",
      "Batch 218 loss: 46621.77573471582\n",
      "Batch 219 loss: 40043.5893968211\n",
      "Batch 220 loss: 49420.87572515354\n",
      "Batch 221 loss: 47163.83072161611\n",
      "Batch 222 loss: 38091.96553992969\n",
      "Batch 223 loss: 37652.968795592686\n",
      "Batch 224 loss: 42355.011834673685\n",
      "Batch 225 loss: 45730.08493564289\n",
      "Batch 226 loss: 35285.64611928288\n",
      "Batch 227 loss: 40574.17239084585\n",
      "Batch 228 loss: 46520.75618722216\n",
      "Batch 229 loss: 54213.266532947004\n",
      "Batch 230 loss: 46108.48163722759\n",
      "Batch 231 loss: 48624.49283197292\n",
      "Batch 232 loss: 23640.019556677496\n",
      "Batch 233 loss: 54666.05319164838\n",
      "Batch 234 loss: 34590.74053558351\n",
      "Batch 235 loss: 33191.28938110737\n",
      "Batch 236 loss: 30447.757188442753\n",
      "Batch 237 loss: 26259.462114679503\n",
      "Batch 238 loss: 23178.585868227485\n",
      "Batch 239 loss: 49168.08141179808\n",
      "Batch 240 loss: 34412.62323871918\n",
      "Batch 241 loss: 29560.990723225834\n",
      "Batch 242 loss: 46467.08428701473\n",
      "Batch 243 loss: 34153.42431461498\n",
      "Batch 244 loss: 60617.436834708875\n",
      "Batch 245 loss: 39450.024019336466\n",
      "Batch 246 loss: 33150.447075607095\n",
      "Batch 247 loss: 24521.720533556716\n",
      "Batch 248 loss: 36898.781678020256\n",
      "Batch 249 loss: 18609.93574553736\n",
      "### Epoch 3\n",
      "Batch 0 loss: 33910.1645465179\n",
      "Batch 1 loss: 60040.435930006366\n",
      "Batch 2 loss: 47082.72155103558\n",
      "Batch 3 loss: 25208.069011119267\n",
      "Batch 4 loss: 30752.68348865437\n",
      "Batch 5 loss: 28553.260756334017\n",
      "Batch 6 loss: 46529.00454233854\n",
      "Batch 7 loss: 40275.003440716086\n",
      "Batch 8 loss: 28723.291413989566\n",
      "Batch 9 loss: 39514.38602273977\n",
      "Batch 10 loss: 33766.51890774693\n",
      "Batch 11 loss: 29981.620709988718\n",
      "Batch 12 loss: 43767.65647426038\n",
      "Batch 13 loss: 34797.74680660563\n",
      "Batch 14 loss: 58104.129410752\n",
      "Batch 15 loss: 33880.794676634265\n",
      "Batch 16 loss: 35527.07136943676\n",
      "Batch 17 loss: 35913.199243591895\n",
      "Batch 18 loss: 17569.291802183667\n",
      "Batch 19 loss: 34310.708286494286\n",
      "Batch 20 loss: 49394.21456298504\n",
      "Batch 21 loss: 54349.37612145235\n",
      "Batch 22 loss: 36965.65249555769\n",
      "Batch 23 loss: 26683.586924730807\n",
      "Batch 24 loss: 40294.0420228929\n",
      "Batch 25 loss: 24180.323762220847\n",
      "Batch 26 loss: 39845.36898390584\n",
      "Batch 27 loss: 41666.791631092805\n",
      "Batch 28 loss: 39091.88155417898\n",
      "Batch 29 loss: 53127.20928546544\n",
      "Batch 30 loss: 23252.945244575076\n",
      "Batch 31 loss: 25740.973896266965\n",
      "Batch 32 loss: 37473.872844180754\n",
      "Batch 33 loss: 30664.989451344067\n",
      "Batch 34 loss: 36198.143233749506\n",
      "Batch 35 loss: 43460.45508591687\n",
      "Batch 36 loss: 56023.58354385838\n",
      "Batch 37 loss: 40938.06420999469\n",
      "Batch 38 loss: 41103.68782974301\n",
      "Batch 39 loss: 42031.16312438094\n",
      "Batch 40 loss: 25044.986999610956\n",
      "Batch 41 loss: 26318.86379009571\n",
      "Batch 42 loss: 36819.5879544988\n",
      "Batch 43 loss: 48389.734816712895\n",
      "Batch 44 loss: 42438.29590533101\n",
      "Batch 45 loss: 35465.72130143671\n",
      "Batch 46 loss: 50948.56284387086\n",
      "Batch 47 loss: 45415.3525987999\n",
      "Batch 48 loss: 40515.323148172916\n",
      "Batch 49 loss: 39728.94052516699\n",
      "Batch 50 loss: 16912.116026538883\n",
      "Batch 51 loss: 35068.236080784875\n",
      "Batch 52 loss: 21506.0861667573\n",
      "Batch 53 loss: 65674.498359701\n",
      "Batch 54 loss: 30281.92041567679\n",
      "Batch 55 loss: 26658.29758582113\n",
      "Batch 56 loss: 27326.414223744523\n",
      "Batch 57 loss: 34687.49401033345\n",
      "Batch 58 loss: 32153.348065941296\n",
      "Batch 59 loss: 31606.431551791662\n",
      "Batch 60 loss: 44884.51497116686\n",
      "Batch 61 loss: 58419.606290463315\n",
      "Batch 62 loss: 40567.177451126365\n",
      "Batch 63 loss: 28894.39503152938\n",
      "Batch 64 loss: 37809.61557348994\n",
      "Batch 65 loss: 36349.861748963114\n",
      "Batch 66 loss: 58322.89285217396\n",
      "Batch 67 loss: 36834.048276035355\n",
      "Batch 68 loss: 34546.89196774399\n",
      "Batch 69 loss: 49976.191054558265\n",
      "Batch 70 loss: 35122.817023928415\n",
      "Batch 71 loss: 32832.68261609685\n",
      "Batch 72 loss: 37494.0985740356\n",
      "Batch 73 loss: 31047.00887055457\n",
      "Batch 74 loss: 45298.8435009826\n",
      "Batch 75 loss: 35525.14506389325\n",
      "Batch 76 loss: 44493.22281887659\n",
      "Batch 77 loss: 43652.33103226798\n",
      "Batch 78 loss: 47861.31922193329\n",
      "Batch 79 loss: 57190.89771235206\n",
      "Batch 80 loss: 52800.849621875546\n",
      "Batch 81 loss: 30859.7263095086\n",
      "Batch 82 loss: 36720.777189116416\n",
      "Batch 83 loss: 40115.1518237806\n",
      "Batch 84 loss: 38833.58057021677\n",
      "Batch 85 loss: 30649.65905857379\n",
      "Batch 86 loss: 21015.983621405212\n",
      "Batch 87 loss: 38993.63682743452\n",
      "Batch 88 loss: 31076.1174885455\n",
      "Batch 89 loss: 37844.96749855472\n",
      "Batch 90 loss: 42339.899870167035\n",
      "Batch 91 loss: 45230.097610287354\n",
      "Batch 92 loss: 51103.82011354591\n",
      "Batch 93 loss: 44328.19051616149\n",
      "Batch 94 loss: 29406.236653270767\n",
      "Batch 95 loss: 27768.70942418048\n",
      "Batch 96 loss: 36659.7495476991\n",
      "Batch 97 loss: 35735.548646512216\n",
      "Batch 98 loss: 50167.121047351306\n",
      "Batch 99 loss: 35493.75103543856\n",
      "Batch 100 loss: 23778.053682415135\n",
      "Batch 101 loss: 38867.85876596642\n",
      "Batch 102 loss: 42607.67889193444\n",
      "Batch 103 loss: 30435.255376687142\n",
      "Batch 104 loss: 25372.31807119107\n",
      "Batch 105 loss: 27186.38976664016\n",
      "Batch 106 loss: 41502.89020637042\n",
      "Batch 107 loss: 40755.839574917045\n",
      "Batch 108 loss: 54106.50552069952\n",
      "Batch 109 loss: 36743.123301439846\n",
      "Batch 110 loss: 37309.884079279276\n",
      "Batch 111 loss: 33957.81273239239\n",
      "Batch 112 loss: 35153.513019725986\n",
      "Batch 113 loss: 44616.4146948431\n",
      "Batch 114 loss: 36764.847273260755\n",
      "Batch 115 loss: 32933.0289459064\n",
      "Batch 116 loss: 42891.63740203307\n",
      "Batch 117 loss: 52482.70610140964\n",
      "Batch 118 loss: 37102.571932233564\n",
      "Batch 119 loss: 28176.0209625378\n",
      "Batch 120 loss: 66664.82664635853\n",
      "Batch 121 loss: 50057.30297116485\n",
      "Batch 122 loss: 33470.48342598041\n",
      "Batch 123 loss: 54697.03771141067\n",
      "Batch 124 loss: 30005.576099659396\n",
      "Batch 125 loss: 46364.0711603759\n",
      "Batch 126 loss: 22486.180190637835\n",
      "Batch 127 loss: 38523.45331544333\n",
      "Batch 128 loss: 41957.28224172289\n",
      "Batch 129 loss: 32936.04911754012\n",
      "Batch 130 loss: 27872.10122131665\n",
      "Batch 131 loss: 24863.15311934475\n",
      "Batch 132 loss: 50440.647775348385\n",
      "Batch 133 loss: 34193.71425184026\n",
      "Batch 134 loss: 41198.166918161856\n",
      "Batch 135 loss: 26958.131324148566\n",
      "Batch 136 loss: 50671.1427081779\n",
      "Batch 137 loss: 46461.976581469826\n",
      "Batch 138 loss: 18459.46384963191\n",
      "Batch 139 loss: 44660.208234399004\n",
      "Batch 140 loss: 33944.06708804733\n",
      "Batch 141 loss: 38689.35053986021\n",
      "Batch 142 loss: 28107.099707885987\n",
      "Batch 143 loss: 43399.96138597497\n",
      "Batch 144 loss: 29993.73709154642\n",
      "Batch 145 loss: 52088.77807775205\n",
      "Batch 146 loss: 32670.831458222456\n",
      "Batch 147 loss: 38696.26481880256\n",
      "Batch 148 loss: 23839.476999012742\n",
      "Batch 149 loss: 40005.84196536581\n",
      "Batch 150 loss: 39455.039852326016\n",
      "Batch 151 loss: 47590.63967445172\n",
      "Batch 152 loss: 22136.566922135826\n",
      "Batch 153 loss: 36564.52758326335\n",
      "Batch 154 loss: 33741.77900601819\n",
      "Batch 155 loss: 30790.20110157586\n",
      "Batch 156 loss: 34803.12037413079\n",
      "Batch 157 loss: 40354.082362659676\n",
      "Batch 158 loss: 38067.18517805822\n",
      "Batch 159 loss: 38562.40307911373\n",
      "Batch 160 loss: 26875.986053112403\n",
      "Batch 161 loss: 41699.147580489545\n",
      "Batch 162 loss: 24869.713586539146\n",
      "Batch 163 loss: 42358.50143655143\n",
      "Batch 164 loss: 34371.39473173191\n",
      "Batch 165 loss: 35265.445777178975\n",
      "Batch 166 loss: 33234.26810851006\n",
      "Batch 167 loss: 16140.979266313516\n",
      "Batch 168 loss: 41578.8411000994\n",
      "Batch 169 loss: 37711.7413342907\n",
      "Batch 170 loss: 43221.83264604308\n",
      "Batch 171 loss: 36862.92506933206\n",
      "Batch 172 loss: 40663.79819304733\n",
      "Batch 173 loss: 34762.87727772823\n",
      "Batch 174 loss: 49478.302233664974\n",
      "Batch 175 loss: 48626.35934798929\n",
      "Batch 176 loss: 42393.22827670933\n",
      "Batch 177 loss: 46783.870075705025\n",
      "Batch 178 loss: 16376.984911922187\n",
      "Batch 179 loss: 43314.547273321645\n",
      "Batch 180 loss: 30441.264691908327\n",
      "Batch 181 loss: 41879.66937153224\n",
      "Batch 182 loss: 38858.849217647876\n",
      "Batch 183 loss: 58448.75900993172\n",
      "Batch 184 loss: 56859.60581415013\n",
      "Batch 185 loss: 39997.15213816535\n",
      "Batch 186 loss: 37082.822966332766\n",
      "Batch 187 loss: 56037.528741592076\n",
      "Batch 188 loss: 20872.386291687788\n",
      "Batch 189 loss: 45051.82929066806\n",
      "Batch 190 loss: 58083.31342917033\n",
      "Batch 191 loss: 38934.11472552566\n",
      "Batch 192 loss: 37255.73651714914\n",
      "Batch 193 loss: 44504.01490776427\n",
      "Batch 194 loss: 51823.06588991981\n",
      "Batch 195 loss: 40168.43107093392\n",
      "Batch 196 loss: 38615.95594722641\n",
      "Batch 197 loss: 24291.617583242205\n",
      "Batch 198 loss: 34434.53722357964\n",
      "Batch 199 loss: 46113.122356898275\n",
      "Batch 200 loss: 30566.192614486725\n",
      "Batch 201 loss: 47832.91252316673\n",
      "Batch 202 loss: 29163.173694960842\n",
      "Batch 203 loss: 23353.645785099572\n",
      "Batch 204 loss: 40673.92807392849\n",
      "Batch 205 loss: 52061.81000834318\n",
      "Batch 206 loss: 40252.15131044381\n",
      "Batch 207 loss: 37814.496139496165\n",
      "Batch 208 loss: 27992.354040055918\n",
      "Batch 209 loss: 49632.89950172782\n",
      "Batch 210 loss: 56310.83217270804\n",
      "Batch 211 loss: 30108.72425747552\n",
      "Batch 212 loss: 31583.06465989832\n",
      "Batch 213 loss: 39756.06024871634\n",
      "Batch 214 loss: 46197.26746601754\n",
      "Batch 215 loss: 31671.151979580904\n",
      "Batch 216 loss: 37680.47010366635\n",
      "Batch 217 loss: 31065.91966950473\n",
      "Batch 218 loss: 41581.86741982318\n",
      "Batch 219 loss: 24966.475408728023\n",
      "Batch 220 loss: 29240.77292133635\n",
      "Batch 221 loss: 28812.608867719424\n",
      "Batch 222 loss: 43647.096643381985\n",
      "Batch 223 loss: 31631.0782734742\n",
      "Batch 224 loss: 60037.72369130297\n",
      "Batch 225 loss: 53987.47603479751\n",
      "Batch 226 loss: 49008.87631069617\n",
      "Batch 227 loss: 33088.35593209722\n",
      "Batch 228 loss: 32485.872881020754\n",
      "Batch 229 loss: 30860.088502202372\n",
      "Batch 230 loss: 42459.059351127114\n",
      "Batch 231 loss: 35922.43781273876\n",
      "Batch 232 loss: 37758.05129559072\n",
      "Batch 233 loss: 34444.122549787106\n",
      "Batch 234 loss: 36914.841856624895\n",
      "Batch 235 loss: 37610.10384515639\n",
      "Batch 236 loss: 37362.48851670377\n",
      "Batch 237 loss: 31741.85489623875\n",
      "Batch 238 loss: 38650.13753665004\n",
      "Batch 239 loss: 43922.01740156622\n",
      "Batch 240 loss: 32100.387863532433\n",
      "Batch 241 loss: 25887.878677683846\n",
      "Batch 242 loss: 40332.600595132586\n",
      "Batch 243 loss: 22831.817750498984\n",
      "Batch 244 loss: 34225.93594631\n",
      "Batch 245 loss: 38714.033081789785\n",
      "Batch 246 loss: 32708.376233451207\n",
      "Batch 247 loss: 32151.04532608609\n",
      "Batch 248 loss: 49377.91983606261\n",
      "Batch 249 loss: 36919.84555920941\n",
      "### Epoch 4\n",
      "Batch 0 loss: 55310.02620767763\n",
      "Batch 1 loss: 28768.157112336547\n",
      "Batch 2 loss: 37396.47384013201\n",
      "Batch 3 loss: 54118.45125473727\n",
      "Batch 4 loss: 40333.51945389804\n",
      "Batch 5 loss: 24838.326155066075\n",
      "Batch 6 loss: 30044.4643150212\n",
      "Batch 7 loss: 40838.05995584088\n",
      "Batch 8 loss: 33739.14773087431\n",
      "Batch 9 loss: 26830.06302245832\n",
      "Batch 10 loss: 28610.7211499124\n",
      "Batch 11 loss: 56326.0108953146\n",
      "Batch 12 loss: 20932.92873324044\n",
      "Batch 13 loss: 46271.138753845546\n",
      "Batch 14 loss: 29739.39485835417\n",
      "Batch 15 loss: 35591.436920996835\n",
      "Batch 16 loss: 27938.313310633464\n",
      "Batch 17 loss: 50556.155118166884\n",
      "Batch 18 loss: 33575.927027427344\n",
      "Batch 19 loss: 32586.19468742403\n",
      "Batch 20 loss: 41428.98502145506\n",
      "Batch 21 loss: 42745.83143332541\n",
      "Batch 22 loss: 27244.592535336793\n",
      "Batch 23 loss: 48741.44193129972\n",
      "Batch 24 loss: 48841.274764232265\n",
      "Batch 25 loss: 60636.44452108897\n",
      "Batch 26 loss: 34638.63828312513\n",
      "Batch 27 loss: 38611.53887240586\n",
      "Batch 28 loss: 36181.168844982094\n",
      "Batch 29 loss: 40742.604368078246\n",
      "Batch 30 loss: 37803.33441225798\n",
      "Batch 31 loss: 70085.23632897082\n",
      "Batch 32 loss: 39225.27501937019\n",
      "Batch 33 loss: 23710.21065946485\n",
      "Batch 34 loss: 34655.270665252334\n",
      "Batch 35 loss: 44450.30997691958\n",
      "Batch 36 loss: 34531.13925758098\n",
      "Batch 37 loss: 37756.82865211734\n",
      "Batch 38 loss: 37603.73310427465\n",
      "Batch 39 loss: 55496.964897777885\n",
      "Batch 40 loss: 36933.449745779915\n",
      "Batch 41 loss: 42599.05558130822\n",
      "Batch 42 loss: 54941.01454185843\n",
      "Batch 43 loss: 42005.93227690784\n",
      "Batch 44 loss: 31619.003182288634\n",
      "Batch 45 loss: 39349.637867105084\n",
      "Batch 46 loss: 44834.02561688639\n",
      "Batch 47 loss: 37625.450115275875\n",
      "Batch 48 loss: 50702.312785622074\n",
      "Batch 49 loss: 24090.151939672272\n",
      "Batch 50 loss: 56043.040758909214\n",
      "Batch 51 loss: 43314.308216199715\n",
      "Batch 52 loss: 30072.184700869962\n",
      "Batch 53 loss: 45426.421429011454\n",
      "Batch 54 loss: 33150.446371843034\n",
      "Batch 55 loss: 29018.874044423304\n",
      "Batch 56 loss: 58663.40296476075\n",
      "Batch 57 loss: 43491.212645779364\n",
      "Batch 58 loss: 25284.779670898948\n",
      "Batch 59 loss: 37855.200248867775\n",
      "Batch 60 loss: 39443.384076971604\n",
      "Batch 61 loss: 31904.017571464476\n",
      "Batch 62 loss: 32459.448927576308\n",
      "Batch 63 loss: 32704.538335075966\n",
      "Batch 64 loss: 25769.825932431715\n",
      "Batch 65 loss: 37824.90934865707\n",
      "Batch 66 loss: 34784.47031889171\n",
      "Batch 67 loss: 47533.63311110223\n",
      "Batch 68 loss: 41430.33288980447\n",
      "Batch 69 loss: 66902.60715159286\n",
      "Batch 70 loss: 41994.53389339865\n",
      "Batch 71 loss: 40450.08539903157\n",
      "Batch 72 loss: 38288.86164402586\n",
      "Batch 73 loss: 49426.705268861384\n",
      "Batch 74 loss: 39473.10127016154\n",
      "Batch 75 loss: 39644.44169443349\n",
      "Batch 76 loss: 21096.837595240093\n",
      "Batch 77 loss: 35940.06171864339\n",
      "Batch 78 loss: 66516.0628564532\n",
      "Batch 79 loss: 34723.256616019324\n",
      "Batch 80 loss: 29518.29054686112\n",
      "Batch 81 loss: 24967.026854797266\n",
      "Batch 82 loss: 51592.433431854835\n",
      "Batch 83 loss: 40023.11317844184\n",
      "Batch 84 loss: 37332.727186414515\n",
      "Batch 85 loss: 52469.61929178334\n",
      "Batch 86 loss: 54606.6579184247\n",
      "Batch 87 loss: 39409.727268111346\n",
      "Batch 88 loss: 49252.44130103605\n",
      "Batch 89 loss: 33186.51389492246\n",
      "Batch 90 loss: 39337.66080482188\n",
      "Batch 91 loss: 41248.51235602879\n",
      "Batch 92 loss: 47445.010787347885\n",
      "Batch 93 loss: 50115.844151544334\n",
      "Batch 94 loss: 43715.81467732654\n",
      "Batch 95 loss: 35082.21866116597\n",
      "Batch 96 loss: 21188.889318510275\n",
      "Batch 97 loss: 44307.51430328619\n",
      "Batch 98 loss: 41638.07099058959\n",
      "Batch 99 loss: 42340.07613224792\n",
      "Batch 100 loss: 59689.3997858192\n",
      "Batch 101 loss: 32071.79027657798\n",
      "Batch 102 loss: 39743.90793232056\n",
      "Batch 103 loss: 36797.96004355515\n",
      "Batch 104 loss: 40613.65564311892\n",
      "Batch 105 loss: 38761.379310975506\n",
      "Batch 106 loss: 37799.08749775876\n",
      "Batch 107 loss: 40194.64267319617\n",
      "Batch 108 loss: 40574.683484489564\n",
      "Batch 109 loss: 15927.633628162019\n",
      "Batch 110 loss: 43570.43133225391\n",
      "Batch 111 loss: 26832.712061403447\n",
      "Batch 112 loss: 31849.96165236849\n",
      "Batch 113 loss: 39295.24291653186\n",
      "Batch 114 loss: 32233.165428365734\n",
      "Batch 115 loss: 31100.638828180585\n",
      "Batch 116 loss: 24848.89588710222\n",
      "Batch 117 loss: 28908.26594647693\n",
      "Batch 118 loss: 36695.05885718274\n",
      "Batch 119 loss: 33351.92229985835\n",
      "Batch 120 loss: 33825.05232772982\n",
      "Batch 121 loss: 31909.750939510384\n",
      "Batch 122 loss: 50791.27916983534\n",
      "Batch 123 loss: 28087.228591455103\n",
      "Batch 124 loss: 29707.51959622784\n",
      "Batch 125 loss: 31464.369080629644\n",
      "Batch 126 loss: 45287.39614016982\n",
      "Batch 127 loss: 30428.453645638743\n",
      "Batch 128 loss: 40295.94296893009\n",
      "Batch 129 loss: 33778.75130244783\n",
      "Batch 130 loss: 35687.916716270935\n",
      "Batch 131 loss: 31446.39571916493\n",
      "Batch 132 loss: 36465.686090996336\n",
      "Batch 133 loss: 38655.98661197294\n",
      "Batch 134 loss: 41405.10444620614\n",
      "Batch 135 loss: 26651.286874150606\n",
      "Batch 136 loss: 39247.821090576355\n",
      "Batch 137 loss: 35868.16334341878\n",
      "Batch 138 loss: 26934.98231105262\n",
      "Batch 139 loss: 38137.64000432291\n",
      "Batch 140 loss: 41125.486950678845\n",
      "Batch 141 loss: 59413.261461264236\n",
      "Batch 142 loss: 21158.192950043136\n",
      "Batch 143 loss: 31284.90749483346\n",
      "Batch 144 loss: 38289.76151853528\n",
      "Batch 145 loss: 37991.81838169566\n",
      "Batch 146 loss: 30388.967148138327\n",
      "Batch 147 loss: 34241.94461906231\n",
      "Batch 148 loss: 39892.64872985993\n",
      "Batch 149 loss: 30319.63953694698\n",
      "Batch 150 loss: 35181.370396832244\n",
      "Batch 151 loss: 34987.971459646666\n",
      "Batch 152 loss: 51297.97777615064\n",
      "Batch 153 loss: 38351.278047830754\n",
      "Batch 154 loss: 42254.15656664798\n",
      "Batch 155 loss: 25761.207674730824\n",
      "Batch 156 loss: 48160.51102272464\n",
      "Batch 157 loss: 42427.57018655352\n",
      "Batch 158 loss: 49540.105404534464\n",
      "Batch 159 loss: 35227.58286823296\n",
      "Batch 160 loss: 33073.22330877689\n",
      "Batch 161 loss: 34942.79159669878\n",
      "Batch 162 loss: 49115.98026249457\n",
      "Batch 163 loss: 27973.852699720603\n",
      "Batch 164 loss: 52093.343397986275\n",
      "Batch 165 loss: 27562.634425317778\n",
      "Batch 166 loss: 50316.843955417266\n",
      "Batch 167 loss: 22125.107575213024\n",
      "Batch 168 loss: 37561.78193513738\n",
      "Batch 169 loss: 17842.461992506098\n",
      "Batch 170 loss: 41370.67178242529\n",
      "Batch 171 loss: 38101.056487343325\n",
      "Batch 172 loss: 30266.51752759454\n",
      "Batch 173 loss: 28244.417064431087\n",
      "Batch 174 loss: 43288.67808949879\n",
      "Batch 175 loss: 29558.673098982574\n",
      "Batch 176 loss: 27921.087684907652\n",
      "Batch 177 loss: 34666.73967978877\n",
      "Batch 178 loss: 49617.70347485424\n",
      "Batch 179 loss: 38713.063185060484\n",
      "Batch 180 loss: 34055.09054697142\n",
      "Batch 181 loss: 36303.53009314793\n",
      "Batch 182 loss: 32396.779650211643\n",
      "Batch 183 loss: 40907.472428000736\n",
      "Batch 184 loss: 40436.53069323604\n",
      "Batch 185 loss: 32237.48163769892\n",
      "Batch 186 loss: 59974.79232148755\n",
      "Batch 187 loss: 52931.79946049502\n",
      "Batch 188 loss: 46301.85783696176\n",
      "Batch 189 loss: 34757.09838985781\n",
      "Batch 190 loss: 51554.08914355402\n",
      "Batch 191 loss: 37651.0537221829\n",
      "Batch 192 loss: 40037.11689102149\n",
      "Batch 193 loss: 36540.34860336353\n",
      "Batch 194 loss: 47487.71651812982\n",
      "Batch 195 loss: 55669.70722597058\n",
      "Batch 196 loss: 24191.291260698057\n",
      "Batch 197 loss: 47415.77139708215\n",
      "Batch 198 loss: 34567.605541056895\n",
      "Batch 199 loss: 20300.573398176966\n",
      "Batch 200 loss: 43118.55075596226\n",
      "Batch 201 loss: 34775.693959273216\n",
      "Batch 202 loss: 43555.469532548595\n",
      "Batch 203 loss: 48311.99898850726\n",
      "Batch 204 loss: 38378.77371689984\n",
      "Batch 205 loss: 24738.596531735573\n",
      "Batch 206 loss: 26184.326048421128\n",
      "Batch 207 loss: 43972.45855140424\n",
      "Batch 208 loss: 40048.65138919498\n",
      "Batch 209 loss: 45608.411388091045\n",
      "Batch 210 loss: 33144.85050413312\n",
      "Batch 211 loss: 46475.30492754428\n",
      "Batch 212 loss: 40407.821249236935\n",
      "Batch 213 loss: 41192.670939307085\n",
      "Batch 214 loss: 32326.863062169148\n",
      "Batch 215 loss: 38957.98400485065\n",
      "Batch 216 loss: 40038.691177369765\n",
      "Batch 217 loss: 35099.154743419225\n",
      "Batch 218 loss: 29096.4448313373\n",
      "Batch 219 loss: 33231.650946144815\n",
      "Batch 220 loss: 47618.40971662287\n",
      "Batch 221 loss: 43398.48809455511\n",
      "Batch 222 loss: 35777.08994550266\n",
      "Batch 223 loss: 50594.88365206969\n",
      "Batch 224 loss: 19897.100520130523\n",
      "Batch 225 loss: 42755.94527422353\n",
      "Batch 226 loss: 26112.859121599828\n",
      "Batch 227 loss: 36045.51506241652\n",
      "Batch 228 loss: 33434.894928162656\n",
      "Batch 229 loss: 28834.62569242101\n",
      "Batch 230 loss: 26441.227837005346\n",
      "Batch 231 loss: 30015.92880101762\n",
      "Batch 232 loss: 38290.12024499811\n",
      "Batch 233 loss: 37589.67844591105\n",
      "Batch 234 loss: 34259.35312515592\n",
      "Batch 235 loss: 36090.92377980379\n",
      "Batch 236 loss: 32239.746040235616\n",
      "Batch 237 loss: 34324.521137504766\n",
      "Batch 238 loss: 40781.06654224615\n",
      "Batch 239 loss: 29472.19842534481\n",
      "Batch 240 loss: 23500.547821771936\n",
      "Batch 241 loss: 41829.84976366699\n",
      "Batch 242 loss: 37545.455112386844\n",
      "Batch 243 loss: 38956.41142890866\n",
      "Batch 244 loss: 31914.685423296927\n",
      "Batch 245 loss: 43857.370524909915\n",
      "Batch 246 loss: 33465.10234778104\n",
      "Batch 247 loss: 26804.22917407773\n",
      "Batch 248 loss: 34249.245280185394\n",
      "Batch 249 loss: 45447.51908143466\n",
      "### Epoch 5\n",
      "Batch 0 loss: 37787.95910738953\n",
      "Batch 1 loss: 34509.45880898921\n",
      "Batch 2 loss: 24304.060998405035\n",
      "Batch 3 loss: 34606.26031363108\n",
      "Batch 4 loss: 35445.08603955173\n",
      "Batch 5 loss: 35319.234980847395\n",
      "Batch 6 loss: 43882.38777952269\n",
      "Batch 7 loss: 41143.76377368668\n",
      "Batch 8 loss: 42411.079596642456\n",
      "Batch 9 loss: 47076.500139317184\n",
      "Batch 10 loss: 25845.212895690438\n",
      "Batch 11 loss: 29681.016376251275\n",
      "Batch 12 loss: 45470.993721703344\n",
      "Batch 13 loss: 51488.023567392\n",
      "Batch 14 loss: 41504.452525025044\n",
      "Batch 15 loss: 39128.32547490748\n",
      "Batch 16 loss: 44427.750163064025\n",
      "Batch 17 loss: 47034.56792768001\n",
      "Batch 18 loss: 24935.92407498946\n",
      "Batch 19 loss: 37977.97950435463\n",
      "Batch 20 loss: 43543.7398450927\n",
      "Batch 21 loss: 24524.615940259522\n",
      "Batch 22 loss: 38717.82776759536\n",
      "Batch 23 loss: 35511.55016497311\n",
      "Batch 24 loss: 41703.235485971396\n",
      "Batch 25 loss: 19584.28328401812\n",
      "Batch 26 loss: 46583.709338621804\n",
      "Batch 27 loss: 41321.16586066648\n",
      "Batch 28 loss: 39956.10772736653\n",
      "Batch 29 loss: 45803.46102956145\n",
      "Batch 30 loss: 37257.833417951624\n",
      "Batch 31 loss: 51734.631366977366\n",
      "Batch 32 loss: 31551.20603569158\n",
      "Batch 33 loss: 43492.73177773002\n",
      "Batch 34 loss: 40061.7620983527\n",
      "Batch 35 loss: 46575.09343003399\n",
      "Batch 36 loss: 33537.275560764814\n",
      "Batch 37 loss: 48269.81795552456\n",
      "Batch 38 loss: 53270.07374832225\n",
      "Batch 39 loss: 40649.22407839929\n",
      "Batch 40 loss: 40186.17877357358\n",
      "Batch 41 loss: 31563.809263013325\n",
      "Batch 42 loss: 24173.520104950534\n",
      "Batch 43 loss: 25400.614790663356\n",
      "Batch 44 loss: 33702.65923861171\n",
      "Batch 45 loss: 53216.26855457376\n",
      "Batch 46 loss: 34434.39932332584\n",
      "Batch 47 loss: 27522.851699721105\n",
      "Batch 48 loss: 42744.06506333302\n",
      "Batch 49 loss: 36925.43080036799\n",
      "Batch 50 loss: 33137.74141903082\n",
      "Batch 51 loss: 26546.947557946707\n",
      "Batch 52 loss: 33925.44435676072\n",
      "Batch 53 loss: 22639.382734230654\n",
      "Batch 54 loss: 33957.20723168039\n",
      "Batch 55 loss: 57723.70251190789\n",
      "Batch 56 loss: 41878.78135481383\n",
      "Batch 57 loss: 41981.58688315813\n",
      "Batch 58 loss: 61273.207241937474\n",
      "Batch 59 loss: 46022.51960164972\n",
      "Batch 60 loss: 47879.059216951195\n",
      "Batch 61 loss: 30986.98293594794\n",
      "Batch 62 loss: 28670.321803357074\n",
      "Batch 63 loss: 34269.37019314419\n",
      "Batch 64 loss: 39871.6781725702\n",
      "Batch 65 loss: 37060.01430663119\n",
      "Batch 66 loss: 40638.48127716572\n",
      "Batch 67 loss: 30838.046679220766\n",
      "Batch 68 loss: 32677.284694380556\n",
      "Batch 69 loss: 23064.7136279614\n",
      "Batch 70 loss: 49645.10768480254\n",
      "Batch 71 loss: 51837.353022776995\n",
      "Batch 72 loss: 23155.71320701358\n",
      "Batch 73 loss: 39890.16525814374\n",
      "Batch 74 loss: 26420.612003076352\n",
      "Batch 75 loss: 43054.811100451916\n",
      "Batch 76 loss: 48151.763271637385\n",
      "Batch 77 loss: 30993.355276274015\n",
      "Batch 78 loss: 34229.014045596676\n",
      "Batch 79 loss: 35118.451272716884\n",
      "Batch 80 loss: 43043.466519341244\n",
      "Batch 81 loss: 51288.92291986472\n",
      "Batch 82 loss: 37427.47495334336\n",
      "Batch 83 loss: 53132.37048409762\n",
      "Batch 84 loss: 32121.843400386057\n",
      "Batch 85 loss: 42303.28030139594\n",
      "Batch 86 loss: 57033.467906985374\n",
      "Batch 87 loss: 37040.50558010692\n",
      "Batch 88 loss: 27607.09587576529\n",
      "Batch 89 loss: 40789.0949573964\n",
      "Batch 90 loss: 49702.20147699803\n",
      "Batch 91 loss: 45124.61758407703\n",
      "Batch 92 loss: 34986.464535313935\n",
      "Batch 93 loss: 30575.80616385418\n",
      "Batch 94 loss: 29678.83357272532\n",
      "Batch 95 loss: 36729.74988125108\n",
      "Batch 96 loss: 34581.529537214956\n",
      "Batch 97 loss: 43404.200295471404\n",
      "Batch 98 loss: 31343.30099115659\n",
      "Batch 99 loss: 31950.810309911372\n",
      "Batch 100 loss: 44230.86701173409\n",
      "Batch 101 loss: 39405.673403220855\n",
      "Batch 102 loss: 24937.112298359178\n",
      "Batch 103 loss: 26285.971158969533\n",
      "Batch 104 loss: 35092.67953175201\n",
      "Batch 105 loss: 36485.07800181141\n",
      "Batch 106 loss: 27456.00500010765\n",
      "Batch 107 loss: 33480.35350602497\n",
      "Batch 108 loss: 44591.8693522966\n",
      "Batch 109 loss: 16925.460411519165\n",
      "Batch 110 loss: 28519.737004839146\n",
      "Batch 111 loss: 43029.749939271984\n",
      "Batch 112 loss: 46964.38127011634\n",
      "Batch 113 loss: 38938.69336527708\n",
      "Batch 114 loss: 33705.840014702626\n",
      "Batch 115 loss: 39370.26449485477\n",
      "Batch 116 loss: 33007.95762720493\n",
      "Batch 117 loss: 33866.06649700595\n",
      "Batch 118 loss: 44338.37449011592\n",
      "Batch 119 loss: 41745.99549542974\n",
      "Batch 120 loss: 39799.23706273878\n",
      "Batch 121 loss: 28349.079993091094\n",
      "Batch 122 loss: 27791.14203395523\n",
      "Batch 123 loss: 32537.31792629636\n",
      "Batch 124 loss: 34729.65417545687\n",
      "Batch 125 loss: 41495.23956839474\n",
      "Batch 126 loss: 31812.21843326291\n",
      "Batch 127 loss: 51172.23808238303\n",
      "Batch 128 loss: 30035.898943773576\n",
      "Batch 129 loss: 55338.642250216704\n",
      "Batch 130 loss: 57170.341621283136\n",
      "Batch 131 loss: 24838.41132910646\n",
      "Batch 132 loss: 43931.79545450279\n",
      "Batch 133 loss: 40869.051922484156\n",
      "Batch 134 loss: 39224.90575636471\n",
      "Batch 135 loss: 28626.279252877135\n",
      "Batch 136 loss: 45106.70941047347\n",
      "Batch 137 loss: 37271.70770982865\n",
      "Batch 138 loss: 37107.592686336226\n",
      "Batch 139 loss: 43271.06690837961\n",
      "Batch 140 loss: 28921.774909958734\n",
      "Batch 141 loss: 46102.25161862558\n",
      "Batch 142 loss: 39865.236215750614\n",
      "Batch 143 loss: 17775.62460512226\n",
      "Batch 144 loss: 41266.61317555583\n",
      "Batch 145 loss: 43852.93093392006\n",
      "Batch 146 loss: 29636.917708908673\n",
      "Batch 147 loss: 40211.03041670338\n",
      "Batch 148 loss: 47272.06606574774\n",
      "Batch 149 loss: 39696.325730859935\n",
      "Batch 150 loss: 49819.25708114712\n",
      "Batch 151 loss: 20670.022432147078\n",
      "Batch 152 loss: 44193.919611923\n",
      "Batch 153 loss: 36781.90536475469\n",
      "Batch 154 loss: 55552.0490497992\n",
      "Batch 155 loss: 41989.26572163733\n",
      "Batch 156 loss: 33212.50107623975\n",
      "Batch 157 loss: 46671.63229640022\n",
      "Batch 158 loss: 30802.21056207401\n",
      "Batch 159 loss: 32808.8754097125\n",
      "Batch 160 loss: 18003.891211184848\n",
      "Batch 161 loss: 35088.95486025763\n",
      "Batch 162 loss: 40200.02723336585\n",
      "Batch 163 loss: 53203.437641891156\n",
      "Batch 164 loss: 40876.23415674311\n",
      "Batch 165 loss: 31429.277691343093\n",
      "Batch 166 loss: 35185.572634928445\n",
      "Batch 167 loss: 47996.92374453557\n",
      "Batch 168 loss: 31803.603809420674\n",
      "Batch 169 loss: 34622.022406792974\n",
      "Batch 170 loss: 50297.983385311585\n",
      "Batch 171 loss: 38577.70404234881\n",
      "Batch 172 loss: 30984.43076349364\n",
      "Batch 173 loss: 39476.94568740318\n",
      "Batch 174 loss: 37207.43404524621\n",
      "Batch 175 loss: 41408.36179344321\n",
      "Batch 176 loss: 44722.24367247232\n",
      "Batch 177 loss: 32190.058169082247\n",
      "Batch 178 loss: 42211.54704585229\n",
      "Batch 179 loss: 34796.105715209866\n",
      "Batch 180 loss: 38451.0449433014\n",
      "Batch 181 loss: 55286.06825103928\n",
      "Batch 182 loss: 25724.298013004038\n",
      "Batch 183 loss: 27067.946475807534\n",
      "Batch 184 loss: 32560.784960154517\n",
      "Batch 185 loss: 45249.76967646835\n",
      "Batch 186 loss: 36527.03693735105\n",
      "Batch 187 loss: 41175.17470274434\n",
      "Batch 188 loss: 44278.68753449352\n",
      "Batch 189 loss: 34095.14000123259\n",
      "Batch 190 loss: 43014.77061303261\n",
      "Batch 191 loss: 40101.37921667277\n",
      "Batch 192 loss: 40318.482289911066\n",
      "Batch 193 loss: 36152.13869435292\n",
      "Batch 194 loss: 38072.23535453314\n",
      "Batch 195 loss: 39507.13540290719\n",
      "Batch 196 loss: 38792.09094405492\n",
      "Batch 197 loss: 54511.180974282775\n",
      "Batch 198 loss: 26366.243586185825\n",
      "Batch 199 loss: 38506.414573625174\n",
      "Batch 200 loss: 28371.462309284434\n",
      "Batch 201 loss: 26506.213286093604\n",
      "Batch 202 loss: 37048.16454076354\n",
      "Batch 203 loss: 33695.531310920174\n",
      "Batch 204 loss: 42370.97386123145\n",
      "Batch 205 loss: 37571.12226049939\n",
      "Batch 206 loss: 55654.996410582266\n",
      "Batch 207 loss: 33500.68116973626\n",
      "Batch 208 loss: 38715.80481742762\n",
      "Batch 209 loss: 35492.51033663684\n",
      "Batch 210 loss: 48762.06289138454\n",
      "Batch 211 loss: 51610.34385879421\n",
      "Batch 212 loss: 33945.54012158645\n",
      "Batch 213 loss: 43665.78876464011\n",
      "Batch 214 loss: 29206.94886447622\n",
      "Batch 215 loss: 29511.465050265033\n",
      "Batch 216 loss: 46138.267898874736\n",
      "Batch 217 loss: 42801.61084049457\n",
      "Batch 218 loss: 29682.885994877215\n",
      "Batch 219 loss: 37889.38354176626\n",
      "Batch 220 loss: 32810.73625311292\n",
      "Batch 221 loss: 24990.905843647695\n",
      "Batch 222 loss: 33242.17479233685\n",
      "Batch 223 loss: 39519.33083249284\n",
      "Batch 224 loss: 49177.151729515506\n",
      "Batch 225 loss: 40944.38264954145\n",
      "Batch 226 loss: 46271.74998180456\n",
      "Batch 227 loss: 43236.02924340954\n",
      "Batch 228 loss: 24083.444682848014\n",
      "Batch 229 loss: 32384.226319832094\n",
      "Batch 230 loss: 36784.477477978144\n",
      "Batch 231 loss: 39731.520973372666\n",
      "Batch 232 loss: 25216.837564854097\n",
      "Batch 233 loss: 27307.225963558856\n",
      "Batch 234 loss: 33632.268653047875\n",
      "Batch 235 loss: 31030.467708940643\n",
      "Batch 236 loss: 41212.03612774731\n",
      "Batch 237 loss: 51791.00956161935\n",
      "Batch 238 loss: 27734.62243112866\n",
      "Batch 239 loss: 41633.10976664154\n",
      "Batch 240 loss: 50709.71371714635\n",
      "Batch 241 loss: 40097.33594222129\n",
      "Batch 242 loss: 24921.318338304434\n",
      "Batch 243 loss: 48915.72354137481\n",
      "Batch 244 loss: 46141.45544657288\n",
      "Batch 245 loss: 36004.52839951647\n",
      "Batch 246 loss: 40137.936605804345\n",
      "Batch 247 loss: 33965.13802996487\n",
      "Batch 248 loss: 48839.7513502145\n",
      "Batch 249 loss: 41878.84774228759\n",
      "### Epoch 6\n",
      "Batch 0 loss: 33820.35478417642\n",
      "Batch 1 loss: 42190.25965025381\n",
      "Batch 2 loss: 28401.858111722046\n",
      "Batch 3 loss: 40948.42383845038\n",
      "Batch 4 loss: 31311.720721653008\n",
      "Batch 5 loss: 45419.70182205814\n",
      "Batch 6 loss: 44166.64064094733\n",
      "Batch 7 loss: 33047.47900863805\n",
      "Batch 8 loss: 47903.210283148146\n",
      "Batch 9 loss: 47442.12735650547\n",
      "Batch 10 loss: 30550.512930008463\n",
      "Batch 11 loss: 41160.14059928732\n",
      "Batch 12 loss: 54027.9248084899\n",
      "Batch 13 loss: 42823.836774690695\n",
      "Batch 14 loss: 46205.94654405367\n",
      "Batch 15 loss: 49111.42840224582\n",
      "Batch 16 loss: 48202.827697021305\n",
      "Batch 17 loss: 38608.25434772162\n",
      "Batch 18 loss: 38039.02941228314\n",
      "Batch 19 loss: 39227.35750589629\n",
      "Batch 20 loss: 29693.738811794952\n",
      "Batch 21 loss: 24138.53394359748\n",
      "Batch 22 loss: 38833.730819673874\n",
      "Batch 23 loss: 60540.41958842567\n",
      "Batch 24 loss: 42930.88597750629\n",
      "Batch 25 loss: 43803.60536334797\n",
      "Batch 26 loss: 56238.58795730777\n",
      "Batch 27 loss: 39704.99028524217\n",
      "Batch 28 loss: 33246.745875614855\n",
      "Batch 29 loss: 49453.02861086563\n",
      "Batch 30 loss: 41678.23142787181\n",
      "Batch 31 loss: 48147.321309299616\n",
      "Batch 32 loss: 39004.33801824135\n",
      "Batch 33 loss: 50853.25797040216\n",
      "Batch 34 loss: 40287.18504504732\n",
      "Batch 35 loss: 23449.421081941204\n",
      "Batch 36 loss: 28503.468825707918\n",
      "Batch 37 loss: 47627.99422176988\n",
      "Batch 38 loss: 38430.459310753744\n",
      "Batch 39 loss: 21027.427625035798\n",
      "Batch 40 loss: 51926.56555022535\n",
      "Batch 41 loss: 49958.13236572913\n",
      "Batch 42 loss: 26274.201947957845\n",
      "Batch 43 loss: 46425.40873540557\n",
      "Batch 44 loss: 39092.17519049211\n",
      "Batch 45 loss: 29038.452508772443\n",
      "Batch 46 loss: 35929.9263673133\n",
      "Batch 47 loss: 36590.43544669377\n",
      "Batch 48 loss: 39294.793630519926\n",
      "Batch 49 loss: 37434.835132577246\n",
      "Batch 50 loss: 46873.91608725405\n",
      "Batch 51 loss: 45547.28144141747\n",
      "Batch 52 loss: 30450.45516816737\n",
      "Batch 53 loss: 40910.29380217314\n",
      "Batch 54 loss: 55374.696491143\n",
      "Batch 55 loss: 49440.715152483\n",
      "Batch 56 loss: 55226.74481432973\n",
      "Batch 57 loss: 31764.516832834564\n",
      "Batch 58 loss: 45976.34143633199\n",
      "Batch 59 loss: 29038.400672788386\n",
      "Batch 60 loss: 17531.812633780646\n",
      "Batch 61 loss: 28760.503286588224\n",
      "Batch 62 loss: 52056.04119775784\n",
      "Batch 63 loss: 34292.372566633756\n",
      "Batch 64 loss: 41763.425074605286\n",
      "Batch 65 loss: 26963.126305712365\n",
      "Batch 66 loss: 46895.80776665354\n",
      "Batch 67 loss: 30960.663757736285\n",
      "Batch 68 loss: 29919.00907075849\n",
      "Batch 69 loss: 26485.864781864617\n",
      "Batch 70 loss: 45869.02862004889\n",
      "Batch 71 loss: 48005.671437826895\n",
      "Batch 72 loss: 43587.92003841724\n",
      "Batch 73 loss: 39018.69321995099\n",
      "Batch 74 loss: 27568.534545483824\n",
      "Batch 75 loss: 37498.23782606991\n",
      "Batch 76 loss: 31870.59619045222\n",
      "Batch 77 loss: 32896.798479563426\n",
      "Batch 78 loss: 27471.942744573218\n",
      "Batch 79 loss: 22935.73551475126\n",
      "Batch 80 loss: 40689.1750496927\n",
      "Batch 81 loss: 41138.68678152076\n",
      "Batch 82 loss: 38512.31062807357\n",
      "Batch 83 loss: 33816.442196167685\n",
      "Batch 84 loss: 50364.80942350754\n",
      "Batch 85 loss: 26632.33263454264\n",
      "Batch 86 loss: 46805.7533571585\n",
      "Batch 87 loss: 32622.094844359865\n",
      "Batch 88 loss: 27767.435144381656\n",
      "Batch 89 loss: 23016.2897525147\n",
      "Batch 90 loss: 27690.27448928282\n",
      "Batch 91 loss: 44684.02138157021\n",
      "Batch 92 loss: 39196.99482376471\n",
      "Batch 93 loss: 22501.562456378277\n",
      "Batch 94 loss: 24941.430167348826\n",
      "Batch 95 loss: 38420.92246339955\n",
      "Batch 96 loss: 44731.45330606949\n",
      "Batch 97 loss: 24074.258440548045\n",
      "Batch 98 loss: 66447.44743897876\n",
      "Batch 99 loss: 28534.046899212157\n",
      "Batch 100 loss: 42216.33596828983\n",
      "Batch 101 loss: 33102.93717488858\n",
      "Batch 102 loss: 32841.01126138579\n",
      "Batch 103 loss: 33258.753124294904\n",
      "Batch 104 loss: 34484.98782045062\n",
      "Batch 105 loss: 31052.028633402442\n",
      "Batch 106 loss: 31759.87455450934\n",
      "Batch 107 loss: 44258.980552450026\n",
      "Batch 108 loss: 42820.58539466225\n",
      "Batch 109 loss: 38389.57732134233\n",
      "Batch 110 loss: 35710.344291428584\n",
      "Batch 111 loss: 40778.35644246676\n",
      "Batch 112 loss: 44125.07611161102\n",
      "Batch 113 loss: 29133.780744975465\n",
      "Batch 114 loss: 44272.79993600992\n",
      "Batch 115 loss: 44666.56816951743\n",
      "Batch 116 loss: 36276.680217141344\n",
      "Batch 117 loss: 30988.782941684567\n",
      "Batch 118 loss: 38683.759051637004\n",
      "Batch 119 loss: 35206.8157817016\n",
      "Batch 120 loss: 32211.460872312608\n",
      "Batch 121 loss: 33044.223214443686\n",
      "Batch 122 loss: 29196.50541766795\n",
      "Batch 123 loss: 30988.173422837703\n",
      "Batch 124 loss: 24311.864147894856\n",
      "Batch 125 loss: 26696.560140482063\n",
      "Batch 126 loss: 49482.37477422456\n",
      "Batch 127 loss: 44920.18158598764\n",
      "Batch 128 loss: 54532.27106088784\n",
      "Batch 129 loss: 38265.768927861216\n",
      "Batch 130 loss: 46392.25064973418\n",
      "Batch 131 loss: 31515.574128742388\n",
      "Batch 132 loss: 20943.43893912037\n",
      "Batch 133 loss: 31263.587687343326\n",
      "Batch 134 loss: 35095.44664857604\n",
      "Batch 135 loss: 67089.00954988468\n",
      "Batch 136 loss: 38103.878029475665\n",
      "Batch 137 loss: 35541.796457698314\n",
      "Batch 138 loss: 32238.899419001693\n",
      "Batch 139 loss: 28033.164329249103\n",
      "Batch 140 loss: 44643.84396752874\n",
      "Batch 141 loss: 35463.370797846124\n",
      "Batch 142 loss: 39105.42640359576\n",
      "Batch 143 loss: 44390.981928704496\n",
      "Batch 144 loss: 34825.22464005859\n",
      "Batch 145 loss: 25055.139805512496\n",
      "Batch 146 loss: 28364.724896563417\n",
      "Batch 147 loss: 31196.43060045342\n",
      "Batch 148 loss: 36096.73594706493\n",
      "Batch 149 loss: 30644.9020101965\n",
      "Batch 150 loss: 47809.99916510268\n",
      "Batch 151 loss: 27969.87271660405\n",
      "Batch 152 loss: 43842.24696226754\n",
      "Batch 153 loss: 31381.79683278319\n",
      "Batch 154 loss: 40190.14163127757\n",
      "Batch 155 loss: 53837.71074436428\n",
      "Batch 156 loss: 35084.46852898609\n",
      "Batch 157 loss: 38092.214471221\n",
      "Batch 158 loss: 32893.25613081955\n",
      "Batch 159 loss: 42148.35146768694\n",
      "Batch 160 loss: 32687.69913968288\n",
      "Batch 161 loss: 35716.947072523064\n",
      "Batch 162 loss: 42799.62662250675\n",
      "Batch 163 loss: 35742.93705811126\n",
      "Batch 164 loss: 39298.67890959691\n",
      "Batch 165 loss: 36128.211938768465\n",
      "Batch 166 loss: 66816.96130803417\n",
      "Batch 167 loss: 24965.074863420956\n",
      "Batch 168 loss: 31764.78523339235\n",
      "Batch 169 loss: 42819.5393836725\n",
      "Batch 170 loss: 49480.86547346987\n",
      "Batch 171 loss: 38921.934248973805\n",
      "Batch 172 loss: 41846.792617072744\n",
      "Batch 173 loss: 38683.82927270471\n",
      "Batch 174 loss: 47477.22292048413\n",
      "Batch 175 loss: 41584.51573720477\n",
      "Batch 176 loss: 32434.71363906785\n",
      "Batch 177 loss: 49304.94945427543\n",
      "Batch 178 loss: 33934.92430876053\n",
      "Batch 179 loss: 30791.28864732231\n",
      "Batch 180 loss: 31403.83981802229\n",
      "Batch 181 loss: 43144.44937511455\n",
      "Batch 182 loss: 43988.7321851201\n",
      "Batch 183 loss: 25476.2307821192\n",
      "Batch 184 loss: 28612.740541016967\n",
      "Batch 185 loss: 22335.488091744555\n",
      "Batch 186 loss: 40143.47060858235\n",
      "Batch 187 loss: 37145.93729051205\n",
      "Batch 188 loss: 68234.96993122678\n",
      "Batch 189 loss: 34551.22964244393\n",
      "Batch 190 loss: 65920.72712010535\n",
      "Batch 191 loss: 29817.889629878635\n",
      "Batch 192 loss: 30564.578365315487\n",
      "Batch 193 loss: 39005.66320362578\n",
      "Batch 194 loss: 26393.779745733336\n",
      "Batch 195 loss: 53874.567509518485\n",
      "Batch 196 loss: 57742.10388253722\n",
      "Batch 197 loss: 30035.077466354847\n",
      "Batch 198 loss: 28829.767270218526\n",
      "Batch 199 loss: 26383.04919632367\n",
      "Batch 200 loss: 39639.98663971295\n",
      "Batch 201 loss: 37770.01257683319\n",
      "Batch 202 loss: 50708.13394965154\n",
      "Batch 203 loss: 33348.07990428923\n",
      "Batch 204 loss: 37868.73812959283\n",
      "Batch 205 loss: 26234.68885754964\n",
      "Batch 206 loss: 34014.38446695708\n",
      "Batch 207 loss: 36491.253705519164\n",
      "Batch 208 loss: 28618.048282815987\n",
      "Batch 209 loss: 22423.35379453609\n",
      "Batch 210 loss: 27748.507004163308\n",
      "Batch 211 loss: 30986.048331640337\n",
      "Batch 212 loss: 38307.421211176974\n",
      "Batch 213 loss: 63186.72077966154\n",
      "Batch 214 loss: 37540.260683801025\n",
      "Batch 215 loss: 32916.016918416586\n",
      "Batch 216 loss: 31257.156063764676\n",
      "Batch 217 loss: 43673.45201094757\n",
      "Batch 218 loss: 29555.078791695152\n",
      "Batch 219 loss: 30204.65666865608\n",
      "Batch 220 loss: 37096.673459787045\n",
      "Batch 221 loss: 35384.563106871916\n",
      "Batch 222 loss: 32078.53282255294\n",
      "Batch 223 loss: 26494.782646787306\n",
      "Batch 224 loss: 37449.77752906733\n",
      "Batch 225 loss: 42154.7939419639\n",
      "Batch 226 loss: 34888.95835482089\n",
      "Batch 227 loss: 35497.62441347146\n",
      "Batch 228 loss: 37937.89691907496\n",
      "Batch 229 loss: 47275.22015985498\n",
      "Batch 230 loss: 52523.17553719582\n",
      "Batch 231 loss: 50414.24205734579\n",
      "Batch 232 loss: 38059.42175307163\n",
      "Batch 233 loss: 42672.77040167596\n",
      "Batch 234 loss: 34829.73898381235\n",
      "Batch 235 loss: 48626.68966241734\n",
      "Batch 236 loss: 37198.87066951295\n",
      "Batch 237 loss: 39481.96671183604\n",
      "Batch 238 loss: 24905.0621428273\n",
      "Batch 239 loss: 33678.77001678654\n",
      "Batch 240 loss: 44385.44494716235\n",
      "Batch 241 loss: 41556.686900107736\n",
      "Batch 242 loss: 40972.548387722476\n",
      "Batch 243 loss: 42376.02990826722\n",
      "Batch 244 loss: 40761.670502647976\n",
      "Batch 245 loss: 28895.360523829448\n",
      "Batch 246 loss: 49372.78291772282\n",
      "Batch 247 loss: 35064.25713506543\n",
      "Batch 248 loss: 41603.390360652695\n",
      "Batch 249 loss: 38619.38265974757\n",
      "### Epoch 7\n",
      "Batch 0 loss: 25101.62800861466\n",
      "Batch 1 loss: 34446.74000909858\n",
      "Batch 2 loss: 25735.78380998761\n",
      "Batch 3 loss: 34856.72660851776\n",
      "Batch 4 loss: 25982.648719930803\n",
      "Batch 5 loss: 31451.635352224956\n",
      "Batch 6 loss: 37589.53798737784\n",
      "Batch 7 loss: 49691.39627475253\n",
      "Batch 8 loss: 24977.549073985392\n",
      "Batch 9 loss: 32137.873304869034\n",
      "Batch 10 loss: 24137.351978316652\n",
      "Batch 11 loss: 35502.126558720665\n",
      "Batch 12 loss: 53674.61961239177\n",
      "Batch 13 loss: 31073.714400575554\n",
      "Batch 14 loss: 16675.07681388539\n",
      "Batch 15 loss: 42990.36720729633\n",
      "Batch 16 loss: 36206.90748364667\n",
      "Batch 17 loss: 34612.00540880847\n",
      "Batch 18 loss: 33067.98466625397\n",
      "Batch 19 loss: 32754.23690194356\n",
      "Batch 20 loss: 43462.17811459633\n",
      "Batch 21 loss: 29827.7564018956\n",
      "Batch 22 loss: 30383.487744726055\n",
      "Batch 23 loss: 48547.74803810297\n",
      "Batch 24 loss: 31030.306505149852\n",
      "Batch 25 loss: 41153.473828892544\n",
      "Batch 26 loss: 31957.628440368135\n",
      "Batch 27 loss: 36469.66683361924\n",
      "Batch 28 loss: 57276.000931489776\n",
      "Batch 29 loss: 32416.624319459086\n",
      "Batch 30 loss: 40866.77090283979\n",
      "Batch 31 loss: 33639.26671689621\n",
      "Batch 32 loss: 34642.070610350536\n",
      "Batch 33 loss: 31134.625767163183\n",
      "Batch 34 loss: 24389.91596432851\n",
      "Batch 35 loss: 67937.78527832268\n",
      "Batch 36 loss: 43299.648018786786\n",
      "Batch 37 loss: 25484.336503511084\n",
      "Batch 38 loss: 40052.78898883311\n",
      "Batch 39 loss: 39215.79094549845\n",
      "Batch 40 loss: 61855.31990357513\n",
      "Batch 41 loss: 45125.260395804675\n",
      "Batch 42 loss: 38294.02602336072\n",
      "Batch 43 loss: 34555.29392940283\n",
      "Batch 44 loss: 43455.89222710331\n",
      "Batch 45 loss: 30207.421574100103\n",
      "Batch 46 loss: 42957.94906430274\n",
      "Batch 47 loss: 39066.38254535649\n",
      "Batch 48 loss: 54763.31189347179\n",
      "Batch 49 loss: 29124.08205658337\n",
      "Batch 50 loss: 53330.443317968085\n",
      "Batch 51 loss: 38925.43273274356\n",
      "Batch 52 loss: 37987.64634613741\n",
      "Batch 53 loss: 46008.266381737456\n",
      "Batch 54 loss: 36519.560148832905\n",
      "Batch 55 loss: 37366.980585083256\n",
      "Batch 56 loss: 51222.1813668169\n",
      "Batch 57 loss: 28140.07241577847\n",
      "Batch 58 loss: 39357.23725918452\n",
      "Batch 59 loss: 45882.042484237216\n",
      "Batch 60 loss: 21093.74957490882\n",
      "Batch 61 loss: 34788.9489133688\n",
      "Batch 62 loss: 51535.69185606978\n",
      "Batch 63 loss: 35520.784267772244\n",
      "Batch 64 loss: 62317.37616972638\n",
      "Batch 65 loss: 31395.700802396364\n",
      "Batch 66 loss: 29331.541200601056\n",
      "Batch 67 loss: 59409.845684606895\n",
      "Batch 68 loss: 30403.945636597644\n",
      "Batch 69 loss: 33000.465372223574\n",
      "Batch 70 loss: 43115.497961555244\n",
      "Batch 71 loss: 26304.082201795005\n",
      "Batch 72 loss: 27698.628223356292\n",
      "Batch 73 loss: 30870.308167636053\n",
      "Batch 74 loss: 36887.17917549165\n",
      "Batch 75 loss: 45608.08348279088\n",
      "Batch 76 loss: 52785.790689661175\n",
      "Batch 77 loss: 36754.19397166835\n",
      "Batch 78 loss: 38121.803644739164\n",
      "Batch 79 loss: 30144.346900251443\n",
      "Batch 80 loss: 27821.868695110672\n",
      "Batch 81 loss: 22463.262648074582\n",
      "Batch 82 loss: 41221.3370817074\n",
      "Batch 83 loss: 41674.868649848715\n",
      "Batch 84 loss: 26095.797336512\n",
      "Batch 85 loss: 70276.43858306712\n",
      "Batch 86 loss: 37263.59222355213\n",
      "Batch 87 loss: 30470.304285083555\n",
      "Batch 88 loss: 27283.42669645329\n",
      "Batch 89 loss: 44859.03404420536\n",
      "Batch 90 loss: 32766.921789474844\n",
      "Batch 91 loss: 38596.351062506255\n",
      "Batch 92 loss: 45489.80365848022\n",
      "Batch 93 loss: 32973.33920240812\n",
      "Batch 94 loss: 31909.195879378312\n",
      "Batch 95 loss: 48743.14325486263\n",
      "Batch 96 loss: 44086.40304606968\n",
      "Batch 97 loss: 31922.189886122942\n",
      "Batch 98 loss: 27892.9288166493\n",
      "Batch 99 loss: 51151.54427746837\n",
      "Batch 100 loss: 36242.67855795508\n",
      "Batch 101 loss: 29264.073003970458\n",
      "Batch 102 loss: 33985.522119866975\n",
      "Batch 103 loss: 29161.960820632474\n",
      "Batch 104 loss: 36840.84503867203\n",
      "Batch 105 loss: 57645.49829021128\n",
      "Batch 106 loss: 28411.437276479734\n",
      "Batch 107 loss: 28782.786292215987\n",
      "Batch 108 loss: 36178.60820510742\n",
      "Batch 109 loss: 42182.13702539591\n",
      "Batch 110 loss: 23177.17478401689\n",
      "Batch 111 loss: 44441.68088167595\n",
      "Batch 112 loss: 43232.036894065706\n",
      "Batch 113 loss: 70058.42015992117\n",
      "Batch 114 loss: 32406.112384213196\n",
      "Batch 115 loss: 36230.906167752946\n",
      "Batch 116 loss: 35748.90289657911\n",
      "Batch 117 loss: 21201.97996875699\n",
      "Batch 118 loss: 38930.18701995322\n",
      "Batch 119 loss: 22287.44179808318\n",
      "Batch 120 loss: 20461.74113283818\n",
      "Batch 121 loss: 26345.913428890737\n",
      "Batch 122 loss: 38177.04780347041\n",
      "Batch 123 loss: 47485.16977159325\n",
      "Batch 124 loss: 34132.882552054885\n",
      "Batch 125 loss: 38760.5682010461\n",
      "Batch 126 loss: 45118.16844672258\n",
      "Batch 127 loss: 45232.88076406392\n",
      "Batch 128 loss: 36850.02520279105\n",
      "Batch 129 loss: 23039.745162587784\n",
      "Batch 130 loss: 43444.5546295706\n",
      "Batch 131 loss: 50473.41894569587\n",
      "Batch 132 loss: 35215.271020210464\n",
      "Batch 133 loss: 51767.498920498605\n",
      "Batch 134 loss: 45229.31578340686\n",
      "Batch 135 loss: 48410.41034793158\n",
      "Batch 136 loss: 44308.49938035453\n",
      "Batch 137 loss: 33003.78952353395\n",
      "Batch 138 loss: 25940.823186916277\n",
      "Batch 139 loss: 43406.53862539111\n",
      "Batch 140 loss: 37918.594210495416\n",
      "Batch 141 loss: 30751.063365040485\n",
      "Batch 142 loss: 21311.89309090664\n",
      "Batch 143 loss: 67732.67917493713\n",
      "Batch 144 loss: 24957.822378736815\n",
      "Batch 145 loss: 36392.353305507364\n",
      "Batch 146 loss: 38560.180643918226\n",
      "Batch 147 loss: 38250.123580843574\n",
      "Batch 148 loss: 49283.04877565432\n",
      "Batch 149 loss: 36859.89612876745\n",
      "Batch 150 loss: 64108.97189421277\n",
      "Batch 151 loss: 43561.32794494153\n",
      "Batch 152 loss: 39952.80974005513\n",
      "Batch 153 loss: 52003.496458522975\n",
      "Batch 154 loss: 23005.920358899584\n",
      "Batch 155 loss: 32964.99925113481\n",
      "Batch 156 loss: 37429.028423922915\n",
      "Batch 157 loss: 26382.623967818436\n",
      "Batch 158 loss: 31703.570425612066\n",
      "Batch 159 loss: 41986.02657449232\n",
      "Batch 160 loss: 39610.47173040897\n",
      "Batch 161 loss: 49445.74843500722\n",
      "Batch 162 loss: 18719.559124512376\n",
      "Batch 163 loss: 38409.503636387686\n",
      "Batch 164 loss: 38697.944727971284\n",
      "Batch 165 loss: 43806.26487956074\n",
      "Batch 166 loss: 37191.399318229174\n",
      "Batch 167 loss: 44551.14033407447\n",
      "Batch 168 loss: 42017.14917739014\n",
      "Batch 169 loss: 43947.217053655506\n",
      "Batch 170 loss: 39963.6109708751\n",
      "Batch 171 loss: 53088.251909582475\n",
      "Batch 172 loss: 32190.92396240364\n",
      "Batch 173 loss: 35808.308827933106\n",
      "Batch 174 loss: 48447.41811116021\n",
      "Batch 175 loss: 26037.034272089193\n",
      "Batch 176 loss: 40557.45421987245\n",
      "Batch 177 loss: 39487.418102170894\n",
      "Batch 178 loss: 27644.02942879606\n",
      "Batch 179 loss: 28468.543793065917\n",
      "Batch 180 loss: 55470.57533586961\n",
      "Batch 181 loss: 34626.88889902373\n",
      "Batch 182 loss: 26993.153763342103\n",
      "Batch 183 loss: 35216.440105991554\n",
      "Batch 184 loss: 42103.58477651332\n",
      "Batch 185 loss: 35737.64985069229\n",
      "Batch 186 loss: 33714.44860445376\n",
      "Batch 187 loss: 40126.12922038764\n",
      "Batch 188 loss: 37979.46914811443\n",
      "Batch 189 loss: 44733.99928386652\n",
      "Batch 190 loss: 30839.0889738581\n",
      "Batch 191 loss: 24365.547578097725\n",
      "Batch 192 loss: 34394.30887780426\n",
      "Batch 193 loss: 49493.34736887249\n",
      "Batch 194 loss: 29613.02401158824\n",
      "Batch 195 loss: 34998.031856506124\n",
      "Batch 196 loss: 43045.17202120574\n",
      "Batch 197 loss: 26088.691161339288\n",
      "Batch 198 loss: 46440.770937603666\n",
      "Batch 199 loss: 41919.21773391945\n",
      "Batch 200 loss: 42337.88977441826\n",
      "Batch 201 loss: 32698.39835919183\n",
      "Batch 202 loss: 25075.335850839954\n",
      "Batch 203 loss: 28475.556398897512\n",
      "Batch 204 loss: 42973.002761613905\n",
      "Batch 205 loss: 38978.024548776346\n",
      "Batch 206 loss: 30573.15938699532\n",
      "Batch 207 loss: 12362.650634774727\n",
      "Batch 208 loss: 35489.74568006177\n",
      "Batch 209 loss: 40286.79908291408\n",
      "Batch 210 loss: 38220.99564337491\n",
      "Batch 211 loss: 23072.446906913523\n",
      "Batch 212 loss: 50065.90183280947\n",
      "Batch 213 loss: 40909.80684149534\n",
      "Batch 214 loss: 40153.68279710975\n",
      "Batch 215 loss: 63330.73832806594\n",
      "Batch 216 loss: 29010.03812859813\n",
      "Batch 217 loss: 24607.35750553017\n",
      "Batch 218 loss: 35812.45035620178\n",
      "Batch 219 loss: 39399.42915280723\n",
      "Batch 220 loss: 37471.095409377805\n",
      "Batch 221 loss: 33989.675750554554\n",
      "Batch 222 loss: 42813.06433601335\n",
      "Batch 223 loss: 46629.90833747851\n",
      "Batch 224 loss: 42108.30729261708\n",
      "Batch 225 loss: 43187.26797388478\n",
      "Batch 226 loss: 47739.905631188696\n",
      "Batch 227 loss: 47492.45285148941\n",
      "Batch 228 loss: 41400.34854759515\n",
      "Batch 229 loss: 36936.705188641376\n",
      "Batch 230 loss: 55232.937812719785\n",
      "Batch 231 loss: 39997.786188949816\n",
      "Batch 232 loss: 35478.47947694149\n",
      "Batch 233 loss: 48779.66875760059\n",
      "Batch 234 loss: 42176.676394008384\n",
      "Batch 235 loss: 28005.560540321792\n",
      "Batch 236 loss: 50685.34970372801\n",
      "Batch 237 loss: 47333.3195432791\n",
      "Batch 238 loss: 45724.72471983243\n",
      "Batch 239 loss: 45449.52130127067\n",
      "Batch 240 loss: 33681.91341038566\n",
      "Batch 241 loss: 50374.661868754614\n",
      "Batch 242 loss: 40441.95714523671\n",
      "Batch 243 loss: 40063.62029500684\n",
      "Batch 244 loss: 40804.05036593592\n",
      "Batch 245 loss: 30914.08821046641\n",
      "Batch 246 loss: 44277.962111341556\n",
      "Batch 247 loss: 36069.403441988165\n",
      "Batch 248 loss: 29667.66610966719\n",
      "Batch 249 loss: 29090.58376503206\n",
      "### Epoch 8\n",
      "Batch 0 loss: 39050.00341061732\n",
      "Batch 1 loss: 59847.73256224718\n",
      "Batch 2 loss: 43401.00823636395\n",
      "Batch 3 loss: 35193.260151577866\n",
      "Batch 4 loss: 23771.68985504801\n",
      "Batch 5 loss: 31875.87558169739\n",
      "Batch 6 loss: 39875.924666548446\n",
      "Batch 7 loss: 31629.87452037041\n",
      "Batch 8 loss: 45634.3052725446\n",
      "Batch 9 loss: 43645.48605906798\n",
      "Batch 10 loss: 47598.1800232177\n",
      "Batch 11 loss: 51750.9967906714\n",
      "Batch 12 loss: 35559.150756635245\n",
      "Batch 13 loss: 35600.86271355148\n",
      "Batch 14 loss: 49986.704628116495\n",
      "Batch 15 loss: 63221.52796699421\n",
      "Batch 16 loss: 43614.39347860765\n",
      "Batch 17 loss: 40587.17099723007\n",
      "Batch 18 loss: 41614.267042568805\n",
      "Batch 19 loss: 34498.95672022144\n",
      "Batch 20 loss: 46435.05581141083\n",
      "Batch 21 loss: 37226.19979050108\n",
      "Batch 22 loss: 26982.884732730123\n",
      "Batch 23 loss: 27659.690692103504\n",
      "Batch 24 loss: 35914.74214289716\n",
      "Batch 25 loss: 29612.696077546167\n",
      "Batch 26 loss: 38022.97213090552\n",
      "Batch 27 loss: 23854.100378456365\n",
      "Batch 28 loss: 37062.26999633937\n",
      "Batch 29 loss: 45629.30515479665\n",
      "Batch 30 loss: 41671.612456804694\n",
      "Batch 31 loss: 35425.74911223208\n",
      "Batch 32 loss: 38012.26255240359\n",
      "Batch 33 loss: 33123.42465065942\n",
      "Batch 34 loss: 62087.38583307492\n",
      "Batch 35 loss: 60961.18644862815\n",
      "Batch 36 loss: 25627.782424677112\n",
      "Batch 37 loss: 30052.288372722513\n",
      "Batch 38 loss: 31383.689029223864\n",
      "Batch 39 loss: 34036.93485426355\n",
      "Batch 40 loss: 44418.32248034204\n",
      "Batch 41 loss: 29531.361424180664\n",
      "Batch 42 loss: 26820.492834088007\n",
      "Batch 43 loss: 19732.980730785064\n",
      "Batch 44 loss: 60544.67572331015\n",
      "Batch 45 loss: 47216.43214815859\n",
      "Batch 46 loss: 33480.52625144129\n",
      "Batch 47 loss: 35660.34761147022\n",
      "Batch 48 loss: 43159.48710736609\n",
      "Batch 49 loss: 21445.72251934853\n",
      "Batch 50 loss: 29919.78762315855\n",
      "Batch 51 loss: 32269.99883869884\n",
      "Batch 52 loss: 47750.053459701754\n",
      "Batch 53 loss: 38140.163339976614\n",
      "Batch 54 loss: 31648.16216265021\n",
      "Batch 55 loss: 43996.99008474804\n",
      "Batch 56 loss: 49762.241752795504\n",
      "Batch 57 loss: 18316.212689771986\n",
      "Batch 58 loss: 56039.20768885802\n",
      "Batch 59 loss: 31010.986114294625\n",
      "Batch 60 loss: 39505.592658399444\n",
      "Batch 61 loss: 53733.52394539405\n",
      "Batch 62 loss: 50396.35383357778\n",
      "Batch 63 loss: 34475.09513013997\n",
      "Batch 64 loss: 40027.962153237095\n",
      "Batch 65 loss: 43443.46799050291\n",
      "Batch 66 loss: 38559.35167593244\n",
      "Batch 67 loss: 24733.49108091977\n",
      "Batch 68 loss: 34010.485988243774\n",
      "Batch 69 loss: 24960.88238153199\n",
      "Batch 70 loss: 31534.423657579788\n",
      "Batch 71 loss: 44225.97804578701\n",
      "Batch 72 loss: 28564.56434521347\n",
      "Batch 73 loss: 37509.70371222371\n",
      "Batch 74 loss: 34816.1734948809\n",
      "Batch 75 loss: 44315.8121653155\n",
      "Batch 76 loss: 30898.671907736374\n",
      "Batch 77 loss: 36036.864726869724\n",
      "Batch 78 loss: 30087.402649376134\n",
      "Batch 79 loss: 40907.31892297705\n",
      "Batch 80 loss: 40432.099617664746\n",
      "Batch 81 loss: 46970.94992491288\n",
      "Batch 82 loss: 56370.93526476456\n",
      "Batch 83 loss: 38095.836636539694\n",
      "Batch 84 loss: 44266.767952329144\n",
      "Batch 85 loss: 44903.318040752565\n",
      "Batch 86 loss: 37060.37492219428\n",
      "Batch 87 loss: 33652.176686044506\n",
      "Batch 88 loss: 28487.446645179327\n",
      "Batch 89 loss: 49000.88915316694\n",
      "Batch 90 loss: 44721.60388519704\n",
      "Batch 91 loss: 37930.61243802039\n",
      "Batch 92 loss: 35225.77690287291\n",
      "Batch 93 loss: 22757.699820651345\n",
      "Batch 94 loss: 38677.847202397854\n",
      "Batch 95 loss: 36519.464144620906\n",
      "Batch 96 loss: 35467.18238326437\n",
      "Batch 97 loss: 35808.52666289229\n",
      "Batch 98 loss: 42867.79760073098\n",
      "Batch 99 loss: 47715.43073282659\n",
      "Batch 100 loss: 32800.41405807962\n",
      "Batch 101 loss: 56468.637889749254\n",
      "Batch 102 loss: 34921.76450675375\n",
      "Batch 103 loss: 31927.706249732226\n",
      "Batch 104 loss: 41253.12302913319\n",
      "Batch 105 loss: 37951.87813201946\n",
      "Batch 106 loss: 43359.0836364362\n",
      "Batch 107 loss: 34435.2515190458\n",
      "Batch 108 loss: 54155.442032021594\n",
      "Batch 109 loss: 60799.39677023041\n",
      "Batch 110 loss: 45773.581002780986\n",
      "Batch 111 loss: 37691.73667054185\n",
      "Batch 112 loss: 52927.434409437046\n",
      "Batch 113 loss: 28321.391481201757\n",
      "Batch 114 loss: 56684.21543958028\n",
      "Batch 115 loss: 24576.547039448033\n",
      "Batch 116 loss: 56261.16444691331\n",
      "Batch 117 loss: 32090.598227705468\n",
      "Batch 118 loss: 19476.70136573512\n",
      "Batch 119 loss: 26321.566593624502\n",
      "Batch 120 loss: 30173.434132774728\n",
      "Batch 121 loss: 48221.74003119375\n",
      "Batch 122 loss: 41006.60297728234\n",
      "Batch 123 loss: 19855.038945185708\n",
      "Batch 124 loss: 38446.40290270939\n",
      "Batch 125 loss: 37351.03189712581\n",
      "Batch 126 loss: 51616.737296139494\n",
      "Batch 127 loss: 38498.60506719819\n",
      "Batch 128 loss: 32848.182900755346\n",
      "Batch 129 loss: 34412.494417373025\n",
      "Batch 130 loss: 36811.14020423684\n",
      "Batch 131 loss: 19456.887192108898\n",
      "Batch 132 loss: 23866.572804149673\n",
      "Batch 133 loss: 24900.959369786506\n",
      "Batch 134 loss: 34199.566614933385\n",
      "Batch 135 loss: 40935.899606856576\n",
      "Batch 136 loss: 50452.93573692044\n",
      "Batch 137 loss: 33341.18845699977\n",
      "Batch 138 loss: 54681.11192229109\n",
      "Batch 139 loss: 43140.2124914245\n",
      "Batch 140 loss: 41626.52074633276\n",
      "Batch 141 loss: 29865.986171478635\n",
      "Batch 142 loss: 35588.17064901905\n",
      "Batch 143 loss: 46658.80560774843\n",
      "Batch 144 loss: 35021.27646345609\n",
      "Batch 145 loss: 38425.24082843431\n",
      "Batch 146 loss: 39414.07394211068\n",
      "Batch 147 loss: 37665.82811765258\n",
      "Batch 148 loss: 30066.550839711846\n",
      "Batch 149 loss: 35849.71863781016\n",
      "Batch 150 loss: 36564.40847420774\n",
      "Batch 151 loss: 40062.17765715157\n",
      "Batch 152 loss: 56711.40451301616\n",
      "Batch 153 loss: 23629.961282159344\n",
      "Batch 154 loss: 21739.583483969593\n",
      "Batch 155 loss: 30289.03742574278\n",
      "Batch 156 loss: 29903.175723131866\n",
      "Batch 157 loss: 35176.95790328932\n",
      "Batch 158 loss: 33070.33598025595\n",
      "Batch 159 loss: 38132.839345471926\n",
      "Batch 160 loss: 28097.297555063888\n",
      "Batch 161 loss: 22710.262643020185\n",
      "Batch 162 loss: 43646.45976271309\n",
      "Batch 163 loss: 33616.55384811455\n",
      "Batch 164 loss: 37600.83490682719\n",
      "Batch 165 loss: 47341.260305731266\n",
      "Batch 166 loss: 41089.671963259956\n",
      "Batch 167 loss: 29637.97199807702\n",
      "Batch 168 loss: 33443.66416873525\n",
      "Batch 169 loss: 33233.552508652836\n",
      "Batch 170 loss: 28847.64719000285\n",
      "Batch 171 loss: 29380.908861863893\n",
      "Batch 172 loss: 33256.6706032464\n",
      "Batch 173 loss: 33795.276757626816\n",
      "Batch 174 loss: 50007.03883321097\n",
      "Batch 175 loss: 35659.543109167695\n",
      "Batch 176 loss: 37884.33154919378\n",
      "Batch 177 loss: 40096.088337359804\n",
      "Batch 178 loss: 27502.63527083785\n",
      "Batch 179 loss: 37921.928084258376\n",
      "Batch 180 loss: 41260.86546394446\n",
      "Batch 181 loss: 43050.212389006636\n",
      "Batch 182 loss: 31775.869170734775\n",
      "Batch 183 loss: 45679.15886803431\n",
      "Batch 184 loss: 34949.82821763639\n",
      "Batch 185 loss: 28977.805867218412\n",
      "Batch 186 loss: 35030.435689046695\n",
      "Batch 187 loss: 50366.248885490306\n",
      "Batch 188 loss: 22948.94558517075\n",
      "Batch 189 loss: 38292.48928555107\n",
      "Batch 190 loss: 28923.244383206973\n",
      "Batch 191 loss: 39275.39028428059\n",
      "Batch 192 loss: 39624.52767097431\n",
      "Batch 193 loss: 40479.773203999765\n",
      "Batch 194 loss: 31470.880994865875\n",
      "Batch 195 loss: 26178.316356868374\n",
      "Batch 196 loss: 40733.70457444756\n",
      "Batch 197 loss: 38113.29069267062\n",
      "Batch 198 loss: 46257.64474502165\n",
      "Batch 199 loss: 31750.21249969526\n",
      "Batch 200 loss: 36957.39270753803\n",
      "Batch 201 loss: 45191.25351880696\n",
      "Batch 202 loss: 26676.311142617393\n",
      "Batch 203 loss: 25232.059244279466\n",
      "Batch 204 loss: 42017.6416953215\n",
      "Batch 205 loss: 29883.70291176985\n",
      "Batch 206 loss: 53135.33053119251\n",
      "Batch 207 loss: 41342.30186910735\n",
      "Batch 208 loss: 48599.42403584262\n",
      "Batch 209 loss: 38545.25440823205\n",
      "Batch 210 loss: 30468.33642834757\n",
      "Batch 211 loss: 35748.625988799686\n",
      "Batch 212 loss: 32679.38805984016\n",
      "Batch 213 loss: 46726.06725210068\n",
      "Batch 214 loss: 35853.835756439235\n",
      "Batch 215 loss: 30330.256153470975\n",
      "Batch 216 loss: 57982.38339566907\n",
      "Batch 217 loss: 35025.05451209417\n",
      "Batch 218 loss: 40244.93241857886\n",
      "Batch 219 loss: 35219.78345782951\n",
      "Batch 220 loss: 29857.657260379267\n",
      "Batch 221 loss: 67127.28215429116\n",
      "Batch 222 loss: 48250.01813611628\n",
      "Batch 223 loss: 35098.42022689089\n",
      "Batch 224 loss: 31496.68654737539\n",
      "Batch 225 loss: 38215.18168516368\n",
      "Batch 226 loss: 34984.52143533335\n",
      "Batch 227 loss: 43789.8772896777\n",
      "Batch 228 loss: 49331.168464454975\n",
      "Batch 229 loss: 33419.726673311256\n",
      "Batch 230 loss: 28136.04676441397\n",
      "Batch 231 loss: 40140.85763174379\n",
      "Batch 232 loss: 37766.69741780187\n",
      "Batch 233 loss: 35470.63378776898\n",
      "Batch 234 loss: 23929.775902822574\n",
      "Batch 235 loss: 59149.47199298403\n",
      "Batch 236 loss: 21515.806179136107\n",
      "Batch 237 loss: 49852.013490625104\n",
      "Batch 238 loss: 29293.763177605568\n",
      "Batch 239 loss: 47355.78896885404\n",
      "Batch 240 loss: 30306.361331415064\n",
      "Batch 241 loss: 34644.01817353473\n",
      "Batch 242 loss: 22814.745882368912\n",
      "Batch 243 loss: 48806.69816683431\n",
      "Batch 244 loss: 34348.23444827161\n",
      "Batch 245 loss: 59080.16263605078\n",
      "Batch 246 loss: 38928.55651508428\n",
      "Batch 247 loss: 27440.822850116274\n",
      "Batch 248 loss: 38273.44286025907\n",
      "Batch 249 loss: 67400.92932490198\n",
      "### Epoch 9\n",
      "Batch 0 loss: 61942.28307029269\n",
      "Batch 1 loss: 29020.195518796387\n",
      "Batch 2 loss: 38856.51131238199\n",
      "Batch 3 loss: 48617.001798071775\n",
      "Batch 4 loss: 46718.392632431394\n",
      "Batch 5 loss: 49172.3534459502\n",
      "Batch 6 loss: 36108.62236541422\n",
      "Batch 7 loss: 28643.322652856\n",
      "Batch 8 loss: 45948.50158835712\n",
      "Batch 9 loss: 40631.231812269936\n",
      "Batch 10 loss: 49049.59761070678\n",
      "Batch 11 loss: 36516.92244836372\n",
      "Batch 12 loss: 23309.617085918082\n",
      "Batch 13 loss: 49849.82400279487\n",
      "Batch 14 loss: 36281.73530622599\n",
      "Batch 15 loss: 33873.62801779216\n",
      "Batch 16 loss: 36869.02297448186\n",
      "Batch 17 loss: 35991.433216945996\n",
      "Batch 18 loss: 45495.58452448694\n",
      "Batch 19 loss: 37625.58361160902\n",
      "Batch 20 loss: 27861.62433541978\n",
      "Batch 21 loss: 32515.22774680607\n",
      "Batch 22 loss: 41529.44478783193\n",
      "Batch 23 loss: 40284.3097799673\n",
      "Batch 24 loss: 42755.82681726046\n",
      "Batch 25 loss: 34344.40866888223\n",
      "Batch 26 loss: 56724.232168292685\n",
      "Batch 27 loss: 34327.21372847671\n",
      "Batch 28 loss: 27834.086723844477\n",
      "Batch 29 loss: 34329.12096107038\n",
      "Batch 30 loss: 53136.81721617071\n",
      "Batch 31 loss: 48888.919792108114\n",
      "Batch 32 loss: 23514.906644775594\n",
      "Batch 33 loss: 25869.631314471673\n",
      "Batch 34 loss: 40692.874508689856\n",
      "Batch 35 loss: 41161.89275874491\n",
      "Batch 36 loss: 36839.60777107984\n",
      "Batch 37 loss: 29216.71126808838\n",
      "Batch 38 loss: 20722.657745943936\n",
      "Batch 39 loss: 44169.70930576332\n",
      "Batch 40 loss: 26628.208266361493\n",
      "Batch 41 loss: 32028.15877607202\n",
      "Batch 42 loss: 35001.350608587105\n",
      "Batch 43 loss: 61427.95017464271\n",
      "Batch 44 loss: 36813.47050269964\n",
      "Batch 45 loss: 32886.95587893605\n",
      "Batch 46 loss: 36713.00573861181\n",
      "Batch 47 loss: 32292.076348785195\n",
      "Batch 48 loss: 40112.88171770584\n",
      "Batch 49 loss: 38060.059542120835\n",
      "Batch 50 loss: 29081.10615573873\n",
      "Batch 51 loss: 34530.54311710592\n",
      "Batch 52 loss: 27089.58642633723\n",
      "Batch 53 loss: 33153.47665745758\n",
      "Batch 54 loss: 29318.02975474068\n",
      "Batch 55 loss: 37516.04883382071\n",
      "Batch 56 loss: 31875.41214419196\n",
      "Batch 57 loss: 41922.81317751348\n",
      "Batch 58 loss: 29376.5822014106\n",
      "Batch 59 loss: 22803.880851480946\n",
      "Batch 60 loss: 37956.16733027241\n",
      "Batch 61 loss: 23718.470365768\n",
      "Batch 62 loss: 42409.20961029163\n",
      "Batch 63 loss: 39748.42331785972\n",
      "Batch 64 loss: 35859.95804766777\n",
      "Batch 65 loss: 30641.05246047485\n",
      "Batch 66 loss: 35919.97856224544\n",
      "Batch 67 loss: 39446.59286150981\n",
      "Batch 68 loss: 35107.33693464608\n",
      "Batch 69 loss: 35408.635109387156\n",
      "Batch 70 loss: 51261.10613020602\n",
      "Batch 71 loss: 30410.858317603088\n",
      "Batch 72 loss: 27090.9710822406\n",
      "Batch 73 loss: 34409.92095281168\n",
      "Batch 74 loss: 47075.558017872594\n",
      "Batch 75 loss: 23846.87561032929\n",
      "Batch 76 loss: 31731.235589889053\n",
      "Batch 77 loss: 49327.037227070075\n",
      "Batch 78 loss: 31936.863011672\n",
      "Batch 79 loss: 37610.847741024176\n",
      "Batch 80 loss: 40957.60990620019\n",
      "Batch 81 loss: 50913.70664309299\n",
      "Batch 82 loss: 52381.13555566133\n",
      "Batch 83 loss: 27303.966902604818\n",
      "Batch 84 loss: 27609.083624004033\n",
      "Batch 85 loss: 33757.29135011314\n",
      "Batch 86 loss: 35185.16790597405\n",
      "Batch 87 loss: 46484.51124751135\n",
      "Batch 88 loss: 56468.91180796386\n",
      "Batch 89 loss: 37052.15849323747\n",
      "Batch 90 loss: 47986.117893522875\n",
      "Batch 91 loss: 27877.085282998447\n",
      "Batch 92 loss: 25660.703436871772\n",
      "Batch 93 loss: 40703.94788501591\n",
      "Batch 94 loss: 35549.459639253924\n",
      "Batch 95 loss: 34614.4573797642\n",
      "Batch 96 loss: 35988.397552054776\n",
      "Batch 97 loss: 24812.30295525465\n",
      "Batch 98 loss: 63224.56835025665\n",
      "Batch 99 loss: 27161.801576970793\n",
      "Batch 100 loss: 41282.05323804803\n",
      "Batch 101 loss: 21748.352794064915\n",
      "Batch 102 loss: 34204.33806622909\n",
      "Batch 103 loss: 53297.514252080844\n",
      "Batch 104 loss: 44933.30325022807\n",
      "Batch 105 loss: 48628.62057995665\n",
      "Batch 106 loss: 43423.96327899809\n",
      "Batch 107 loss: 48210.13249406278\n",
      "Batch 108 loss: 37419.4923147681\n",
      "Batch 109 loss: 45787.0756595035\n",
      "Batch 110 loss: 30778.20358178176\n",
      "Batch 111 loss: 27879.386509470787\n",
      "Batch 112 loss: 47528.48414060601\n",
      "Batch 113 loss: 40257.005843484185\n",
      "Batch 114 loss: 38421.258919397624\n",
      "Batch 115 loss: 35805.66379935428\n",
      "Batch 116 loss: 46865.884569675836\n",
      "Batch 117 loss: 69331.14903357069\n",
      "Batch 118 loss: 36830.91591849743\n",
      "Batch 119 loss: 38953.277576262844\n",
      "Batch 120 loss: 37964.552874547444\n",
      "Batch 121 loss: 45746.94931360428\n",
      "Batch 122 loss: 67797.11223962234\n",
      "Batch 123 loss: 29672.29618193403\n",
      "Batch 124 loss: 28095.67594061318\n",
      "Batch 125 loss: 29104.59014729673\n",
      "Batch 126 loss: 40325.56598127751\n",
      "Batch 127 loss: 28970.06566715806\n",
      "Batch 128 loss: 39550.79444919111\n",
      "Batch 129 loss: 24050.53353952674\n",
      "Batch 130 loss: 59496.79256435277\n",
      "Batch 131 loss: 50010.6770341073\n",
      "Batch 132 loss: 33780.411147539104\n",
      "Batch 133 loss: 22063.239524596924\n",
      "Batch 134 loss: 36436.499584249104\n",
      "Batch 135 loss: 57610.528430350125\n",
      "Batch 136 loss: 30087.311495408452\n",
      "Batch 137 loss: 38811.432448537445\n",
      "Batch 138 loss: 26683.569675240255\n",
      "Batch 139 loss: 35458.55452874094\n",
      "Batch 140 loss: 39189.807741515586\n",
      "Batch 141 loss: 36442.017166964055\n",
      "Batch 142 loss: 37375.811761940684\n",
      "Batch 143 loss: 36278.37357634038\n",
      "Batch 144 loss: 22988.284107406336\n",
      "Batch 145 loss: 55821.72514708321\n",
      "Batch 146 loss: 45165.371051192444\n",
      "Batch 147 loss: 35583.66755768524\n",
      "Batch 148 loss: 50209.800573069835\n",
      "Batch 149 loss: 28693.99830949946\n",
      "Batch 150 loss: 36340.408555386064\n",
      "Batch 151 loss: 41760.02742631205\n",
      "Batch 152 loss: 23149.027266698755\n",
      "Batch 153 loss: 35934.536660551355\n",
      "Batch 154 loss: 43651.615277758574\n",
      "Batch 155 loss: 35854.62233565652\n",
      "Batch 156 loss: 43565.11406093898\n",
      "Batch 157 loss: 31104.8355222169\n",
      "Batch 158 loss: 39011.47181989112\n",
      "Batch 159 loss: 42505.813105939844\n",
      "Batch 160 loss: 43159.652172189424\n",
      "Batch 161 loss: 34045.778288314046\n",
      "Batch 162 loss: 36063.88951445536\n",
      "Batch 163 loss: 26275.167301396843\n",
      "Batch 164 loss: 44348.08389829347\n",
      "Batch 165 loss: 48085.383281361246\n",
      "Batch 166 loss: 44473.49299972373\n",
      "Batch 167 loss: 41885.59005124366\n",
      "Batch 168 loss: 30221.862779067535\n",
      "Batch 169 loss: 57792.98410568976\n",
      "Batch 170 loss: 46456.87184306098\n",
      "Batch 171 loss: 59986.630179683016\n",
      "Batch 172 loss: 50311.08758919506\n",
      "Batch 173 loss: 45031.5684676667\n",
      "Batch 174 loss: 36972.403340725716\n",
      "Batch 175 loss: 35876.857312750646\n",
      "Batch 176 loss: 36461.41926487633\n",
      "Batch 177 loss: 36373.53825218752\n",
      "Batch 178 loss: 47626.52164028715\n",
      "Batch 179 loss: 34215.31913631643\n",
      "Batch 180 loss: 36312.68907208399\n",
      "Batch 181 loss: 42068.02313680546\n",
      "Batch 182 loss: 47219.20862687402\n",
      "Batch 183 loss: 25808.892316351677\n",
      "Batch 184 loss: 29355.05938333305\n",
      "Batch 185 loss: 39067.6813802608\n",
      "Batch 186 loss: 35710.19703112287\n",
      "Batch 187 loss: 28933.054470730316\n",
      "Batch 188 loss: 47974.70464664072\n",
      "Batch 189 loss: 32514.40936966711\n",
      "Batch 190 loss: 63829.103910336904\n",
      "Batch 191 loss: 50704.167458316784\n",
      "Batch 192 loss: 28859.57580895208\n",
      "Batch 193 loss: 24767.03108860461\n",
      "Batch 194 loss: 37351.98978970502\n",
      "Batch 195 loss: 37350.35865776597\n",
      "Batch 196 loss: 35426.66242051759\n",
      "Batch 197 loss: 51478.62798829715\n",
      "Batch 198 loss: 26085.92149748051\n",
      "Batch 199 loss: 32426.236152714657\n",
      "Batch 200 loss: 45577.10517773614\n",
      "Batch 201 loss: 35724.57525584477\n",
      "Batch 202 loss: 26486.86615659146\n",
      "Batch 203 loss: 26984.849016719647\n",
      "Batch 204 loss: 36623.192449380396\n",
      "Batch 205 loss: 26960.87116093263\n",
      "Batch 206 loss: 46471.98964324297\n",
      "Batch 207 loss: 35427.51010531522\n",
      "Batch 208 loss: 37264.29202479498\n",
      "Batch 209 loss: 20406.75954514304\n",
      "Batch 210 loss: 41586.692235550996\n",
      "Batch 211 loss: 29001.20060599109\n",
      "Batch 212 loss: 56556.51673149471\n",
      "Batch 213 loss: 39246.08755498913\n",
      "Batch 214 loss: 29776.04558590655\n",
      "Batch 215 loss: 36465.330089127485\n",
      "Batch 216 loss: 44759.12897485208\n",
      "Batch 217 loss: 26333.86297447239\n",
      "Batch 218 loss: 46960.21645751003\n",
      "Batch 219 loss: 30531.01154809773\n",
      "Batch 220 loss: 17848.249720260927\n",
      "Batch 221 loss: 33234.86491086526\n",
      "Batch 222 loss: 28808.951969779093\n",
      "Batch 223 loss: 37366.24516153366\n",
      "Batch 224 loss: 32722.854243536225\n",
      "Batch 225 loss: 36478.758094198936\n",
      "Batch 226 loss: 31038.285908787624\n",
      "Batch 227 loss: 24000.098445826352\n",
      "Batch 228 loss: 46264.24591287902\n",
      "Batch 229 loss: 45389.91771920303\n",
      "Batch 230 loss: 41293.426542516914\n",
      "Batch 231 loss: 38593.25491097808\n",
      "Batch 232 loss: 28562.866643174653\n",
      "Batch 233 loss: 40780.16875974843\n",
      "Batch 234 loss: 27120.37320148618\n",
      "Batch 235 loss: 35169.1911579915\n",
      "Batch 236 loss: 38028.273774475034\n",
      "Batch 237 loss: 34025.31777413448\n",
      "Batch 238 loss: 27361.091539173878\n",
      "Batch 239 loss: 40012.62025763358\n",
      "Batch 240 loss: 40638.326575143205\n",
      "Batch 241 loss: 25823.223193460923\n",
      "Batch 242 loss: 34235.053868788214\n",
      "Batch 243 loss: 38551.038398041914\n",
      "Batch 244 loss: 45366.61389121858\n",
      "Batch 245 loss: 82758.99939874654\n",
      "Batch 246 loss: 39063.05054326844\n",
      "Batch 247 loss: 30148.5408182847\n",
      "Batch 248 loss: 43252.79320242962\n",
      "Batch 249 loss: 52271.581873846066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380638.296480749"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(model, train_dl, optimizer, criterion, epochs=10):\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"### Epoch {epoch}\")\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Batch {i} loss: {loss.item()}\")\n",
    "            \n",
    "    return running_loss / len(train_dl)\n",
    "                            \n",
    "\n",
    "train(model, train_dl, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_test, output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Github\\ode-biomarker-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input_data'"
     ]
    }
   ],
   "source": [
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, output.detach().numpy())\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from torch import nn  # type: ignore\n",
    "import torch.nn.functional as F  # type: ignore\n",
    "\n",
    "class GroupLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, group_feat_size: int):\n",
    "        super(GroupLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(group_feat_size, 1).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "\n",
    "    def __init__(self, group_feat_size: int, total_feat_size: int):\n",
    "\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.group_feat_size = group_feat_size\n",
    "        self.total_feat_size = total_feat_size\n",
    "\n",
    "        num_groups = self.total_feat_size // self.group_feat_size\n",
    "\n",
    "        # if num_groups not an integer, throw error\n",
    "        if num_groups != self.total_feat_size / self.group_feat_size:\n",
    "            raise ValueError(\"Total feature size must be divisible by group feature size\")\n",
    "\n",
    "        self.num_groups = num_groups\n",
    "        self.group_layers = nn.ModuleList()\n",
    "        i = 0\n",
    "        while i < num_groups:\n",
    "            self.group_layers.append(GroupLayer(group_feat_size))\n",
    "            i += 1\n",
    "        self.layer_2_size = int(num_groups / 2)\n",
    "        self.fc1 = nn.Linear(num_groups, self.layer_2_size).double()\n",
    "        self.fc_out = nn.Linear(self.layer_2_size, 1).double()\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # print(input_data.shape)\n",
    "\n",
    "        xs = []\n",
    "        i = 0\n",
    "        while i < self.total_feat_size:\n",
    "            xs.append(input_data[:, i:i+self.group_feat_size])\n",
    "            i += group_feat_size\n",
    "\n",
    "\n",
    "        outs = []\n",
    "        for i, x in enumerate(xs):\n",
    "            # print(i+1, x.shape)\n",
    "            outs.append(self.group_layers[i](x))\n",
    "\n",
    "\n",
    "        x = torch.cat(outs, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    TorchModel,\n",
    "    module__group_feat_size=group_feat_size,\n",
    "    module__total_feat_size=total_feat_size,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    max_epochs=20,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=2000, n_features=260, noise=0.01)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m44220.1780\u001b[0m    \u001b[32m41889.8243\u001b[0m  0.2841\n",
      "      2    \u001b[36m44217.0392\u001b[0m    \u001b[32m41888.1898\u001b[0m  0.2816\n",
      "      3    \u001b[36m44213.2126\u001b[0m    \u001b[32m41885.4407\u001b[0m  0.4046\n",
      "      4    \u001b[36m44207.1179\u001b[0m    \u001b[32m41878.1976\u001b[0m  0.2894\n",
      "      5    \u001b[36m44195.1706\u001b[0m    \u001b[32m41864.8819\u001b[0m  0.3597\n",
      "      6    \u001b[36m44171.7564\u001b[0m    \u001b[32m41831.5821\u001b[0m  0.3213\n",
      "      7    \u001b[36m44114.1985\u001b[0m    \u001b[32m41757.8835\u001b[0m  0.2898\n",
      "      8    \u001b[36m43991.0618\u001b[0m    \u001b[32m41613.0394\u001b[0m  0.2901\n",
      "      9    \u001b[36m43773.5824\u001b[0m    \u001b[32m41377.8202\u001b[0m  0.2817\n",
      "     10    \u001b[36m43432.8481\u001b[0m    \u001b[32m41025.9599\u001b[0m  0.2767\n",
      "     11    \u001b[36m42951.5874\u001b[0m    \u001b[32m40571.1405\u001b[0m  0.2775\n",
      "     12    \u001b[36m42314.6568\u001b[0m    \u001b[32m39983.1273\u001b[0m  0.2840\n",
      "     13    \u001b[36m41505.3027\u001b[0m    \u001b[32m39251.0583\u001b[0m  0.3642\n",
      "     14    \u001b[36m40515.5702\u001b[0m    \u001b[32m38337.4193\u001b[0m  0.2965\n",
      "     15    \u001b[36m39319.2995\u001b[0m    \u001b[32m37272.9808\u001b[0m  0.3049\n",
      "     16    \u001b[36m37918.3252\u001b[0m    \u001b[32m36104.4874\u001b[0m  0.2923\n",
      "     17    \u001b[36m36355.2355\u001b[0m    \u001b[32m34759.9018\u001b[0m  0.2877\n",
      "     18    \u001b[36m34654.7682\u001b[0m    \u001b[32m33326.3728\u001b[0m  0.2810\n",
      "     19    \u001b[36m32792.4481\u001b[0m    \u001b[32m31790.2439\u001b[0m  0.2804\n",
      "     20    \u001b[36m30846.2020\u001b[0m    \u001b[32m30226.4489\u001b[0m  0.3557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=TorchModel(\n",
       "    (group_layers): ModuleList(\n",
       "      (0-25): 26 x GroupLayer(\n",
       "        (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
       "    (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make prediction \n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "\n",
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n",
    "# Bring in CCLE data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in original ccle data\n",
    "loading_code = 'generic-gdsc-1-FGFR_0939-LN_IC50-fgfr4_ccle_dynamic_features-true-Row'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pAkt_auc</th>\n",
       "      <th>pAkt_median</th>\n",
       "      <th>pAkt_tfc</th>\n",
       "      <th>pAkt_tmax</th>\n",
       "      <th>pAkt_max</th>\n",
       "      <th>pAkt_tmin</th>\n",
       "      <th>pAkt_min</th>\n",
       "      <th>pAkt_ttsv</th>\n",
       "      <th>pAkt_tsv</th>\n",
       "      <th>pAkt_init</th>\n",
       "      <th>...</th>\n",
       "      <th>amTORC2_auc</th>\n",
       "      <th>amTORC2_median</th>\n",
       "      <th>amTORC2_tfc</th>\n",
       "      <th>amTORC2_tmax</th>\n",
       "      <th>amTORC2_max</th>\n",
       "      <th>amTORC2_tmin</th>\n",
       "      <th>amTORC2_min</th>\n",
       "      <th>amTORC2_ttsv</th>\n",
       "      <th>amTORC2_tsv</th>\n",
       "      <th>amTORC2_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SIDM01085</th>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.021032</td>\n",
       "      <td>4.682872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032885</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>5.486629</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00949</th>\n",
       "      <td>0.077292</td>\n",
       "      <td>0.084704</td>\n",
       "      <td>9.089875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140803</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.013955</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.013955</td>\n",
       "      <td>0.013955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218305</td>\n",
       "      <td>0.226915</td>\n",
       "      <td>0.364546</td>\n",
       "      <td>1</td>\n",
       "      <td>0.264206</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.193622</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.193622</td>\n",
       "      <td>0.193622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00494</th>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>3.560627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.020458</td>\n",
       "      <td>0.403167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024825</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.017692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00377</th>\n",
       "      <td>0.036978</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>5.138051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065341</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>1.189183</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00988</th>\n",
       "      <td>0.027368</td>\n",
       "      <td>0.030932</td>\n",
       "      <td>8.449997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039991</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.465354</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.008079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00697</th>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>9.576674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.539425</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM01188</th>\n",
       "      <td>0.032034</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>11.164995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056863</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140425</td>\n",
       "      <td>0.146074</td>\n",
       "      <td>0.332654</td>\n",
       "      <td>1</td>\n",
       "      <td>0.167943</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.126022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00999</th>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>5.579325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>4.879114</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00149</th>\n",
       "      <td>0.078582</td>\n",
       "      <td>0.089001</td>\n",
       "      <td>14.430451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135102</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142652</td>\n",
       "      <td>0.147998</td>\n",
       "      <td>0.297572</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170342</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.131277</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.131277</td>\n",
       "      <td>0.131277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00848</th>\n",
       "      <td>0.031927</td>\n",
       "      <td>0.037403</td>\n",
       "      <td>11.575419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053087</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463970</td>\n",
       "      <td>0.483035</td>\n",
       "      <td>0.281069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544116</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.424736</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.424736</td>\n",
       "      <td>0.424736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665 rows  260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pAkt_auc  pAkt_median   pAkt_tfc  pAkt_tmax  pAkt_max  pAkt_tmin  \\\n",
       "SIDM01085  0.019410     0.021032   4.682872        1.0  0.032885       0.04   \n",
       "SIDM00949  0.077292     0.084704   9.089875        1.0  0.140803       0.04   \n",
       "SIDM00494  0.010086     0.011210   3.560627        1.0  0.015136       0.04   \n",
       "SIDM00377  0.036978     0.039602   5.138051        1.0  0.065341       0.04   \n",
       "SIDM00988  0.027368     0.030932   8.449997        1.0  0.039991       0.04   \n",
       "...             ...          ...        ...        ...       ...        ...   \n",
       "SIDM00697  0.013521     0.014763   9.576674        1.0  0.024837       0.04   \n",
       "SIDM01188  0.032034     0.036092  11.164995        1.0  0.056863       0.04   \n",
       "SIDM00999  0.015288     0.016662   5.579325        1.0  0.026040       0.04   \n",
       "SIDM00149  0.078582     0.089001  14.430451        1.0  0.135102       0.04   \n",
       "SIDM00848  0.031927     0.037403  11.575419        1.0  0.053087       0.04   \n",
       "\n",
       "           pAkt_min  pAkt_ttsv  pAkt_tsv  pAkt_init  ...  amTORC2_auc  \\\n",
       "SIDM01085  0.005787       0.04  0.005787   0.005787  ...     0.001717   \n",
       "SIDM00949  0.013955       0.04  0.013955   0.013955  ...     0.218305   \n",
       "SIDM00494  0.003319       0.04  0.003319   0.003319  ...     0.019850   \n",
       "SIDM00377  0.010645       0.04  0.010645   0.010645  ...     0.000361   \n",
       "SIDM00988  0.004232       0.04  0.004232   0.004232  ...     0.008990   \n",
       "...             ...        ...       ...        ...  ...          ...   \n",
       "SIDM00697  0.002348       0.04  0.002348   0.002348  ...     0.000116   \n",
       "SIDM01188  0.004674       0.04  0.004674   0.004674  ...     0.140425   \n",
       "SIDM00999  0.003958       0.04  0.003958   0.003958  ...     0.002991   \n",
       "SIDM00149  0.008756       0.04  0.008756   0.008756  ...     0.142652   \n",
       "SIDM00848  0.004222       0.04  0.004222   0.004222  ...     0.463970   \n",
       "\n",
       "           amTORC2_median  amTORC2_tfc  amTORC2_tmax  amTORC2_max  \\\n",
       "SIDM01085        0.001599     5.486629             1     0.003834   \n",
       "SIDM00949        0.226915     0.364546             1     0.264206   \n",
       "SIDM00494        0.020458     0.403167             1     0.024825   \n",
       "SIDM00377        0.000332     1.189183             1     0.000639   \n",
       "SIDM00988        0.009058     0.465354             1     0.011839   \n",
       "...                   ...          ...           ...          ...   \n",
       "SIDM00697        0.000116     0.539425             1     0.000160   \n",
       "SIDM01188        0.146074     0.332654             1     0.167943   \n",
       "SIDM00999        0.002788     4.879114             1     0.006578   \n",
       "SIDM00149        0.147998     0.297572             1     0.170342   \n",
       "SIDM00848        0.483035     0.281069             1     0.544116   \n",
       "\n",
       "           amTORC2_tmin  amTORC2_min  amTORC2_ttsv  amTORC2_tsv  amTORC2_init  \n",
       "SIDM01085          0.04     0.000591          0.04     0.000591      0.000591  \n",
       "SIDM00949          0.04     0.193622          0.04     0.193622      0.193622  \n",
       "SIDM00494          0.04     0.017692          0.04     0.017692      0.017692  \n",
       "SIDM00377          0.04     0.000292          0.04     0.000292      0.000292  \n",
       "SIDM00988          0.12     0.008078          0.04     0.008079      0.008079  \n",
       "...                 ...          ...           ...          ...           ...  \n",
       "SIDM00697          0.04     0.000104          0.04     0.000104      0.000104  \n",
       "SIDM01188          0.04     0.126022          0.04     0.126022      0.126022  \n",
       "SIDM00999          0.04     0.001119          0.04     0.001119      0.001119  \n",
       "SIDM00149          0.04     0.131277          0.04     0.131277      0.131277  \n",
       "SIDM00848          0.04     0.424736          0.04     0.424736      0.424736  \n",
       "\n",
       "[665 rows x 260 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "feature_data_numpy = feature_data.to_numpy()\n",
    "label_data_numpy = label_data.to_numpy()\n",
    "label_data_numpy = label_data_numpy.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(665, 260)\n",
      "(665, 1)\n"
     ]
    }
   ],
   "source": [
    "print(feature_data_numpy.shape)\n",
    "print(label_data_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on all data test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_feat_size = 10\n",
    "total_feat_size = 260\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    TorchModel,\n",
    "    module__group_feat_size=group_feat_size,\n",
    "    module__total_feat_size=total_feat_size,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    max_epochs=100,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m33052.7457\u001b[0m       \u001b[32m19.8155\u001b[0m  0.1719\n",
      "      2    \u001b[36m16930.9675\u001b[0m        \u001b[32m9.9198\u001b[0m  0.1598\n",
      "      3     \u001b[36m8752.5953\u001b[0m        \u001b[32m5.8865\u001b[0m  0.1471\n",
      "      4     \u001b[36m5340.3912\u001b[0m        \u001b[32m4.2817\u001b[0m  0.1500\n",
      "      5      \u001b[36m684.9815\u001b[0m        \u001b[32m3.6904\u001b[0m  0.1640\n",
      "      6      \u001b[36m269.3790\u001b[0m        \u001b[32m3.2360\u001b[0m  0.2565\n",
      "      7       \u001b[36m24.0988\u001b[0m        \u001b[32m3.0237\u001b[0m  0.1354\n",
      "      8        \u001b[36m2.6645\u001b[0m        \u001b[32m2.8962\u001b[0m  0.1291\n",
      "      9        \u001b[36m2.0203\u001b[0m        \u001b[32m2.7256\u001b[0m  0.1460\n",
      "     10        \u001b[36m1.8913\u001b[0m        \u001b[32m2.6295\u001b[0m  0.1731\n",
      "     11        \u001b[36m1.7945\u001b[0m        \u001b[32m2.5011\u001b[0m  0.2775\n",
      "     12        \u001b[36m1.6956\u001b[0m        \u001b[32m2.3786\u001b[0m  0.1756\n",
      "     13        \u001b[36m1.6144\u001b[0m        \u001b[32m2.2779\u001b[0m  0.2169\n",
      "     14        \u001b[36m1.5475\u001b[0m        \u001b[32m2.2392\u001b[0m  0.1563\n",
      "     15        \u001b[36m1.5174\u001b[0m        \u001b[32m2.1347\u001b[0m  0.1685\n",
      "     16        \u001b[36m1.4592\u001b[0m        \u001b[32m2.0589\u001b[0m  0.2175\n",
      "     17        \u001b[36m1.4140\u001b[0m        \u001b[32m2.0523\u001b[0m  0.1799\n",
      "     18        1.4483        \u001b[32m1.9727\u001b[0m  0.1741\n",
      "     19        1.4626        \u001b[32m1.9675\u001b[0m  0.1386\n",
      "     20        \u001b[36m1.3299\u001b[0m        \u001b[32m1.9004\u001b[0m  0.1303\n",
      "     21        \u001b[36m1.3035\u001b[0m        \u001b[32m1.8954\u001b[0m  0.1356\n",
      "     22        \u001b[36m1.2816\u001b[0m        \u001b[32m1.8496\u001b[0m  0.1595\n",
      "     23        \u001b[36m1.2565\u001b[0m        \u001b[32m1.8094\u001b[0m  0.1505\n",
      "     24        \u001b[36m1.2434\u001b[0m        \u001b[32m1.7820\u001b[0m  0.2022\n",
      "     25        1.2439        \u001b[32m1.7575\u001b[0m  0.1757\n",
      "     26        \u001b[36m1.1985\u001b[0m        \u001b[32m1.7338\u001b[0m  0.1351\n",
      "     27        \u001b[36m1.1773\u001b[0m        \u001b[32m1.7307\u001b[0m  0.1298\n",
      "     28        \u001b[36m1.1746\u001b[0m        \u001b[32m1.7050\u001b[0m  0.1383\n",
      "     29        \u001b[36m1.1417\u001b[0m        \u001b[32m1.6704\u001b[0m  0.1379\n",
      "     30        \u001b[36m1.1371\u001b[0m        \u001b[32m1.6495\u001b[0m  0.1364\n",
      "     31        \u001b[36m1.1047\u001b[0m        \u001b[32m1.6317\u001b[0m  0.1328\n",
      "     32        \u001b[36m1.0976\u001b[0m        \u001b[32m1.6192\u001b[0m  0.1903\n",
      "     33        \u001b[36m1.0882\u001b[0m        \u001b[32m1.6016\u001b[0m  0.2003\n",
      "     34        \u001b[36m1.0753\u001b[0m        \u001b[32m1.5870\u001b[0m  0.1499\n",
      "     35        \u001b[36m1.0619\u001b[0m        \u001b[32m1.5680\u001b[0m  0.1491\n",
      "     36        \u001b[36m1.0529\u001b[0m        \u001b[32m1.5520\u001b[0m  0.1481\n",
      "     37        1.0569        \u001b[32m1.5471\u001b[0m  0.1584\n",
      "     38        1.0621        \u001b[32m1.5439\u001b[0m  0.1475\n",
      "     39        \u001b[36m1.0289\u001b[0m        \u001b[32m1.5297\u001b[0m  0.2233\n",
      "     40        \u001b[36m1.0251\u001b[0m        \u001b[32m1.5108\u001b[0m  0.1494\n",
      "     41        \u001b[36m1.0150\u001b[0m        \u001b[32m1.4982\u001b[0m  0.1274\n",
      "     42        \u001b[36m1.0130\u001b[0m        1.5427  0.2147\n",
      "     43        1.0484        \u001b[32m1.4749\u001b[0m  0.1539\n",
      "     44        \u001b[36m0.9955\u001b[0m        \u001b[32m1.4693\u001b[0m  0.1435\n",
      "     45        1.0127        \u001b[32m1.4594\u001b[0m  0.1452\n",
      "     46        1.0023        \u001b[32m1.4506\u001b[0m  0.1423\n",
      "     47        0.9996        \u001b[32m1.4495\u001b[0m  0.1478\n",
      "     48        \u001b[36m0.9786\u001b[0m        \u001b[32m1.4399\u001b[0m  0.1302\n",
      "     49        \u001b[36m0.9701\u001b[0m        \u001b[32m1.4279\u001b[0m  0.1296\n",
      "     50        \u001b[36m0.9643\u001b[0m        \u001b[32m1.4155\u001b[0m  0.1295\n",
      "     51        0.9721        1.4261  0.1312\n",
      "     52        \u001b[36m0.9633\u001b[0m        \u001b[32m1.4085\u001b[0m  0.1328\n",
      "     53        0.9687        1.4231  0.1283\n",
      "     54        0.9724        \u001b[32m1.4037\u001b[0m  0.1951\n",
      "     55        \u001b[36m0.9533\u001b[0m        \u001b[32m1.3891\u001b[0m  0.1432\n",
      "     56        \u001b[36m0.9506\u001b[0m        1.3918  0.1280\n",
      "     57        0.9534        1.3979  0.1235\n",
      "     58        \u001b[36m0.9482\u001b[0m        \u001b[32m1.3709\u001b[0m  0.1296\n",
      "     59        \u001b[36m0.9326\u001b[0m        1.3795  0.1296\n",
      "     60        0.9331        \u001b[32m1.3628\u001b[0m  0.1263\n",
      "     61        \u001b[36m0.9313\u001b[0m        1.3684  0.1276\n",
      "     62        \u001b[36m0.9247\u001b[0m        \u001b[32m1.3582\u001b[0m  0.1656\n",
      "     63        0.9252        \u001b[32m1.3518\u001b[0m  0.1327\n",
      "     64        0.9356        1.3666  0.1235\n",
      "     65        0.9420        \u001b[32m1.3505\u001b[0m  0.1209\n",
      "     66        \u001b[36m0.9227\u001b[0m        \u001b[32m1.3445\u001b[0m  0.1268\n",
      "     67        0.9256        \u001b[32m1.3377\u001b[0m  0.1546\n",
      "     68        \u001b[36m0.9108\u001b[0m        1.3438  0.1359\n",
      "     69        0.9145        \u001b[32m1.3275\u001b[0m  0.1312\n",
      "     70        0.9111        \u001b[32m1.3175\u001b[0m  0.1894\n",
      "     71        \u001b[36m0.9057\u001b[0m        1.3293  0.2036\n",
      "     72        0.9092        \u001b[32m1.3116\u001b[0m  0.1592\n",
      "     73        0.9248        \u001b[32m1.3113\u001b[0m  0.1280\n",
      "     74        0.9124        \u001b[32m1.3063\u001b[0m  0.1322\n",
      "     75        0.9110        \u001b[32m1.3039\u001b[0m  0.1293\n",
      "     76        \u001b[36m0.8983\u001b[0m        \u001b[32m1.3015\u001b[0m  0.1401\n",
      "     77        0.9035        1.3037  0.1350\n",
      "     78        0.9180        \u001b[32m1.2923\u001b[0m  0.1255\n",
      "     79        \u001b[36m0.8939\u001b[0m        \u001b[32m1.2890\u001b[0m  0.1286\n",
      "     80        \u001b[36m0.8928\u001b[0m        1.2906  0.1450\n",
      "     81        0.8943        \u001b[32m1.2873\u001b[0m  0.1932\n",
      "     82        0.8967        1.3196  0.1535\n",
      "     83        0.9099        \u001b[32m1.2809\u001b[0m  0.1742\n",
      "     84        \u001b[36m0.8885\u001b[0m        1.2832  0.1471\n",
      "     85        \u001b[36m0.8824\u001b[0m        1.2848  0.1868\n",
      "     86        0.8877        \u001b[32m1.2747\u001b[0m  0.1499\n",
      "     87        0.8882        \u001b[32m1.2718\u001b[0m  0.1441\n",
      "     88        0.8936        1.2748  0.1372\n",
      "     89        \u001b[36m0.8799\u001b[0m        1.2789  0.1416\n",
      "     90        0.8920        \u001b[32m1.2646\u001b[0m  0.1897\n",
      "     91        0.8904        1.2652  0.1289\n",
      "     92        0.8894        1.2708  0.1281\n",
      "     93        0.8884        \u001b[32m1.2582\u001b[0m  0.1251\n",
      "     94        0.9071        1.2939  0.1287\n",
      "     95        0.9182        1.2642  0.1277\n",
      "     96        0.9275        1.2700  0.1283\n",
      "     97        1.0061        \u001b[32m1.2576\u001b[0m  0.1386\n",
      "     98        1.2072        1.2627  0.1247\n",
      "     99        1.6082        \u001b[32m1.2563\u001b[0m  0.1398\n",
      "    100        2.2836        1.2588  0.1364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=TorchModel(\n",
       "    (group_layers): ModuleList(\n",
       "      (0-25): 26 x GroupLayer(\n",
       "        (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=26, out_features=13, bias=True)\n",
       "    (fc_out): Linear(in_features=13, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(feature_data_numpy, label_data_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAGJCAYAAACAUygaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7g0lEQVR4nO3deVgT1/4G8DdBCMgmoggqKm5VRK1KrUp/ahU31LrUWrdbtXazuFCvrdpbt7qgrbW21qvW3uJKte6t63VvccXrUqk7F5eqVNzYxLBkfn/Y5BqSkEkyIcnwfp6H54HJZPLNkLw5OXPmjEIQBAFERCQ7SkcXQERE9sGAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCfJ1apVC8OGDdP9ffDgQSgUChw8eNBhNRVXvEYqHc74WpAzBrzMLF++HAqFQvfj6emJ+vXrY9SoUfjzzz8dXZ5FduzYgWnTpjm6DLto37693v/J1I8jn3+TJk1Qo0YNlDSbSVRUFKpUqYLCwsJSrIzEKufoAsg+Pv30U4SFheHJkydISkrC4sWLsWPHDqSkpKB8+fKlWkvbtm2Rl5cHDw8Pi+63Y8cOLFq0SJYh/49//ANvvfWW7u/k5GR8/fXX+Pjjj9GwYUPd8iZNmjiiPADA4MGDMXHiRPz6669o27atwe3Xrl3D0aNHMWrUKJQrxyhxRvyvyFS3bt0QGRkJAHjrrbcQGBiI+fPnY+vWrRg4cKDR++Tm5sLb21vyWpRKJTw9PSXfrivr1KmT3t+enp74+uuv0alTJ7Rv397k/ez1PzJm0KBBmDRpEhITE40G/A8//ABBEDB48OBSqYcsxy6aMqJDhw4AgLS0NADAsGHD4OPjg9TUVMTExMDX11f3RtVoNFiwYAEaNWoET09PVKlSBe+++y4ePnyot01BEDBz5kxUr14d5cuXx8svv4zff//d4LFN9bseP34cMTExCAgIgLe3N5o0aYKvvvpKV9+iRYsAQK/LQkvqGosrKChAxYoVMXz4cIPbsrKy4OnpifHjx+uWLVy4EI0aNUL58uUREBCAyMhIJCYmmn2ckkybNg0KhQLnz5/HoEGDEBAQgJdeegnA0y4eYx8Ew4YNQ61atfSWid1XxYWGhqJt27bYsGEDCgoKDG5PTExEnTp18OKLL+L69et4//338dxzz8HLywuBgYF47bXXcO3aNbPP09TxEGPPUa1WY+rUqahbty5UKhVCQ0Px0UcfQa1Wm32csogt+DIiNTUVABAYGKhbVlhYiC5duuCll17CvHnzdF037777LpYvX47hw4djzJgxSEtLwzfffIPTp0/j8OHDcHd3BwBMmTIFM2fORExMDGJiYnDq1Cl07twZ+fn5ZuvZs2cPevTogZCQEIwdOxbBwcG4cOECtm3bhrFjx+Ldd9/F7du3sWfPHqxatcrg/vau0d3dHX369MGmTZuwdOlSve6lLVu2QK1WY8CAAQCAZcuWYcyYMejXrx/Gjh2LJ0+e4LfffsPx48cxaNAgs/vCnNdeew316tXD7NmzS+wPN0XsvjJm8ODBeOedd7B792706NFDt/zcuXNISUnBlClTADztYjpy5AgGDBiA6tWr49q1a1i8eDHat2+P8+fPS9ItqNFo8MorryApKQnvvPMOGjZsiHPnzuHLL7/E5cuXsWXLFpsfQ3YEkpWEhAQBgLB3714hIyNDuHnzprB27VohMDBQ8PLyEv744w9BEARh6NChAgBh4sSJevf/9ddfBQDCmjVr9Jbv2rVLb/ndu3cFDw8PoXv37oJGo9Gt9/HHHwsAhKFDh+qWHThwQAAgHDhwQBAEQSgsLBTCwsKEmjVrCg8fPtR7nGe3FRsbKxh7idqjRmN2794tABB+/vlnveUxMTFC7dq1dX/36tVLaNSoUYnbMmf9+vV6+0gQBGHq1KkCAGHgwIEG67dr105o166dwfKhQ4cKNWvW1P0tdl+Z8uDBA0GlUhnUMHHiRAGAcOnSJUEQBOHx48cG9z169KgAQFi5cqVuWfHXgiAIQs2aNY3+L4o/x1WrVglKpVL49ddf9dZbsmSJAEA4fPhwic+lLGIXjUxFR0ejcuXKCA0NxYABA+Dj44PNmzejWrVqeuuNHDlS7+/169fD398fnTp1wr1793Q/LVq0gI+PDw4cOAAA2Lt3L/Lz8zF69Gi9rpO4uDiztZ0+fRppaWmIi4tDhQoV9G57dlumlEaNwNNurUqVKmHdunW6ZQ8fPsSePXvw+uuv65ZVqFABf/zxB5KTk0Vt11Lvvfee1fcVu69MCQgIQExMDH766Sfk5uYCeNrttXbtWkRGRqJ+/foAAC8vL919CgoKcP/+fdStWxcVKlTAqVOnrK6/+HNp2LAhGjRooPdctN2P5p5LWcQuGplatGgR6tevj3LlyqFKlSp47rnnoFTqf56XK1cO1atX11t25coVZGZmIigoyOh27969CwC4fv06AKBevXp6t1euXBkBAQEl1qbtLoqIiBD/hEq5RuDp/nn11VeRmJgItVoNlUqFTZs2oaCgQC/gJ0yYgL1796Jly5aoW7cuOnfujEGDBiEqKsqq51dcWFiY1fcVu69KMnjwYGzevBlbt27FoEGDcOTIEVy7dg1jx47VrZOXl4f4+HgkJCTg1q1bel1JmZmZVtf/rCtXruDChQuoXLmy0dvFPJeyhgEvUy1bttSNojFFpVIZhL5Go0FQUBDWrFlj9D6m3lylqTRrHDBgAJYuXYqdO3eid+/e+PHHH9GgQQM0bdpUt07Dhg1x6dIlbNu2Dbt27cLGjRvxz3/+E1OmTMH06dNtruHZ1rGWQqEw2h9fVFSk97cU+6pHjx7w9/dHYmIiBg0ahMTERLi5uemOQQDA6NGjkZCQgLi4OLRu3Rr+/v5QKBQYMGAANBpNids39a2tqKgIbm5ues+lcePGmD9/vtH1Q0NDzT6XsoYBT3rq1KmDvXv3IioqymiwaNWsWRPA01ZV7dq1dcszMjLMjs6oU6cOACAlJQXR0dEm1zP1xi+NGrXatm2LkJAQrFu3Di+99BL279+Pf/zjHwbreXt74/XXX8frr7+O/Px89O3bF7NmzcKkSZPsMkQ0ICAA//3vfw2Wa7+1aIndVyVRqVTo168fVq5ciT///BPr169Hhw4dEBwcrFtnw4YNGDp0KL744gvdsidPnuDRo0einoux9a5fv673f6tTpw7Onj2Ljh07iurKIw6TpGL69++PoqIizJgxw+C2wsJC3RsxOjoa7u7uWLhwoV5LcsGCBWYfo3nz5ggLC8OCBQsM3tjPbks73rv4OqVRo5ZSqUS/fv3w888/Y9WqVSgsLNTrngGA+/fv6/3t4eGB8PBwCIJgdHihFOrUqYOLFy8iIyNDt+zs2bM4fPiw3npi95U5gwcPRkFBAd59911kZGQYjH13c3Mz+EaxcOFCg28Upp7LsWPH9EY2bdu2DTdv3jR4Lrdu3cKyZcsMtpGXl6c7RkD/wxY86WnXrh3effddxMfH48yZM+jcuTPc3d1x5coVrF+/Hl999RX69euHypUrY/z48YiPj0ePHj0QExOD06dPY+fOnahUqVKJj6FUKrF48WL07NkTzz//PIYPH46QkBBcvHgRv//+O3bv3g0AaNGiBQBgzJgx6NKli65boDRqfNbrr7+OhQsXYurUqWjcuLHemaYA0LlzZwQHB+tO279w4QK++eYbdO/eHb6+vhb+B8R58803MX/+fHTp0gUjRozA3bt3sWTJEjRq1AhZWVm69cTuK3PatWuH6tWrY+vWrfDy8kLfvn31bu/RowdWrVoFf39/hIeH4+jRo9i7d6/esFxT3nrrLWzYsAFdu3ZF//79kZqaitWrV+u+6Wn97W9/w48//oj33nsPBw4cQFRUFIqKinDx4kX8+OOP2L17t9luyTLHcQN4yB60wySTk5NLXG/o0KGCt7e3ydu//fZboUWLFoKXl5fg6+srNG7cWPjoo4+E27dv69YpKioSpk+fLoSEhAheXl5C+/bthZSUFINhb8aGxgmCICQlJQmdOnUSfH19BW9vb6FJkybCwoULdbcXFhYKo0ePFipXriwoFAqDIZNS1lgSjUYjhIaGCgCEmTNnGty+dOlSoW3btkJgYKCgUqmEOnXqCB9++KGQmZkpavuCUPIwyYyMDKP3Wb16tVC7dm3Bw8NDeP7554Xdu3cbDJPUErOvzPnwww8FAEL//v0Nbnv48KEwfPhwoVKlSoKPj4/QpUsX4eLFi6JfC1988YVQrVo1QaVSCVFRUcLJkyeNDgXNz88X5s6dKzRq1EhQqVRCQECA0KJFC2H69OkW7e+yQiEIVpw5QURETo998EREMsWAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimZL9iU4ajQa3b9+Gr68vT28mIlkQBAHZ2dmoWrWqwXxSz5J9wN++fZuTEBGRLN28edNgRthnyT7gtaeK37x5E35+fg6uhojIdllZWQgNDTU7FYbsA17bLePn58eAJyJZMdftzIOsREQyxYAnIpIpBjwRkUwx4ImIZIoBT0QkUwx4IiKZkv0wSaKyrkgj4ETaA9zNfoIgX0+0DKsINyXP6i4LGPBEMrYr5Q6m/3wedzKf6JaF+Htias9wdI0IcWBlVBrYRUMkU7tS7mDk6lN64Q4A6ZlPMHL1KexKueOgyqi0MOCJZKhII2D6z+dh7ILL2mXTfz6PIg0vySxnDHgiGTqR9sCg5f4sAcCdzCc4kfag9IqiUseAJ5Khu9mmw92a9cg1MeCJZCjI11PS9cg1MeCJZKhlWEWE+HvC1GBIBZ6OpmkZVrE0y6JSxoAnkiE3pQJTe4YDgEHIa/+e2jOc4+FljgFPJFNdI0KweEhzBPvrd8ME+3ti8ZDmHAdfBvBEJyIZ6xoRgk7hwTyTtYxiwBPJnJtSgdZ1Ah1dBjkAu2iIiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCcikikGPBGRTDHgiYhkigFPRCRTDHgiIpliwBMRyRQDnohIphjwREQyxYAnIpIpBjwRkUwx4ImIZIoBT0QkUwx4IiKZYsATEckUA56ISKYY8EREMsWAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimXJowP/yyy/o2bMnqlatCoVCgS1btujdLggCpkyZgpCQEHh5eSE6OhpXrlxxTLFERC7GoQGfm5uLpk2bYtGiRUZv/+yzz/D1119jyZIlOH78OLy9vdGlSxc8efKklCslInI95Rz54N26dUO3bt2M3iYIAhYsWIBPPvkEvXr1AgCsXLkSVapUwZYtWzBgwIDSLJWIyOU4bR98Wloa0tPTER0drVvm7++PF198EUePHjV5P7VajaysLL0fIqKyyGkDPj09HQBQpUoVveVVqlTR3WZMfHw8/P39dT+hoaF2rZOIyFk5bcBba9KkScjMzNT93Lx509ElERE5hNMGfHBwMADgzz//1Fv+559/6m4zRqVSwc/PT++HiKgsctqADwsLQ3BwMPbt26dblpWVhePHj6N169YOrIyIyDU4dBRNTk4Orl69qvs7LS0NZ86cQcWKFVGjRg3ExcVh5syZqFevHsLCwjB58mRUrVoVvXv3dlzRREQuwqEBf/LkSbz88su6v8eNGwcAGDp0KJYvX46PPvoIubm5eOedd/Do0SO89NJL2LVrFzw9PR1VMhGRy1AIgiA4ugh7ysrKgr+/PzIzM9kfT0SyIDbXnLYPnoiIbMOAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCcikikGPBGRTDHgiYhkigFPRCRTDHgiIpliwBMRyRQDnohIphjwREQyxYAnIpIpBjwRkUwx4ImIZIoBT0QkUwx4IiKZYsATEckUA56ISKYY8EREMsWAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCcikikGPBGRTDHgiYhkigFPRCRTDHgiIpliwBMRyRQDnohIphjwREQyxYAnIpIpqwM+Pz8fly5dQmFhoZT1EBGRRCwO+MePH2PEiBEoX748GjVqhBs3bgAARo8ejTlz5khaXFFRESZPnoywsDB4eXmhTp06mDFjBgRBkPRxiIjkyOKAnzRpEs6ePYuDBw/C09NTtzw6Ohrr1q2TtLi5c+di8eLF+Oabb3DhwgXMnTsXn332GRYuXCjp4xARyVE5S++wZcsWrFu3Dq1atYJCodAtb9SoEVJTUyUt7siRI+jVqxe6d+8OAKhVqxZ++OEHnDhxQtLHISKSI4tb8BkZGQgKCjJYnpubqxf4UmjTpg327duHy5cvAwDOnj2LpKQkdOvWzeR91Go1srKy9H6IiMoiiwM+MjIS27dv1/2tDfXvvvsOrVu3lq4yABMnTsSAAQPQoEEDuLu7o1mzZoiLi8PgwYNN3ic+Ph7+/v66n9DQUElrIiJyFRZ30cyePRvdunXD+fPnUVhYiK+++grnz5/HkSNHcOjQIUmL+/HHH7FmzRokJiaiUaNGOHPmDOLi4lC1alUMHTrU6H0mTZqEcePG6f7OyspiyBNRmaQQrBiSkpqaijlz5uDs2bPIyclB8+bNMWHCBDRu3FjS4kJDQzFx4kTExsbqls2cOROrV6/GxYsXRW0jKysL/v7+yMzMhJ+fn6T1ERE5gthcs7gFDwB16tTBsmXLrC5OrMePH0Op1O9FcnNzg0ajsftjExG5OosDXjvu3ZQaNWpYXUxxPXv2xKxZs1CjRg00atQIp0+fxvz58/Hmm29K9hhERHJlcReNUqkscbRMUVGRzUVpZWdnY/Lkydi8eTPu3r2LqlWrYuDAgZgyZQo8PDxEbYNdNEQkN2JzzeKAP3v2rN7fBQUFupb1rFmz0LdvX+sqthMGPBHJjd364Js2bWqwLDIyElWrVsXnn3/udAFPRFRWSTab5HPPPYfk5GSpNkdERDayuAVf/MxQQRBw584dTJs2DfXq1ZOsMCIiso3FAV+hQgWDg6yCICA0NBRr166VrDAiIrKNxQF/4MABvb+VSiUqV66MunXrolw5q4bVExGRHVicyO3atbNHHUREJDFRAf/TTz+J3uArr7xidTFERCQdUQHfu3dvURtTKBSSnuhERETWExXwnPuFiMj1SDYOnoiInItVw15yc3Nx6NAh3LhxA/n5+Xq3jRkzRpLCiIjINhYH/OnTpxETE4PHjx8jNzcXFStWxL1791C+fHkEBQUx4ImInITFXTQffPABevbsiYcPH8LLywvHjh3D9evX0aJFC8ybN88eNRIRkRUsDvgzZ87g73//O5RKJdzc3KBWqxEaGorPPvsMH3/8sT1qJCIiK1gc8O7u7rqrLAUFBekuAOLv74+bN29KWx0REVnN4j74Zs2aITk5GfXq1UO7du0wZcoU3Lt3D6tWrUJERIQ9aiQiIiuIbsFrT2CaPXs2QkJCAACzZs1CQEAARo4ciYyMDHz77bf2qZKIiCwmugVfrVo1DBs2DG+++SYiIyMBPO2i2bVrl92KIyIi64luwcfGxmLDhg1o2LAh/u///g/Lly/H48eP7VkbERHZQHTAT548GVevXsW+fftQu3ZtjBo1CiEhIXj77bdx/Phxe9ZIRERWsHgUTfv27bFixQqkp6fjiy++wIULF9C6dWs0atQI8+fPt0eNRERkBYUgCIKtG9m+fTveeOMNPHr0yOlmkxR79XEiIlchNtesnmzs8ePHWL58Odq1a4dXXnkFgYGBmDVrlrWbIyIiiVk8Dv7IkSP4/vvvsX79ehQWFqJfv36YMWMG2rZta4/6iIjISqID/rPPPkNCQgIuX76MyMhIfP755xg4cCB8fX3tWR8REVlJdMB//vnnGDJkCNavX88zVomIXIDogL99+zbc3d3tWQsREUlI9EFWhjsRkWvhJfuIiGSKAU9EJFMMeCIimRJ1kDUrK0v0Bnm2KBGRcxAV8BUqVIBCoRC1QWebqoCIqKwSFfAHDhzQ/X7t2jVMnDgRw4YNQ+vWrQEAR48exYoVKxAfH2+fKomIyGIWTzbWsWNHvPXWWxg4cKDe8sTERHz77bc4ePCglPXZjJONEZHc2G2ysaNHj+qu6PSsyMhInDhxwtLNERGRnVgc8KGhoVi2bJnB8u+++w6hoaGSFEVERLazeDbJL7/8Eq+++ip27tyJF198EQBw4sQJXLlyBRs3bpS8QCIiso7FLfiYmBhcvnwZPXv2xIMHD/DgwQP07NkTly9fRkxMjD1qJCIiK0hyRSdnxoOsRCQ3dr2i06+//oohQ4agTZs2uHXrFgBg1apVSEpKsq5aIiKSnMUBv3HjRnTp0gVeXl44deoU1Go1ACAzMxOzZ8+WvEAiIrKOxQE/c+ZMLFmyBMuWLdObQjgqKgqnTp2StDgAuHXrFoYMGYLAwEB4eXmhcePGOHnypOSPQ0QkNxaPorl06ZLR66/6+/vj0aNHUtSk8/DhQ0RFReHll1/Gzp07UblyZVy5cgUBAQGSPg4RkRxZHPDBwcG4evUqatWqpbc8KSkJtWvXlqouAMDcuXMRGhqKhIQE3bKwsDBJH4OISK4s7qJ5++23MXbsWBw/fhwKhQK3b9/GmjVrMH78eIwcOVLS4n766SdERkbitddeQ1BQEJo1a2b0JKtnqdVqZGVl6f0QEZVJgoU0Go0wc+ZMwdvbW1AoFIJCoRA8PT2FTz75xNJNmaVSqQSVSiVMmjRJOHXqlLB06VLB09NTWL58ucn7TJ06VQBg8JOZmSl5fUREjpCZmSkq16weB5+fn4+rV68iJycH4eHh8PHxke5T5y8eHh6IjIzEkSNHdMvGjBmD5ORkHD161Oh91Gq1bmQP8HS8aGhoKMfBE5Fs2G0c/Jtvvons7Gx4eHggPDwcLVu2hI+PD3Jzc/Hmm2/aVHRxISEhCA8P11vWsGFD3Lhxw+R9VCoV/Pz89H6IiMoiiwN+xYoVyMvLM1iel5eHlStXSlKUVlRUFC5duqS37PLly6hZs6akj0NEJEeiR9FkZWVBEAQIgoDs7Gx4enrqbisqKsKOHTsQFBQkaXEffPAB2rRpg9mzZ6N///44ceIEvv32W3z77beSPg4RkRyJDnjtZfsUCgXq169vcLtCocD06dMlLe6FF17A5s2bMWnSJHz66acICwvDggULMHjwYEkfh4hIjkQfZD106BAEQUCHDh2wceNGVKxYUXebh4cHatasiapVq9qtUGtxsjEikhuxuSa6Bd+uXTsAQFpaGmrUqCH6ItxEROQYFh9k3b9/PzZs2GCwfP369VixYoUkRRERke0sDvj4+HhUqlTJYHlQUBBnkyQiciIWB/yNGzeMzgdTs2bNEsenExFR6bI44IOCgvDbb78ZLD979iwCAwMlKYqIiGxnccAPHDgQY8aMwYEDB1BUVISioiLs378fY8eOxYABA+xRIxERWcHi6YJnzJiBa9euoWPHjihX7undNRoN3njjDfbBExE5EasnG7t8+TLOnj2ru8qSs04fwHHwRCQ3ko+DL65+/fpGz2glIiLnICrgx40bhxkzZsDb2xvjxo0rcd358+dLUhgREdlGVMCfPn0aBQUFut9N4dmtRETOw+o+eFfBPngikhu7XfCDiIhcg6gumr59+4re4KZNm6wuhoiIpCOqBe/v76/78fPzw759+3Dy5End7f/5z3+wb98++Pv7261QIiKyjKgWfEJCgu73CRMmoH///liyZAnc3NwAPL2i0/vvv88+biIiJ2LxQdbKlSsjKSkJzz33nN7yS5cuoU2bNrh//76kBdqKB1mJSG7sdpC1sLAQFy9eNFh+8eJFaDQaSzdHRER2YvGZrMOHD8eIESOQmpqKli1bAgCOHz+OOXPmYPjw4ZIXSERE1rE44OfNm4fg4GB88cUXuHPnDgAgJCQEH374If7+979LXiAREVnHphOdsrKyAMCp+7bZB09EcmPXE50KCwuxd+9e/PDDD7rpCW7fvo2cnBzrqiUiIslZ3EVz/fp1dO3aFTdu3IBarUanTp3g6+uLuXPnQq1WY8mSJfaok4iILGRxC37s2LGIjIzEw4cP4eXlpVvep08f7Nu3T9LiiIjIeha34H/99VccOXIEHh4eestr1aqFW7duSVYYERHZxuIWvEajQVFRkcHyP/74A76+vpIURUREtrM44Dt37owFCxbo/lYoFMjJycHUqVMRExMjZW1ERGQDi4dJ3rx5E127doUgCLhy5QoiIyNx5coVVKpUCb/88guCgoLsVatVOEySiORGbK5ZNQ6+sLAQ69atw9mzZ5GTk4PmzZtj8ODBegddnQUDnojkxi4BX1BQgAYNGmDbtm1o2LChJIXaGwOeiOTGLic6ubu748mTJzYXR0RE9mfxQdbY2FjMnTsXhYWF9qiHiIgkYvE4+OTkZOzbtw///ve/0bhxY3h7e+vdzkv2ERE5B4sDvkKFCnj11VftUQsREUnI4oB/9vJ9RETkvET3wWs0GsydOxdRUVF44YUXMHHiROTl5dmzNiIisoHogJ81axY+/vhj+Pj4oFq1avjqq68QGxtrz9qIiMgGogN+5cqV+Oc//4ndu3djy5Yt+Pnnn7FmzRpeh5WIyEmJDvgbN27ozTUTHR0NhUKB27dv26UwIiKyjeiALywshKenp94yd3d3FBQUSF4UERHZTvQoGkEQMGzYMKhUKt2yJ0+e4L333tMbC89x8EREzkF0wA8dOtRg2ZAhQyQthoiIpCM64Dn+nYjItVg8F40jzZkzBwqFAnFxcY4uhYjI6blMwCcnJ2Pp0qVo0qSJo0shInIJLhHwOTk5GDx4MJYtW4aAgABHl0NE5BJcIuBjY2PRvXt3REdHm11XrVYjKytL74eIqCyyeLKx0rZ27VqcOnUKycnJotaPj4/H9OnT7VwVEZHzc+oW/M2bNzF27FisWbPG4CQrUyZNmoTMzEzdz82bN+1cJRGRc7LqotulZcuWLejTpw/c3Nx0y4qKiqBQKKBUKqFWq/VuM4bXZCUiuRGba07dRdOxY0ecO3dOb9nw4cPRoEEDTJgwwWy4ExGVZU4d8L6+voiIiNBb5u3tjcDAQIPlRESkz6n74ImIyHpO3YI35uDBg44ugYjIJbAFT0QkUwx4IiKZYsATEckUA56ISKYY8EREMsWAJyKSKQY8EZFMMeCJiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCcikikGPBGRTLncfPBE5FyKNAJOpD3A3ewnCPL1RMuwinBTKhxdFoEBT0Q22JVyB9N/Po87mU90y0L8PTG1Zzi6RoQ4sDIC2EVDLq5II+Bo6n1sPXMLR1Pvo0gjOLqkMmNXyh2MXH1KL9wBID3zCUauPoVdKXccVBlpsQVPLoutR8cp0giY/vN5GPs4FQAoAEz/+Tw6hQezu8aB2IInl8TWo2OdSHtgsO+fJQC4k/kEJ9IelF5RZIABTy7HXOsReNp6ZHeN/dzNNh3u1qxH9sGAJ5fD1qPjBfl6SrqeMTy+Yjv2wZPLYevR8VqGVUSIvyfSM58Y/SalABDs/3TIpDV4fEUabMGTyymN1qM1ylKL002pwNSe4QCehvmztH9rb7d0n/D4inTYgieXY+/WozXKYouza0QIFg9pbvC8g/963gDw0tz9Fu0Tjs6RlkIQBPk2MwBkZWXB398fmZmZ8PPzc3Q5JBFtKw+AXhho3/KLhzQvtWDV1lL8jWSsFjme9WnsOe05ny56nzzraOp9DFx2zOxj/vB2K7SuE2h78S5KbK6xBU8uyVzrsbTC3ZIW557z6aJa+a72IeCmVOiFrZhRTv/YnIIODarAo5xSd58TaQ+wU2T3C4+viMOAJ5fVNSIEncKDHRqGYkf0fLP/KhbsvWwQetp+ZW2L1lhXT0Vvd8zsFYGYJlXt8ySeIcWHi7l9AgD3c/PRKn4vZvdpDAAGz9mc0j6+4qoY8OTSircegdJtAYttSSYcTjPbytdoBMQmnjZY70FuAd5PPI13/3iESTHhBtuQ6vlKdRxB7D55kFuA9/7qZhPLEcdXXBkDnmSltA92im1JPsorMHmbtpX/ydYUox8CWkt/SUN+oYDOjYJ1IS7V8zV1HKH4Nwwx7NW6fnZ0jrkPMFfr5rIXHmQll6d9M+85n47vD18zuN2eB16LNAJemrvf5Igeewnx98QrTUPw7S/GvxkAQLeIYNSp7I3WtSuhVZ1AkwGnfQ6muki0reakCR0AwGxw7vjtNt5PPG3tUzMpoLw74vs2Nvs/3JVyB9N+Oo/0rGeOzfh5YtorJY/ecaUPBLG5xoAnl2asBWvMsyEl9RvX1Igee1JY+FgVyrtjjolwPHzlHgb/67jZbfRrXg2/XrmHP7PVumUB5d3R+/mqqFrB6+m3FAFIPHEDDx+b/sZiiyVmPqR3pdwpsdvH2P2NH/fw+Ou4h3MOcWXA/4UBL1+muhVKUnx4nT37r53RkiHN0aFBFaw6eg1p93Nx4/5jJF97gLwCjaNLEyXYT4XDEzsCMPwmAQAtZu7BoxI+XALKu+PkJ510/2Nzr6F324YZPe7haBwmSQ5TGl93SxqKV5JnDwBa039t6rl1jQiBRiNgwqbfkP2kyJqnVCpGJ55CkQC46km26VlqjF17Gv+5/tCgxT3kxRolhjsAPHxcgGP/vY+oupVEvYaW/pKGptUrlMoIJntgwJOkSusgp5iheMZoDwBaelCxSCPgm/1XkXA4Te+AqfarvFIJoyNgnI2LNNRLtO03w7HyD3Lz8fX+q6LufzT1acCLfQ19tPE3dIkIceo+eVMY8DLliINGUo7E0NafnvUED3LUqOjtgWB/L93zsOZEl0BvD7SoGSDqRJyJm87BV+WOVnUCsed8OiZuOme0dfggNx/vJ55CeQ+l04c7PXX1bhYA8cM5c9RFeq1+qd5XpfEeZcA7iD3/uY6YF0XKOURK6s/WPg9rhuLdz81Hu88PYMALNcy23B49LsDgfx2Hv1c5ZOYVmt3243wZNI3LiF2/38WulDuo5K0SfZ/5/76E1ceu49h/7+sdQLb2fVVa71EeZLWCreFsz39uSQeNFLDfHC1SzSEi5sCpAsCiQc0wY/uFUh+eSPJQobw7PMu56Q2ltIY1Q3AtmbvIFLG5xumCLbQr5Q5emrsfA5cdw9i1ZzBw2TG8NHe/6ClM7TkVqrmDRgLsd6UjKeZoF3vgVADw6bbzmNzd+UY3kGt49LjA5nAHLL+CWGlfjYwBbwFbw9ne/1wxB43sdaUjsV93S1rPkgOn6Vlq7Ei5Ay8PN1HrE9mLJVcQK+2rkTHgjTB24QYpwtne/9z0zDxJ1zPF6IUtxPZQlbCepQdOt/12B4/znXdIIpUtYl6/pX01Mh5kLcZU/7i5A3PPhrOpPmZ7/3Mf5OZLup4xpvZPt4hgUfe/l6M2eRtnCCRXJub1W9pXI2PAP6OkYX5f7r0sahslhbO9/7kVfcR1k4hdr/jB5Ie5aqNjvdMznxidA8aYkp6buSs1ETkjS2a4LO2rkTl1wMfHx2PTpk24ePEivLy80KZNG8ydOxfPPfec5I8lpgtGDFsCTOw/19QonmA/cR8MYtYz1lJXKozvC+0wSIXC9BmSYp6b9jqfIy2cQpbIkQSIm+ES0H+NF59PyJLZMsVy6j74Q4cOITY2FseOHcOePXtQUFCAzp07Izc3V/LHsvbMSC0FnnZViAkw7frF7w+Y/+eWNIpH+wFSEnM1ah/D2MHkko79Cs/cbqx6AcDk7uZfuNorNZl7HkTO4s2oWhYNPda+xoOLvcaD/T0lH8bsUuPgMzIyEBQUhEOHDqFt27ai7iN2vOjWM7cwdu0ZUds09ckr9p9j7Th4MeNnAZQ4s+E/BzUrcV4Nc1PHmjMiqhZ2pKSXeJKSmH1UpBFwLPU+3k88hcwS5lInshd3NwUKiszHo7XXh7XlfBpZjoPPzMwEAFSsaLoFqlarkZWVpfcjhth+7w+i69n8yds1IgRJEzrgh7db4asBz+OHt1shaUKHEu8vdhRPp/Bgo60DrRnbL5Q4nNPWbzLR4cGY3L2h0dssGevvplQgql4lzH218dPuH6srIrJMeQ83xHWsi4rl3c2uW9HbA+mZef8bTWYB7dXIej1fDa1LmK/fFk7dB/8sjUaDuLg4REVFISIiwuR68fHxmD59usXbF9s/PqpDPYzqUM/maQaMXWquJJYMsXw6syHwfqJhX7a5eWGsHcGj3T8tagag3ecHTNYIAB9vPoe8Ag2C/czvO1MX1w709sB9G0YDEZni6a5EixoVsWCf+cnLHuTm44MfzwKw/3Qg1nCZLpqRI0di586dSEpKQvXq1U2up1aroVb/byheVlYWQkNDRU1VYOrCDfa8IpBYYruQvhrwPHo0qSr6Cj3Fw1XslAPFtwc83T/+Xh4W3V/smyK/UIMVR9KQfO0hvD3cEOznicW//NeiOonE8lG5IUdt2TkWpZkTsuqiGTVqFLZt24YDBw6UGO4AoFKp4Ofnp/cjVmke/LCUJUMsbTmhSvtNxpLvI8/uH0u/AYjpttmVcgctZ+3FrB0X8e/zf2LzmdsMd7IrS8MdsM9UA7Zy6i4aQRAwevRobN68GQcPHkRYWJjdH7NrRAg6hQc73fUZLRliue2326K2aSyMLRmq+GZULXQKD9bbP5aO4Tc306S5S7ARORMxJzyWJqduwcfGxmL16tVITEyEr68v0tPTkZ6ejrw82061N6c0Dn5YU5PYIZa2nlDVNSIEiwY1Q0lPWwFgZ0q6wYefNd8ATH2jKNIImLDxNwu2RGReabybpZpqwFZOHfCLFy9GZmYm2rdvj5CQEN3PunXrHF2aQ4jtQjIXsmLG7Ad4q8yOezcWyiV9EJlT/E1x7L/3Rc3FTmSJ0ug8cZZpN5y+i4b0ielCkuJsOVvmzTE18sWc4m+Ko6n3Rd+XyFLF3xtSbVPKqQZs5dQBT8aJGWJpKmSDRY5akaKbR/tBlJ6ZhxnbL+Bhbr6FUzTwA57sRwDg61kO2U+k+ZZoj6kGbMWAlzFbDhhLMW/Osx9EXh5uFn+jaF27Er45kGq2ViJrNa8RgEOXMyxqzSvw9IpQqnJKpGf9b0i22MZTaWLAOzEprttq6QlVz95PykmRrPlG0apOICqUdzd6sWsiKRy6nAHg6UR5z/YIB5R3x8PHBSZf+/F9GzvlaLviXOZEJ2vZ45qspcERF84ujTos/dBy5DBJbw835PKCImXKiKhaiP5r6O+e8+lO8R40RmyuMeCdkBQX5ZWSFN8kbHmMXSl3MGHjbwYjasq7K/G4QFPi12trDqRpu5+a16iA7efSLbw3uSpjZ3iXxmvfGmJzjV00TsbcpGIlnRRkL9Z284gl5ltClpHhknkFGgCAf7FuHO19AWDaT+cturiydo/2aBKCZb+mWfpUyIlo+8o1giBquK2xk5Ts/dq3Nwa8k7FkmgFXfuFplXQVrZGrT2HRoGaYsf1CiR94Xu5uWDSiOe7lqg1aWb4qdwz+13HR9QT7e6JHk2D8K6n0wj2uY11kPynEv0q4KpaqnBLqQk2p1eTqtB/Us3o3xqfbfrfofApnOUlJCgx4O7Lm611pX5TXkcR8W/lkawoe5Jo+yKr9wFMqFej1fDWD2+/lmr4G7LPeaF0T3SJC8DBXjfcTT4u6j1RWHL0ODzfTrwsFnh70i6xZEdvOmZ9q2d78vcphSKuaWOREI5yUxa4mpj147+/loTfSRQxnOUlJCgx4O7H24GRpX5TXkcR8Wykp3J9l6gNP7H7qFhGClmEV8dLc/aLWl9JDM6OEBADpWWoMblUTtSqVd/jQ0bmvNoG/l4dTBfxXA5rhbtYTXH/wGDUrlsffWteCRzkltp65JXobznaSkhQY8HZgrtuhpIOkpX1RXkeS8luIqSC3ZH/aerETe9tzPh3lPdwk2572uU/uHo5Pt5k/VhHsp8K0Vxqha0QIijSCU1wgvUJ5d7weWR2zd1zQ+999l5SGqT3DLW4IOdNJSlJgwEvM1oOkpX1RXkcS++ar6O1hxVmwT1myP5292+v7w9fQLSJY0m1qv1F2ifjfmO5K3ipAAdzNVuNBjhoVvT0Q7O+l18UoZr++0zYM6/9zCw8kuDCLdly6VgUvdwyPCkO9IG/EJp4u4RhOc1EfRM4y/FFqHCYpMbEXzDB3HUdnGQdvT9rrv5prXU/uHo7YRNsuxCJmf1pzsZPSpB0VYq5L59n1Tb25pXotmduv+YUatIrfK7qrrTjta+DQhy/jP9cf6h3PAiDqwjaTuzdE7F/HVYztjw+in16lzZUaTRwm6SBSHSR11nnppSS2dd01IgSLldbPqwOI25/a7hxn7aYR8LS/3kdVDjlq06NCAsq7Y1bvCMzYfsHgMoe9nq9qMIe/LcztV49ySszu09hol6U5z74GPMopDRpER1PvixpxFuCtMnoWtdwaTMYw4CUm5UFSVx+DK4bYKQyk+MAztz+1HzhSnzkb4u+JzLwCPJborNj+kdXxfQlDKuP7NkbXiBBEhwdj1dFrBgcebWVsdFhJ+7VrRAjeaRuGpb9YNvTU3Ae4JY2pXs9Xk32DyRgGvMTK0kFSqYgN79L4wOsaEYIlQ5pj4qZzNs2BU9HbHX2er6Z32rtUHxzaFnjxk7iebZEa6zrRHni0pcVqTddhkUbAT2fND+/UfoszdqUwYyxtTJWFBlNx7IO3A2e+eDeJU6QR8M3+q0g4nIZHeYZBH/LXsYEAbw+9A5P3cgxPttL6au8VfLn3stU1FT+VPr9QY7SFbq+pLqzdrthjG4HeHpjVJ0J0bWKP4Ri7uLyrYx+8A9k6Fzs5nptSgbHR9TCqQ92nc9pnPTE5okSsWpXKW11P8RE/plrok7uHY8Z26ae6sGV0mNiulE+6N7TovVGWRpxZiwFvJ2XhIGlZIOXXerFdCv2aV0fS1QyTc42XdJ7F+4kldwNZO9WFLVNoiH3ewf5eouvRYmOqZAx4OyqLfX5kmtjjM3P7NQEAo40Dcy1psSwd82/L6DB7H5diY8o0BjxRKbG0S8FY40Cqs20tPcPTltFhpdGVwsaUcbaPmSIi0bRdCsH++kEY7O8p6uCnrWfbKvD0ALGlrWVtK9xUBJvbrq3Pm6zDFjxRKbOlS8GSlreUrWUpWuHsSil9HCZJ5ELET+/Q0OBMVinO3CwLU2i4Al6y7y8MeJIbsedZ2Otyc856GbuyhAH/FwY8yRFb0mUbT3QikjH2Z5MYDHgiF8WhgWQOh0kSEckUA56ISKYY8EREMsWAJyKSKQY8EZFMMeCJiGRK9sMktedxZWVlObgSIiJpaPPM3Hmqsg/47OxsAEBoaKiDKyEiklZ2djb8/f1N3i77qQo0Gg1u374NX19fKBTSnOWXlZWF0NBQ3Lx5k9MfSID7U1rcn9Jyxv0pCAKys7NRtWpVKJWme9pl34JXKpWoXr26Xbbt5+fnNP9wOeD+lBb3p7ScbX+W1HLX4kFWIiKZYsATEckUA94KKpUKU6dOhUqlcnQpssD9KS3uT2m58v6U/UFWIqKyii14IiKZYsATEckUA56ISKYY8EREMsWAt8G1a9cwYsQIhIWFwcvLC3Xq1MHUqVORn5/v6NJcyqJFi1CrVi14enrixRdfxIkTJxxdksuJj4/HCy+8AF9fXwQFBaF37964dOmSo8uSjTlz5kChUCAuLs7RpViEAW+DixcvQqPRYOnSpfj999/x5ZdfYsmSJfj4448dXZrLWLduHcaNG4epU6fi1KlTaNq0Kbp06YK7d+86ujSXcujQIcTGxuLYsWPYs2cPCgoK0LlzZ+Tm5jq6NJeXnJyMpUuXokmTJo4uxXICSeqzzz4TwsLCHF2Gy2jZsqUQGxur+7uoqEioWrWqEB8f78CqXN/du3cFAMKhQ4ccXYpLy87OFurVqyfs2bNHaNeunTB27FhHl2QRtuAllpmZiYoVKzq6DJeQn5+P//znP4iOjtYtUyqViI6OxtGjRx1YmevLzMwEAL4WbRQbG4vu3bvrvUZdiewnGytNV69excKFCzFv3jxHl+IS7t27h6KiIlSpUkVveZUqVXDx4kUHVeX6NBoN4uLiEBUVhYiICEeX47LWrl2LU6dOITk52dGlWI0teCMmTpwIhUJR4k/xALp16xa6du2K1157DW+//baDKid62upMSUnB2rVrHV2Ky7p58ybGjh2LNWvWwNPT09HlWI1TFRiRkZGB+/fvl7hO7dq14eHhAQC4ffs22rdvj1atWmH58uUlzs9M/5Ofn4/y5ctjw4YN6N27t2750KFD8ejRI2zdutVxxbmoUaNGYevWrfjll18QFhbm6HJc1pYtW9CnTx+4ubnplhUVFUGhUECpVEKtVuvd5qzYRWNE5cqVUblyZVHr3rp1Cy+//DJatGiBhIQEhrsFPDw80KJFC+zbt08X8BqNBvv27cOoUaMcW5yLEQQBo0ePxubNm3Hw4EGGu406duyIc+fO6S0bPnw4GjRogAkTJrhEuAMMeJvcunUL7du3R82aNTFv3jxkZGTobgsODnZgZa5j3LhxGDp0KCIjI9GyZUssWLAAubm5GD58uKNLcymxsbFITEzE1q1b4evri/T0dABPLwrh5eXl4Opcj6+vr8HxC29vbwQGBrrUcQ0GvA327NmDq1ev4urVqwZXjWLPlzivv/46MjIyMGXKFKSnp+P555/Hrl27DA68UskWL14MAGjfvr3e8oSEBAwbNqz0CyKnwD54IiKZYocxEZFMMeCJiGSKAU9EJFMMeCIimWLAExHJFAOeiEimGPBERDLFgCcikikGPJETqVWrFhYsWODoMkgmGPDk0sxN6zxt2rRSqaNx48Z47733jN62atUqqFQq3Lt3r1RqIdJiwJNLu3Pnju5nwYIF8PPz01s2fvx43bqCIKCwsNAudYwYMQJr165FXl6ewW0JCQl45ZVXUKlSJbs8NpEpDHhyacHBwboff39/KBQK3d8XL16Er68vdu7ciRYtWkClUiEpKQnDhg3Tm38eAOLi4vQm6tJoNIiPj0dYWBi8vLzQtGlTbNiwwWQdQ4YMQV5eHjZu3Ki3PC0tDQcPHsSIESOQmpqKXr16oUqVKvDx8cELL7yAvXv3mtzmtWvXoFAocObMGd2yR48eQaFQ4ODBg7plKSkp6NatG3x8fFClShX87W9/47cFAsCApzJg4sSJmDNnDi5cuIAmTZqIuk98fDxWrlyJJUuW4Pfff8cHH3yAIUOG4NChQ0bXr1SpEnr16oXvv/9eb/ny5ctRvXp1dO7cGTk5OYiJicG+fftw+vRpdO3aFT179sSNGzesfm6PHj1Chw4d0KxZM5w8eRK7du3Cn3/+if79+1u9TZIPThdMsvfpp5+iU6dOotdXq9WYPXs29u7di9atWwN4egWvpKQkLF26FO3atTN6vxEjRqBbt25IS0tDWFgYBEHAihUrMHToUCiVSjRt2hRNmzbVrT9jxgxs3rwZP/30k9UXOPnmm2/QrFkzzJ49W7fs+++/R2hoKC5fvoz69etbtV2SB7bgSfYiIyMtWv/q1at4/PgxOnXqBB8fH93PypUrkZqaavJ+nTp1QvXq1ZGQkAAA2LdvH27cuKG7eElOTg7Gjx+Phg0bokKFCvDx8cGFCxdsasGfPXsWBw4c0KuzQYMGAFBirVQ2sAVPsuft7a33t1KpNLggS0FBge73nJwcAMD27dtRrVo1vfVUKpXJx1EqlRg2bBhWrFiBadOmISEhAS+//DJq164NABg/fjz27NmDefPmoW7duvDy8kK/fv2Qn59vcnuA/sVjnq1TW2vPnj0xd+5cg/uHhISYrJXKBgY8lTmVK1dGSkqK3rIzZ87A3d0dABAeHg6VSoUbN26Y7I4xZfjw4Zg5cyY2bdqEzZs347vvvtPddvjwYQwbNgx9+vQB8DScr127VmKdwNORQs2aNdPV+azmzZtj48aNqFWrFsqV49uZ9LGLhsqcDh064OTJk1i5ciWuXLmCqVOn6gW+r68vxo8fjw8++AArVqxAamoqTp06hYULF2LFihUlbjssLAwdOnTAO++8A5VKhb59++puq1evHjZt2oQzZ87g7NmzGDRoEDQajclteXl5oVWrVroDxIcOHcInn3yit05sbCwePHiAgQMHIjk5Gampqdi9ezeGDx+OoqIiK/cQyQUDnsqcLl26YPLkyfjoo4/wwgsvIDs7G2+88YbeOjNmzMDkyZMRHx+Phg0bomvXrti+fTvCwsLMbn/EiBF4+PAhBg0aBE9PT93y+fPnIyAgAG3atEHPnj3RpUsXNG/evMRtff/99ygsLESLFi0QFxeHmTNn6t1etWpVHD58GEVFRejcuTMaN26MuLg4VKhQQdfFQ2UXr8lKRCRT/IgnIpIpBjwRkUwx4ImIZIoBT0QkUwx4IiKZYsATEckUA56ISKYY8EREMsWAJyKSKQY8EZFMMeCJiGTq/wHBr+h1rDT4TQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make prediction \n",
    "\n",
    "y_pred = net.predict(feature_data_numpy)\n",
    "\n",
    "# plot the output against the target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(label_data_numpy, y_pred)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Value')\n",
    "# plt.xlim(-2, 4)\n",
    "# plt.ylim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation: 0.08787286256209878\n"
     ]
    }
   ],
   "source": [
    "# calculate pearson correlation\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr, _ = pearsonr(label_data_numpy.flatten(), y_pred.flatten())\n",
    "\n",
    "print(f'Pearsons correlation: {corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with Powerkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from toolkit import *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit = Powerkit(feature_data, label_data)\n",
    "rngs = list(range(10))\n",
    "\n",
    "def pipeline_func(X_train, y_train, rng, **kwargs):\n",
    "    group_feat_size = 10\n",
    "    total_feat_size = 260\n",
    "\n",
    "    net = NeuralNetRegressor(\n",
    "        TorchModel,\n",
    "        module__group_feat_size=group_feat_size,\n",
    "        module__total_feat_size=total_feat_size,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=torch.nn.MSELoss,\n",
    "        max_epochs=20,\n",
    "        lr=0.001,\n",
    "        batch_size=32,\n",
    "        iterator_train__shuffle=True\n",
    "    )\n",
    "    \n",
    "    x_train_numpy = X_train.to_numpy()\n",
    "    y_train_numpy = y_train.to_numpy()\n",
    "    y_train_numpy = y_train_numpy.reshape(-1, 1)\n",
    "    net.fit(x_train_numpy, y_train_numpy)\n",
    "    return {'model': net}\n",
    "\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    \n",
    "    # preprocess x and y \n",
    "    x_test_numpy = X_test.to_numpy()\n",
    "    y_test_numpy = y_test.to_numpy()\n",
    "    y_test_numpy = y_test_numpy.reshape(-1, 1)\n",
    "    \n",
    "    net = pipeline_components['model']\n",
    "    y_pred = net.predict(x_test_numpy)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test_numpy.flatten(), y_pred.flatten())\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit.add_condition('pytorch', False, pipeline_func, {}, eval_func, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.0793\u001b[0m   \u001b[32m647511.4390\u001b[0m  0.1363\n",
      "      2        \u001b[36m2.5949\u001b[0m   \u001b[32m394832.1879\u001b[0m  0.1009\n",
      "      3        \u001b[36m1.5954\u001b[0m   \u001b[32m297385.5756\u001b[0m  0.1120\n",
      "      4        \u001b[36m1.2479\u001b[0m   \u001b[32m244040.5935\u001b[0m  0.0982\n",
      "      5        \u001b[36m1.1231\u001b[0m   \u001b[32m227604.9333\u001b[0m  0.1022\n",
      "      6        \u001b[36m1.0434\u001b[0m   \u001b[32m209151.3628\u001b[0m  0.0990\n",
      "      7        \u001b[36m1.0122\u001b[0m   232951.1218  0.1009\n",
      "      8        \u001b[36m0.9804\u001b[0m   249600.1023  0.0977\n",
      "      9        \u001b[36m0.9694\u001b[0m   268425.7829  0.0994\n",
      "     10        0.9696   286477.7092  0.1145\n",
      "     11        \u001b[36m0.9602\u001b[0m   347677.9697  0.0981\n",
      "     12        \u001b[36m0.9372\u001b[0m   387715.2953  0.1003\n",
      "     13        \u001b[36m0.9246\u001b[0m   402857.7601  0.1032\n",
      "     14        0.9338   422894.7785  0.0990\n",
      "     15        \u001b[36m0.9087\u001b[0m   423071.4449  0.1004\n",
      "     16        \u001b[36m0.9039\u001b[0m   455659.7200  0.1031\n",
      "     17        0.9159   436020.9033  0.1034\n",
      "     18        0.9039   437892.3736  0.0993\n",
      "     19        0.9087   421148.2441  0.0996\n",
      "     20        \u001b[36m0.8908\u001b[0m   403154.4567  0.1053\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m101.9074\u001b[0m    \u001b[32m82432.9131\u001b[0m  0.1323\n",
      "      2       \u001b[36m20.6798\u001b[0m   144802.3877  0.1069\n",
      "      3        \u001b[36m2.7266\u001b[0m   187426.9676  0.0996\n",
      "      4        \u001b[36m1.6456\u001b[0m   219476.3554  0.0978\n",
      "      5        \u001b[36m1.6347\u001b[0m   241444.1281  0.1002\n",
      "      6        \u001b[36m1.4501\u001b[0m   301547.3072  0.1011\n",
      "      7        \u001b[36m1.3560\u001b[0m   319724.6884  0.1081\n",
      "      8        \u001b[36m1.3260\u001b[0m   382348.2514  0.1023\n",
      "      9        \u001b[36m1.3052\u001b[0m   475659.3246  0.1028\n",
      "     10        \u001b[36m1.2802\u001b[0m   506627.1766  0.1001\n",
      "     11        \u001b[36m1.2520\u001b[0m   632112.6436  0.1027\n",
      "     12        \u001b[36m1.2301\u001b[0m   691083.6660  0.1016\n",
      "     13        \u001b[36m1.2109\u001b[0m   791633.2078  0.1006\n",
      "     14        \u001b[36m1.1983\u001b[0m   893208.7705  0.1018\n",
      "     15        1.2031   916835.9735  0.1022\n",
      "     16        \u001b[36m1.1624\u001b[0m  1076925.5925  0.1054\n",
      "     17        \u001b[36m1.1464\u001b[0m  1104966.7178  0.1012\n",
      "     18        \u001b[36m1.1270\u001b[0m  1227521.9495  0.1046\n",
      "     19        \u001b[36m1.1092\u001b[0m  1263135.9496  0.1065\n",
      "     20        \u001b[36m1.1035\u001b[0m  1340247.4880  0.1065\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2909.5264\u001b[0m        \u001b[32m4.3922\u001b[0m  0.1457\n",
      "      2     \u001b[36m1737.0530\u001b[0m        \u001b[32m3.6718\u001b[0m  0.1335\n",
      "      3     \u001b[36m1217.6139\u001b[0m        3.6907  0.1096\n",
      "      4      \u001b[36m881.2726\u001b[0m        \u001b[32m3.4086\u001b[0m  0.1013\n",
      "      5      \u001b[36m432.6077\u001b[0m        \u001b[32m3.0522\u001b[0m  0.1019\n",
      "      6      \u001b[36m298.9951\u001b[0m        \u001b[32m2.8653\u001b[0m  0.1027\n",
      "      7      \u001b[36m173.6802\u001b[0m        \u001b[32m2.6701\u001b[0m  0.1037\n",
      "      8      \u001b[36m142.4941\u001b[0m        \u001b[32m2.5204\u001b[0m  0.1022\n",
      "      9       \u001b[36m66.3191\u001b[0m        \u001b[32m2.3668\u001b[0m  0.1018\n",
      "     10       \u001b[36m30.4552\u001b[0m        \u001b[32m2.2633\u001b[0m  0.1098\n",
      "     11       \u001b[36m17.5481\u001b[0m        \u001b[32m2.1814\u001b[0m  0.1182\n",
      "     12       \u001b[36m11.1103\u001b[0m        \u001b[32m2.1074\u001b[0m  0.1216\n",
      "     13        \u001b[36m5.3679\u001b[0m        \u001b[32m2.0555\u001b[0m  0.1274\n",
      "     14        \u001b[36m3.2146\u001b[0m        \u001b[32m2.0005\u001b[0m  0.1063\n",
      "     15        \u001b[36m2.2790\u001b[0m        \u001b[32m1.9682\u001b[0m  0.0978\n",
      "     16        \u001b[36m1.4367\u001b[0m        \u001b[32m1.9352\u001b[0m  0.1044\n",
      "     17        \u001b[36m1.3980\u001b[0m        \u001b[32m1.8870\u001b[0m  0.1020\n",
      "     18        \u001b[36m1.0641\u001b[0m        \u001b[32m1.8774\u001b[0m  0.0989\n",
      "     19        \u001b[36m1.0148\u001b[0m        \u001b[32m1.8449\u001b[0m  0.1001\n",
      "     20        \u001b[36m1.0022\u001b[0m        \u001b[32m1.8273\u001b[0m  0.0992\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2924\u001b[0m        \u001b[32m3.4116\u001b[0m  0.1210\n",
      "      2       46.3212        \u001b[32m1.9511\u001b[0m  0.1360\n",
      "      3       16.9065        \u001b[32m1.5535\u001b[0m  0.1153\n",
      "      4        \u001b[36m4.0570\u001b[0m        \u001b[32m1.3200\u001b[0m  0.1342\n",
      "      5        \u001b[36m2.0915\u001b[0m        \u001b[32m1.2262\u001b[0m  0.1355\n",
      "      6        \u001b[36m1.6355\u001b[0m        \u001b[32m1.1684\u001b[0m  0.1371\n",
      "      7        \u001b[36m1.5393\u001b[0m        \u001b[32m1.1233\u001b[0m  0.1288\n",
      "      8        \u001b[36m1.3415\u001b[0m        \u001b[32m1.0937\u001b[0m  0.1062\n",
      "      9        \u001b[36m1.2796\u001b[0m        \u001b[32m1.0653\u001b[0m  0.1038\n",
      "     10        \u001b[36m1.2326\u001b[0m        \u001b[32m1.0398\u001b[0m  0.1068\n",
      "     11        \u001b[36m1.2008\u001b[0m        \u001b[32m1.0248\u001b[0m  0.1170\n",
      "     12        \u001b[36m1.1751\u001b[0m        1.0336  0.1055\n",
      "     13        \u001b[36m1.1543\u001b[0m        \u001b[32m1.0103\u001b[0m  0.1068\n",
      "     14        \u001b[36m1.1340\u001b[0m        \u001b[32m0.9945\u001b[0m  0.1078\n",
      "     15        \u001b[36m1.1175\u001b[0m        \u001b[32m0.9781\u001b[0m  0.1042\n",
      "     16        \u001b[36m1.1078\u001b[0m        \u001b[32m0.9724\u001b[0m  0.1057\n",
      "     17        \u001b[36m1.0929\u001b[0m        0.9737  0.1041\n",
      "     18        \u001b[36m1.0913\u001b[0m        \u001b[32m0.9717\u001b[0m  0.1026\n",
      "     19    15772.4292        1.4623  0.1046\n",
      "     20        1.3875        1.3831  0.1282\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m128660.2626\u001b[0m        \u001b[32m3.1031\u001b[0m  0.1498\n",
      "      2    \u001b[36m91245.5397\u001b[0m        \u001b[32m1.8979\u001b[0m  0.1099\n",
      "      3    \u001b[36m74740.4113\u001b[0m        1.9267  0.1030\n",
      "      4    \u001b[36m57156.8632\u001b[0m        1.9423  0.1092\n",
      "      5    \u001b[36m46735.4906\u001b[0m        1.9831  0.1048\n",
      "      6    \u001b[36m34068.6186\u001b[0m        \u001b[32m1.8220\u001b[0m  0.1063\n",
      "      7    \u001b[36m27531.8134\u001b[0m        \u001b[32m1.7524\u001b[0m  0.1030\n",
      "      8    \u001b[36m21507.0456\u001b[0m        1.8378  0.1070\n",
      "      9    \u001b[36m15799.1677\u001b[0m        1.7534  0.1111\n",
      "     10    \u001b[36m13188.4536\u001b[0m        \u001b[32m1.7175\u001b[0m  0.1020\n",
      "     11    \u001b[36m10069.1205\u001b[0m        \u001b[32m1.6476\u001b[0m  0.1038\n",
      "     12     \u001b[36m6582.8415\u001b[0m        \u001b[32m1.4954\u001b[0m  0.1058\n",
      "     13     \u001b[36m6226.3992\u001b[0m        \u001b[32m1.4430\u001b[0m  0.1040\n",
      "     14     \u001b[36m3646.2965\u001b[0m        \u001b[32m1.3590\u001b[0m  0.1232\n",
      "     15     \u001b[36m2791.0946\u001b[0m        \u001b[32m1.3106\u001b[0m  0.1038\n",
      "     16     \u001b[36m2051.7641\u001b[0m        \u001b[32m1.2694\u001b[0m  0.1024\n",
      "     17     \u001b[36m1867.5894\u001b[0m        1.3183  0.1054\n",
      "     18    11021.7840        1.5757  0.1041\n",
      "     19      \u001b[36m334.5501\u001b[0m        1.4802  0.1040\n",
      "     20      \u001b[36m155.9684\u001b[0m        1.3954  0.1152\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m103.2693\u001b[0m       \u001b[32m26.0828\u001b[0m  0.1590\n",
      "      2        \u001b[36m7.8132\u001b[0m        \u001b[32m1.4744\u001b[0m  0.1162\n",
      "      3        \u001b[36m2.5007\u001b[0m        2.7758  0.1022\n",
      "      4        \u001b[36m2.4088\u001b[0m        \u001b[32m1.4162\u001b[0m  0.1037\n",
      "      5        \u001b[36m1.5898\u001b[0m        \u001b[32m1.1458\u001b[0m  0.1030\n",
      "      6        \u001b[36m1.4717\u001b[0m        1.1517  0.1107\n",
      "      7        \u001b[36m1.4171\u001b[0m        \u001b[32m1.1002\u001b[0m  0.1089\n",
      "      8        \u001b[36m1.3745\u001b[0m        \u001b[32m1.0623\u001b[0m  0.1037\n",
      "      9        \u001b[36m1.3407\u001b[0m        \u001b[32m1.0394\u001b[0m  0.1029\n",
      "     10        \u001b[36m1.3094\u001b[0m        \u001b[32m1.0232\u001b[0m  0.1039\n",
      "     11        \u001b[36m1.2855\u001b[0m        \u001b[32m1.0195\u001b[0m  0.1045\n",
      "     12        \u001b[36m1.2604\u001b[0m        \u001b[32m1.0081\u001b[0m  0.1046\n",
      "     13        \u001b[36m1.2476\u001b[0m        \u001b[32m1.0014\u001b[0m  0.1013\n",
      "     14        \u001b[36m1.2239\u001b[0m        \u001b[32m0.9974\u001b[0m  0.1017\n",
      "     15        \u001b[36m1.2060\u001b[0m        \u001b[32m0.9898\u001b[0m  0.1068\n",
      "     16        \u001b[36m1.1885\u001b[0m        \u001b[32m0.9845\u001b[0m  0.1046\n",
      "     17        \u001b[36m1.1754\u001b[0m        \u001b[32m0.9828\u001b[0m  0.1043\n",
      "     18        \u001b[36m1.1614\u001b[0m        \u001b[32m0.9803\u001b[0m  0.1044\n",
      "     19        \u001b[36m1.1461\u001b[0m        \u001b[32m0.9749\u001b[0m  0.1031\n",
      "     20        \u001b[36m1.1326\u001b[0m        \u001b[32m0.9624\u001b[0m  0.1048\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m10.8293\u001b[0m        \u001b[32m7.4420\u001b[0m  0.1916\n",
      "      2        \u001b[36m3.8263\u001b[0m        \u001b[32m3.1928\u001b[0m  0.1217\n",
      "      3        \u001b[36m2.1285\u001b[0m        \u001b[32m2.1760\u001b[0m  0.1065\n",
      "      4        \u001b[36m1.6871\u001b[0m        \u001b[32m1.7360\u001b[0m  0.1088\n",
      "      5        \u001b[36m1.4206\u001b[0m        \u001b[32m1.4755\u001b[0m  0.1137\n",
      "      6        \u001b[36m1.2772\u001b[0m        \u001b[32m1.3709\u001b[0m  0.1079\n",
      "      7        \u001b[36m1.1800\u001b[0m        \u001b[32m1.2680\u001b[0m  0.1088\n",
      "      8        \u001b[36m1.1169\u001b[0m        1.2868  0.1038\n",
      "      9        \u001b[36m1.0741\u001b[0m        \u001b[32m1.1556\u001b[0m  0.1065\n",
      "     10        \u001b[36m1.0448\u001b[0m        1.2211  0.1075\n",
      "     11        \u001b[36m1.0255\u001b[0m        \u001b[32m1.1391\u001b[0m  0.1052\n",
      "     12        \u001b[36m1.0138\u001b[0m        1.1768  0.1065\n",
      "     13        \u001b[36m0.9976\u001b[0m        \u001b[32m1.1105\u001b[0m  0.1070\n",
      "     14        1.0013        \u001b[32m1.1095\u001b[0m  0.1055\n",
      "     15        \u001b[36m0.9932\u001b[0m        1.1445  0.1072\n",
      "     16        \u001b[36m0.9798\u001b[0m        1.1403  0.1056\n",
      "     17        \u001b[36m0.9761\u001b[0m        \u001b[32m1.1025\u001b[0m  0.1057\n",
      "     18        \u001b[36m0.9733\u001b[0m        \u001b[32m1.1003\u001b[0m  0.1028\n",
      "     19        \u001b[36m0.9648\u001b[0m        1.1204  0.1057\n",
      "     20        \u001b[36m0.9626\u001b[0m        1.1094  0.1063\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m22.1258\u001b[0m       \u001b[32m10.2211\u001b[0m  0.1650\n",
      "      2        \u001b[36m7.6523\u001b[0m        \u001b[32m4.6398\u001b[0m  0.1290\n",
      "      3        \u001b[36m3.5371\u001b[0m        \u001b[32m2.7953\u001b[0m  0.1094\n",
      "      4        \u001b[36m2.0314\u001b[0m        \u001b[32m2.0253\u001b[0m  0.1077\n",
      "      5        \u001b[36m1.4678\u001b[0m        \u001b[32m1.7139\u001b[0m  0.1048\n",
      "      6        \u001b[36m1.2658\u001b[0m        \u001b[32m1.5527\u001b[0m  0.1052\n",
      "      7        \u001b[36m1.1688\u001b[0m        \u001b[32m1.4579\u001b[0m  0.1046\n",
      "      8        1.1735        1.5476  0.1042\n",
      "      9        \u001b[36m1.1294\u001b[0m        \u001b[32m1.4219\u001b[0m  0.1051\n",
      "     10        \u001b[36m1.0800\u001b[0m        \u001b[32m1.2895\u001b[0m  0.1178\n",
      "     11        \u001b[36m1.0523\u001b[0m        \u001b[32m1.2609\u001b[0m  0.1236\n",
      "     12        1.0557        1.2689  0.1053\n",
      "     13        \u001b[36m1.0159\u001b[0m        1.3144  0.1020\n",
      "     14        1.0893        \u001b[32m1.2236\u001b[0m  0.1055\n",
      "     15        \u001b[36m0.9781\u001b[0m        \u001b[32m1.1750\u001b[0m  0.1025\n",
      "     16        0.9790        \u001b[32m1.1633\u001b[0m  0.1038\n",
      "     17        0.9826        1.1988  0.1271\n",
      "     18        \u001b[36m0.9743\u001b[0m        \u001b[32m1.1430\u001b[0m  0.1091\n",
      "     19        \u001b[36m0.9611\u001b[0m        1.1438  0.1572\n",
      "     20        0.9674        \u001b[32m1.1327\u001b[0m  0.1538\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m23572.5197\u001b[0m        \u001b[32m1.7401\u001b[0m  0.1162\n",
      "      2      \u001b[36m956.8906\u001b[0m        \u001b[32m1.5305\u001b[0m  0.1048\n",
      "      3        \u001b[36m1.8453\u001b[0m        \u001b[32m1.4310\u001b[0m  0.1039\n",
      "      4      607.2929        \u001b[32m1.3034\u001b[0m  0.1166\n",
      "      5      178.0902        \u001b[32m1.1610\u001b[0m  0.1054\n",
      "      6      717.6482        \u001b[32m1.1170\u001b[0m  0.1073\n",
      "      7       98.3549        1.1242  0.1067\n",
      "      8      724.1432        \u001b[32m1.1091\u001b[0m  0.1070\n",
      "      9      124.9724        \u001b[32m1.0721\u001b[0m  0.1056\n",
      "     10        7.0476        1.1011  0.1045\n",
      "     11        \u001b[36m1.4520\u001b[0m        1.0866  0.0988\n",
      "     12        \u001b[36m1.3894\u001b[0m        \u001b[32m1.0580\u001b[0m  0.1018\n",
      "     13        \u001b[36m1.2513\u001b[0m        \u001b[32m1.0523\u001b[0m  0.1039\n",
      "     14        1.8675        \u001b[32m1.0356\u001b[0m  0.0974\n",
      "     15        1.8186        1.0373  0.1020\n",
      "     16        1.5593        \u001b[32m1.0282\u001b[0m  0.1001\n",
      "     17        1.2721        \u001b[32m1.0196\u001b[0m  0.1000\n",
      "     18        \u001b[36m1.1270\u001b[0m        1.0241  0.1038\n",
      "     19        \u001b[36m1.0965\u001b[0m        \u001b[32m1.0145\u001b[0m  0.1016\n",
      "     20        \u001b[36m1.0853\u001b[0m        1.0181  0.1144\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m519.9013\u001b[0m      \u001b[32m187.0577\u001b[0m  0.1303\n",
      "      2      \u001b[36m153.8756\u001b[0m       \u001b[32m50.0800\u001b[0m  0.1046\n",
      "      3       \u001b[36m39.6881\u001b[0m       \u001b[32m17.3041\u001b[0m  0.1102\n",
      "      4       \u001b[36m10.0899\u001b[0m       \u001b[32m11.2365\u001b[0m  0.1023\n",
      "      5        \u001b[36m3.2465\u001b[0m       11.4997  0.0996\n",
      "      6        \u001b[36m1.8093\u001b[0m       12.3666  0.1035\n",
      "      7        \u001b[36m1.6849\u001b[0m       12.5168  0.1142\n",
      "      8        \u001b[36m1.6519\u001b[0m       12.2848  0.1015\n",
      "      9        \u001b[36m1.6194\u001b[0m       11.9720  0.0997\n",
      "     10        \u001b[36m1.5909\u001b[0m       11.7802  0.0997\n",
      "     11        \u001b[36m1.5657\u001b[0m       11.6796  0.0991\n",
      "     12        \u001b[36m1.5353\u001b[0m       11.4547  0.0999\n",
      "     13        \u001b[36m1.5095\u001b[0m       11.2733  0.1028\n",
      "     14        \u001b[36m1.4889\u001b[0m       \u001b[32m11.1628\u001b[0m  0.1303\n",
      "     15        \u001b[36m1.4656\u001b[0m       \u001b[32m10.8975\u001b[0m  0.1014\n",
      "     16        \u001b[36m1.4463\u001b[0m       \u001b[32m10.8165\u001b[0m  0.1009\n",
      "     17        \u001b[36m1.4202\u001b[0m       \u001b[32m10.5183\u001b[0m  0.1101\n",
      "     18        \u001b[36m1.3957\u001b[0m       \u001b[32m10.3419\u001b[0m  0.0997\n",
      "     19        \u001b[36m1.3764\u001b[0m       \u001b[32m10.1775\u001b[0m  0.1001\n",
      "     20        \u001b[36m1.3565\u001b[0m        \u001b[32m9.9441\u001b[0m  0.1155\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m8.2843\u001b[0m        \u001b[32m1.2664\u001b[0m  0.1649\n",
      "      2        \u001b[36m1.6620\u001b[0m        1.5873  0.1061\n",
      "      3        \u001b[36m1.5302\u001b[0m        \u001b[32m1.0682\u001b[0m  0.0991\n",
      "      4        \u001b[36m1.2656\u001b[0m        \u001b[32m1.0017\u001b[0m  0.1006\n",
      "      5        \u001b[36m1.1867\u001b[0m        1.0335  0.0997\n",
      "      6        \u001b[36m1.1255\u001b[0m        \u001b[32m0.9973\u001b[0m  0.1006\n",
      "      7        \u001b[36m1.0741\u001b[0m        1.0095  0.1007\n",
      "      8        \u001b[36m1.0461\u001b[0m        \u001b[32m0.9918\u001b[0m  0.1007\n",
      "      9        \u001b[36m1.0215\u001b[0m        \u001b[32m0.9913\u001b[0m  0.0983\n",
      "     10        \u001b[36m0.9983\u001b[0m        \u001b[32m0.9899\u001b[0m  0.1011\n",
      "     11        \u001b[36m0.9859\u001b[0m        \u001b[32m0.9841\u001b[0m  0.0994\n",
      "     12        \u001b[36m0.9817\u001b[0m        \u001b[32m0.9670\u001b[0m  0.0991\n",
      "     13        \u001b[36m0.9599\u001b[0m        0.9883  0.1003\n",
      "     14        \u001b[36m0.9504\u001b[0m        0.9860  0.0988\n",
      "     15        \u001b[36m0.9458\u001b[0m        0.9774  0.1029\n",
      "     16        \u001b[36m0.9442\u001b[0m        0.9741  0.1020\n",
      "     17        \u001b[36m0.9364\u001b[0m        0.9788  0.1012\n",
      "     18        \u001b[36m0.9278\u001b[0m        \u001b[32m0.9666\u001b[0m  0.0989\n",
      "     19        \u001b[36m0.9243\u001b[0m        \u001b[32m0.9657\u001b[0m  0.0998\n",
      "     20        \u001b[36m0.9215\u001b[0m        0.9668  0.0991\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m53.7475\u001b[0m    \u001b[32m54287.4593\u001b[0m  0.1002\n",
      "      2       \u001b[36m14.9242\u001b[0m    86346.8525  0.1465\n",
      "      3        \u001b[36m8.8584\u001b[0m   128486.9593  0.1174\n",
      "      4        \u001b[36m6.7452\u001b[0m   188569.7356  0.1044\n",
      "      5        \u001b[36m5.4344\u001b[0m   237481.0903  0.1032\n",
      "      6        \u001b[36m4.5487\u001b[0m   263343.4727  0.1243\n",
      "      7        \u001b[36m4.0201\u001b[0m   283057.6925  0.1087\n",
      "      8        \u001b[36m3.2438\u001b[0m   271238.6306  0.1041\n",
      "      9        \u001b[36m3.0236\u001b[0m   259311.6181  0.1064\n",
      "     10        \u001b[36m2.5682\u001b[0m   238160.2707  0.1041\n",
      "     11        \u001b[36m2.3264\u001b[0m   222775.8092  0.1020\n",
      "     12        \u001b[36m2.1524\u001b[0m   193977.2993  0.1132\n",
      "     13        \u001b[36m1.9353\u001b[0m   171662.3350  0.1253\n",
      "     14        \u001b[36m1.8390\u001b[0m   148441.5768  0.1451\n",
      "     15        \u001b[36m1.6828\u001b[0m   129060.5684  0.1012\n",
      "     16        \u001b[36m1.6083\u001b[0m   117397.3215  0.1004\n",
      "     17        \u001b[36m1.5122\u001b[0m   103929.2755  0.1026\n",
      "     18        \u001b[36m1.4327\u001b[0m    95898.6673  0.1251\n",
      "     19        \u001b[36m1.3729\u001b[0m    84170.1555  0.1051\n",
      "     20        \u001b[36m1.3323\u001b[0m    74590.6084  0.1389\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m26195.6091\u001b[0m        \u001b[32m1.7566\u001b[0m  0.1523\n",
      "      2    \u001b[36m12932.7757\u001b[0m        \u001b[32m1.7029\u001b[0m  0.1096\n",
      "      3     \u001b[36m4958.2371\u001b[0m        \u001b[32m1.5742\u001b[0m  0.0987\n",
      "      4      \u001b[36m507.8459\u001b[0m        \u001b[32m1.3110\u001b[0m  0.1003\n",
      "      5      \u001b[36m183.9791\u001b[0m        \u001b[32m1.1694\u001b[0m  0.1103\n",
      "      6       \u001b[36m24.6932\u001b[0m        \u001b[32m1.0520\u001b[0m  0.1324\n",
      "      7        \u001b[36m1.5247\u001b[0m        \u001b[32m0.9807\u001b[0m  0.1016\n",
      "      8       24.5078        \u001b[32m0.9351\u001b[0m  0.1017\n",
      "      9      243.8648        0.9461  0.0993\n",
      "     10      142.4501        \u001b[32m0.9077\u001b[0m  0.1007\n",
      "     11       38.6572        \u001b[32m0.8842\u001b[0m  0.1060\n",
      "     12        1.8397        \u001b[32m0.8711\u001b[0m  0.0999\n",
      "     13        2.2291        \u001b[32m0.8538\u001b[0m  0.1004\n",
      "     14        \u001b[36m1.4005\u001b[0m        \u001b[32m0.8450\u001b[0m  0.1019\n",
      "     15        \u001b[36m1.2253\u001b[0m        \u001b[32m0.8342\u001b[0m  0.0997\n",
      "     16        \u001b[36m1.2244\u001b[0m        \u001b[32m0.8269\u001b[0m  0.1002\n",
      "     17        \u001b[36m1.2114\u001b[0m        \u001b[32m0.8206\u001b[0m  0.1001\n",
      "     18        \u001b[36m1.1936\u001b[0m        \u001b[32m0.8199\u001b[0m  0.0983\n",
      "     19     9107.6700        1.3778  0.0969\n",
      "     20     4049.7014        1.0562  0.0987\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9438\u001b[0m  \u001b[32m1283325.5240\u001b[0m  0.1072\n",
      "      2        \u001b[36m1.5560\u001b[0m   \u001b[32m870847.3582\u001b[0m  0.1724\n",
      "      3        \u001b[36m1.3438\u001b[0m   \u001b[32m670454.2643\u001b[0m  0.1563\n",
      "      4        \u001b[36m1.2499\u001b[0m   \u001b[32m479070.0457\u001b[0m  0.1531\n",
      "      5        \u001b[36m1.1922\u001b[0m   \u001b[32m301397.8260\u001b[0m  0.1127\n",
      "      6        \u001b[36m1.1503\u001b[0m   \u001b[32m190836.4141\u001b[0m  0.1619\n",
      "      7        \u001b[36m1.1109\u001b[0m   \u001b[32m136277.7004\u001b[0m  0.1083\n",
      "      8        \u001b[36m1.0764\u001b[0m    \u001b[32m85193.9579\u001b[0m  0.1499\n",
      "      9        \u001b[36m1.0538\u001b[0m    \u001b[32m43470.6821\u001b[0m  0.1268\n",
      "     10        \u001b[36m1.0362\u001b[0m    \u001b[32m18969.9644\u001b[0m  0.0969\n",
      "     11        \u001b[36m1.0083\u001b[0m     \u001b[32m5509.2686\u001b[0m  0.0971\n",
      "     12        \u001b[36m1.0015\u001b[0m       \u001b[32m14.1007\u001b[0m  0.0961\n",
      "     13        \u001b[36m0.9837\u001b[0m     3704.2366  0.1034\n",
      "     14        \u001b[36m0.9744\u001b[0m    24970.6483  0.0974\n",
      "     15        \u001b[36m0.9722\u001b[0m    47430.6806  0.0964\n",
      "     16        \u001b[36m0.9618\u001b[0m    85561.5928  0.0956\n",
      "     17        \u001b[36m0.9471\u001b[0m   152121.2591  0.0960\n",
      "     18        0.9537   197892.4333  0.0962\n",
      "     19        \u001b[36m0.9332\u001b[0m   260373.7736  0.0970\n",
      "     20        0.9371   317068.1782  0.0973\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.7031\u001b[0m        \u001b[32m3.8451\u001b[0m  0.1386\n",
      "      2        \u001b[36m2.8221\u001b[0m        \u001b[32m1.8394\u001b[0m  0.1155\n",
      "      3        \u001b[36m1.8683\u001b[0m        \u001b[32m1.6016\u001b[0m  0.1032\n",
      "      4        \u001b[36m1.6967\u001b[0m        \u001b[32m1.4882\u001b[0m  0.1066\n",
      "      5        \u001b[36m1.5725\u001b[0m        \u001b[32m1.3859\u001b[0m  0.0963\n",
      "      6        \u001b[36m1.5023\u001b[0m        \u001b[32m1.3057\u001b[0m  0.1037\n",
      "      7        \u001b[36m1.4168\u001b[0m        \u001b[32m1.2728\u001b[0m  0.1031\n",
      "      8        \u001b[36m1.3555\u001b[0m        \u001b[32m1.2083\u001b[0m  0.1012\n",
      "      9        \u001b[36m1.2986\u001b[0m        \u001b[32m1.1600\u001b[0m  0.0993\n",
      "     10        \u001b[36m1.2447\u001b[0m        \u001b[32m1.1211\u001b[0m  0.1008\n",
      "     11        \u001b[36m1.1940\u001b[0m        \u001b[32m1.0803\u001b[0m  0.1057\n",
      "     12        \u001b[36m1.1570\u001b[0m        \u001b[32m1.0669\u001b[0m  0.1022\n",
      "     13        \u001b[36m1.1074\u001b[0m        \u001b[32m1.0206\u001b[0m  0.0940\n",
      "     14        \u001b[36m1.0674\u001b[0m        \u001b[32m1.0092\u001b[0m  0.1042\n",
      "     15        \u001b[36m1.0352\u001b[0m        \u001b[32m0.9890\u001b[0m  0.1092\n",
      "     16        \u001b[36m1.0193\u001b[0m        \u001b[32m0.9694\u001b[0m  0.1024\n",
      "     17        \u001b[36m0.9931\u001b[0m        \u001b[32m0.9653\u001b[0m  0.1018\n",
      "     18        \u001b[36m0.9865\u001b[0m        0.9843  0.0983\n",
      "     19        \u001b[36m0.9606\u001b[0m        0.9673  0.0971\n",
      "     20        \u001b[36m0.9443\u001b[0m        \u001b[32m0.9608\u001b[0m  0.1002\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m305948.2667\u001b[0m       \u001b[32m10.4100\u001b[0m  0.1091\n",
      "      2   \u001b[36m160939.9117\u001b[0m        \u001b[32m5.3314\u001b[0m  0.1459\n",
      "      3    \u001b[36m84537.3694\u001b[0m        \u001b[32m4.4436\u001b[0m  0.1124\n",
      "      4    \u001b[36m30248.6282\u001b[0m        \u001b[32m4.1118\u001b[0m  0.0958\n",
      "      5    \u001b[36m12190.0905\u001b[0m        \u001b[32m3.8022\u001b[0m  0.1005\n",
      "      6     \u001b[36m4264.4723\u001b[0m        \u001b[32m3.5213\u001b[0m  0.0980\n",
      "      7     \u001b[36m2082.1584\u001b[0m        \u001b[32m3.2480\u001b[0m  0.0998\n",
      "      8      \u001b[36m131.8806\u001b[0m        \u001b[32m2.7844\u001b[0m  0.0961\n",
      "      9       \u001b[36m16.7059\u001b[0m        \u001b[32m2.3692\u001b[0m  0.0968\n",
      "     10        \u001b[36m9.1166\u001b[0m        \u001b[32m1.9997\u001b[0m  0.1023\n",
      "     11        \u001b[36m5.9973\u001b[0m        \u001b[32m1.7416\u001b[0m  0.0979\n",
      "     12        \u001b[36m2.2783\u001b[0m        \u001b[32m1.5758\u001b[0m  0.0946\n",
      "     13        \u001b[36m1.7974\u001b[0m        \u001b[32m1.4850\u001b[0m  0.0942\n",
      "     14        \u001b[36m1.5490\u001b[0m        \u001b[32m1.4093\u001b[0m  0.0963\n",
      "     15        \u001b[36m1.4892\u001b[0m        \u001b[32m1.3469\u001b[0m  0.0955\n",
      "     16        \u001b[36m1.4363\u001b[0m        \u001b[32m1.3028\u001b[0m  0.0946\n",
      "     17        \u001b[36m1.3967\u001b[0m        \u001b[32m1.2740\u001b[0m  0.0977\n",
      "     18        \u001b[36m1.3652\u001b[0m        \u001b[32m1.2416\u001b[0m  0.0938\n",
      "     19        \u001b[36m1.3356\u001b[0m        \u001b[32m1.2133\u001b[0m  0.0966\n",
      "     20        \u001b[36m1.3197\u001b[0m        \u001b[32m1.1940\u001b[0m  0.0957\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m6.3268\u001b[0m     \u001b[32m5579.9767\u001b[0m  0.0949\n",
      "      2        \u001b[36m2.7159\u001b[0m     \u001b[32m1840.2014\u001b[0m  0.0929\n",
      "      3        \u001b[36m1.4755\u001b[0m     8020.1063  0.1043\n",
      "      4        \u001b[36m1.2877\u001b[0m     9371.4989  0.1485\n",
      "      5        \u001b[36m1.1672\u001b[0m    18719.8722  0.1143\n",
      "      6        \u001b[36m1.1421\u001b[0m    35920.6418  0.1021\n",
      "      7        \u001b[36m1.1197\u001b[0m    62789.4546  0.0949\n",
      "      8        \u001b[36m1.1084\u001b[0m   117919.6860  0.0952\n",
      "      9        \u001b[36m1.0950\u001b[0m   205689.7930  0.0956\n",
      "     10        \u001b[36m1.0805\u001b[0m   344664.1405  0.1001\n",
      "     11        \u001b[36m1.0780\u001b[0m   633822.6294  0.0942\n",
      "     12        \u001b[36m1.0563\u001b[0m   909065.3353  0.0974\n",
      "     13        \u001b[36m1.0477\u001b[0m  1392798.5099  0.0953\n",
      "     14        \u001b[36m1.0357\u001b[0m  2073790.9418  0.0959\n",
      "     15        \u001b[36m1.0255\u001b[0m  2666648.2604  0.0962\n",
      "     16        \u001b[36m1.0142\u001b[0m  3800486.5315  0.0950\n",
      "     17        \u001b[36m1.0002\u001b[0m  4558262.3114  0.0949\n",
      "     18        \u001b[36m0.9914\u001b[0m  5400273.0802  0.0944\n",
      "     19        \u001b[36m0.9813\u001b[0m  6707713.2218  0.0949\n",
      "     20        \u001b[36m0.9765\u001b[0m  7861711.7797  0.0953\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3348\u001b[0m        \u001b[32m2.2515\u001b[0m  0.0950\n",
      "      2        \u001b[36m2.3212\u001b[0m        \u001b[32m1.6725\u001b[0m  0.0958\n",
      "      3        \u001b[36m1.8952\u001b[0m        \u001b[32m1.5993\u001b[0m  0.0930\n",
      "      4        \u001b[36m1.7018\u001b[0m        \u001b[32m1.5399\u001b[0m  0.0949\n",
      "      5        \u001b[36m1.5515\u001b[0m        \u001b[32m1.4775\u001b[0m  0.1144\n",
      "      6        \u001b[36m1.4139\u001b[0m        \u001b[32m1.3730\u001b[0m  0.1402\n",
      "      7        \u001b[36m1.3349\u001b[0m        \u001b[32m1.3256\u001b[0m  0.1083\n",
      "      8        \u001b[36m1.2682\u001b[0m        1.3525  0.0984\n",
      "      9        \u001b[36m1.1986\u001b[0m        \u001b[32m1.2951\u001b[0m  0.0953\n",
      "     10        \u001b[36m1.1492\u001b[0m        \u001b[32m1.2800\u001b[0m  0.1113\n",
      "     11        \u001b[36m1.1083\u001b[0m        1.3459  0.1088\n",
      "     12        \u001b[36m1.1008\u001b[0m        \u001b[32m1.2756\u001b[0m  0.0971\n",
      "     13        \u001b[36m1.0602\u001b[0m        \u001b[32m1.2357\u001b[0m  0.0988\n",
      "     14        \u001b[36m1.0424\u001b[0m        1.2992  0.0970\n",
      "     15        \u001b[36m1.0258\u001b[0m        1.2395  0.0973\n",
      "     16        \u001b[36m0.9936\u001b[0m        \u001b[32m1.2056\u001b[0m  0.0966\n",
      "     17        \u001b[36m0.9858\u001b[0m        1.2141  0.0950\n",
      "     18        \u001b[36m0.9823\u001b[0m        1.2075  0.0965\n",
      "     19        \u001b[36m0.9529\u001b[0m        1.3068  0.1228\n",
      "     20        0.9920        1.2124  0.1426\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m244971.4819\u001b[0m        \u001b[32m4.0962\u001b[0m  0.0983\n",
      "      2   \u001b[36m112003.0968\u001b[0m        \u001b[32m3.6417\u001b[0m  0.0974\n",
      "      3    \u001b[36m37222.7978\u001b[0m        \u001b[32m3.4919\u001b[0m  0.0970\n",
      "      4    \u001b[36m17043.7415\u001b[0m        \u001b[32m3.3358\u001b[0m  0.1009\n",
      "      5     \u001b[36m1531.1654\u001b[0m        \u001b[32m3.0029\u001b[0m  0.1046\n",
      "      6       \u001b[36m22.2460\u001b[0m        \u001b[32m2.4088\u001b[0m  0.1417\n",
      "      7        \u001b[36m2.0894\u001b[0m        \u001b[32m2.0740\u001b[0m  0.1277\n",
      "      8        4.9422        \u001b[32m1.7857\u001b[0m  0.1510\n",
      "      9        2.4856        \u001b[32m1.6580\u001b[0m  0.0976\n",
      "     10        \u001b[36m1.9185\u001b[0m        \u001b[32m1.5528\u001b[0m  0.1012\n",
      "     11        \u001b[36m1.2761\u001b[0m        \u001b[32m1.4813\u001b[0m  0.1041\n",
      "     12        1.2771        \u001b[32m1.4348\u001b[0m  0.1040\n",
      "     13        \u001b[36m1.1746\u001b[0m        \u001b[32m1.3732\u001b[0m  0.1047\n",
      "     14        \u001b[36m1.1257\u001b[0m        \u001b[32m1.3506\u001b[0m  0.1009\n",
      "     15        \u001b[36m1.1030\u001b[0m        \u001b[32m1.3204\u001b[0m  0.0989\n",
      "     16        \u001b[36m1.0870\u001b[0m        \u001b[32m1.2876\u001b[0m  0.0987\n",
      "     17        \u001b[36m1.0840\u001b[0m        \u001b[32m1.2820\u001b[0m  0.1024\n",
      "     18        \u001b[36m1.0621\u001b[0m        \u001b[32m1.2472\u001b[0m  0.1039\n",
      "     19        \u001b[36m1.0481\u001b[0m        \u001b[32m1.2446\u001b[0m  0.0963\n",
      "     20     1432.3260        \u001b[32m1.2203\u001b[0m  0.0964\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m3165.5177\u001b[0m        \u001b[32m3.0496\u001b[0m  0.1009\n",
      "      2      \u001b[36m842.8962\u001b[0m        \u001b[32m2.2460\u001b[0m  0.1016\n",
      "      3       \u001b[36m39.8335\u001b[0m        \u001b[32m2.1358\u001b[0m  0.1049\n",
      "      4        \u001b[36m2.0906\u001b[0m        \u001b[32m1.9992\u001b[0m  0.0960\n",
      "      5       16.8244        \u001b[32m1.8758\u001b[0m  0.0962\n",
      "      6        6.7943        \u001b[32m1.7534\u001b[0m  0.0994\n",
      "      7        \u001b[36m1.7273\u001b[0m        \u001b[32m1.6505\u001b[0m  0.1398\n",
      "      8        \u001b[36m1.6073\u001b[0m        \u001b[32m1.5808\u001b[0m  0.1046\n",
      "      9        3.8621        \u001b[32m1.5367\u001b[0m  0.1013\n",
      "     10        4.8150        \u001b[32m1.4930\u001b[0m  0.1016\n",
      "     11        1.6276        \u001b[32m1.4430\u001b[0m  0.1043\n",
      "     12        1.7450        \u001b[32m1.4186\u001b[0m  0.0998\n",
      "     13        \u001b[36m1.3406\u001b[0m        \u001b[32m1.3682\u001b[0m  0.1078\n",
      "     14        \u001b[36m1.2461\u001b[0m        1.3684  0.0952\n",
      "     15        1.5745        \u001b[32m1.2947\u001b[0m  0.1011\n",
      "     16        \u001b[36m1.0838\u001b[0m        \u001b[32m1.2849\u001b[0m  0.0987\n",
      "     17        1.2332        \u001b[32m1.2711\u001b[0m  0.0957\n",
      "     18        \u001b[36m1.0662\u001b[0m        \u001b[32m1.2569\u001b[0m  0.0954\n",
      "     19        \u001b[36m1.0318\u001b[0m        \u001b[32m1.2550\u001b[0m  0.0957\n",
      "     20        1.0390        \u001b[32m1.2515\u001b[0m  0.0958\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m9.0185\u001b[0m        \u001b[32m4.9277\u001b[0m  0.0977\n",
      "      2        \u001b[36m4.7557\u001b[0m        \u001b[32m2.6519\u001b[0m  0.1003\n",
      "      3        \u001b[36m3.0104\u001b[0m        \u001b[32m1.8690\u001b[0m  0.0988\n",
      "      4        \u001b[36m2.1558\u001b[0m        \u001b[32m1.3190\u001b[0m  0.0952\n",
      "      5        \u001b[36m1.5776\u001b[0m        \u001b[32m1.1836\u001b[0m  0.0938\n",
      "      6        \u001b[36m1.3007\u001b[0m        1.1873  0.0976\n",
      "      7        \u001b[36m1.1455\u001b[0m        \u001b[32m1.0601\u001b[0m  0.0940\n",
      "      8        \u001b[36m1.0801\u001b[0m        \u001b[32m1.0349\u001b[0m  0.1292\n",
      "      9        \u001b[36m1.0425\u001b[0m        \u001b[32m0.9632\u001b[0m  0.1212\n",
      "     10        \u001b[36m0.9900\u001b[0m        0.9795  0.1039\n",
      "     11        \u001b[36m0.9670\u001b[0m        \u001b[32m0.9463\u001b[0m  0.1033\n",
      "     12        \u001b[36m0.9485\u001b[0m        \u001b[32m0.9195\u001b[0m  0.1004\n",
      "     13        \u001b[36m0.9396\u001b[0m        0.9898  0.0952\n",
      "     14        \u001b[36m0.9324\u001b[0m        0.9482  0.0995\n",
      "     15        \u001b[36m0.9204\u001b[0m        0.9368  0.0965\n",
      "     16        \u001b[36m0.9136\u001b[0m        0.9406  0.0997\n",
      "     17        \u001b[36m0.9073\u001b[0m        0.9622  0.0981\n",
      "     18        \u001b[36m0.9017\u001b[0m        \u001b[32m0.9173\u001b[0m  0.0959\n",
      "     19        0.9173        0.9689  0.0948\n",
      "     20        \u001b[36m0.8986\u001b[0m        0.9231  0.0941\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m20415.8505\u001b[0m       \u001b[32m51.5306\u001b[0m  0.1011\n",
      "      2      \u001b[36m404.4512\u001b[0m       \u001b[32m11.0919\u001b[0m  0.0999\n",
      "      3      \u001b[36m278.5648\u001b[0m        \u001b[32m6.9638\u001b[0m  0.0955\n",
      "      4      408.4382        \u001b[32m2.8501\u001b[0m  0.0950\n",
      "      5        \u001b[36m3.9464\u001b[0m        \u001b[32m2.3939\u001b[0m  0.0982\n",
      "      6     2153.0678        \u001b[32m1.9184\u001b[0m  0.0950\n",
      "      7       13.4608        \u001b[32m1.6019\u001b[0m  0.0965\n",
      "      8     1522.7048        \u001b[32m1.5966\u001b[0m  0.0950\n",
      "      9       53.7580        \u001b[32m1.3601\u001b[0m  0.0963\n",
      "     10     1015.0686        \u001b[32m1.3087\u001b[0m  0.1402\n",
      "     11      541.0019        \u001b[32m1.1744\u001b[0m  0.1081\n",
      "     12       87.4786        \u001b[32m1.1368\u001b[0m  0.0969\n",
      "     13       23.8000        \u001b[32m1.0963\u001b[0m  0.0958\n",
      "     14        \u001b[36m3.0874\u001b[0m        \u001b[32m1.0713\u001b[0m  0.0960\n",
      "     15       10.2660        \u001b[32m1.0366\u001b[0m  0.0961\n",
      "     16       20.9724        \u001b[32m1.0224\u001b[0m  0.1036\n",
      "     17       24.1486        \u001b[32m0.9981\u001b[0m  0.1050\n",
      "     18       23.6224        \u001b[32m0.9833\u001b[0m  0.1004\n",
      "     19        \u001b[36m2.2750\u001b[0m        \u001b[32m0.9720\u001b[0m  0.0959\n",
      "     20       31.9956        \u001b[32m0.9563\u001b[0m  0.0965\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m7.3044\u001b[0m    \u001b[32m74980.0940\u001b[0m  0.1032\n",
      "      2        \u001b[36m2.9260\u001b[0m    \u001b[32m14379.1929\u001b[0m  0.1352\n",
      "      3        \u001b[36m1.6871\u001b[0m    25527.6222  0.1401\n",
      "      4        \u001b[36m1.4439\u001b[0m    70098.8598  0.1350\n",
      "      5        \u001b[36m1.3374\u001b[0m   105422.1362  0.1242\n",
      "      6        \u001b[36m1.2320\u001b[0m   134091.9333  0.1157\n",
      "      7        \u001b[36m1.1581\u001b[0m   160200.3686  0.1110\n",
      "      8        \u001b[36m1.1102\u001b[0m   191289.4180  0.1027\n",
      "      9        \u001b[36m1.0761\u001b[0m   250197.2243  0.1416\n",
      "     10        \u001b[36m1.0552\u001b[0m   204343.0235  0.1859\n",
      "     11        \u001b[36m1.0379\u001b[0m   159015.0254  0.1273\n",
      "     12        \u001b[36m1.0366\u001b[0m   158783.8567  0.1307\n",
      "     13        \u001b[36m1.0233\u001b[0m   163674.9139  0.1554\n",
      "     14        \u001b[36m1.0106\u001b[0m   173608.3118  0.2661\n",
      "     15        1.0132   184920.7500  0.1246\n",
      "     16        \u001b[36m0.9954\u001b[0m   193138.6793  0.1179\n",
      "     17        0.9989   194364.3153  0.0980\n",
      "     18        \u001b[36m0.9873\u001b[0m   202400.1184  0.0946\n",
      "     19        0.9989   208740.8303  0.0975\n",
      "     20        0.9913   215572.2714  0.0972\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m183648.5663\u001b[0m        \u001b[32m2.4086\u001b[0m  0.1002\n",
      "      2   \u001b[36m115021.4882\u001b[0m        \u001b[32m1.5179\u001b[0m  0.0952\n",
      "      3    \u001b[36m82647.9636\u001b[0m        \u001b[32m1.4782\u001b[0m  0.0961\n",
      "      4    \u001b[36m56354.2411\u001b[0m        1.6482  0.0979\n",
      "      5    \u001b[36m37614.5569\u001b[0m        1.7898  0.0962\n",
      "      6    \u001b[36m31373.6708\u001b[0m        2.0602  0.0948\n",
      "      7    \u001b[36m16305.0178\u001b[0m        2.0009  0.0943\n",
      "      8    \u001b[36m11084.9968\u001b[0m        1.9085  0.1213\n",
      "      9     \u001b[36m8422.5928\u001b[0m        1.8286  0.1197\n",
      "     10     \u001b[36m4562.7992\u001b[0m        \u001b[32m1.4443\u001b[0m  0.0989\n",
      "     11     \u001b[36m3283.9028\u001b[0m        \u001b[32m1.2705\u001b[0m  0.0957\n",
      "     12     \u001b[36m1701.2585\u001b[0m        \u001b[32m1.1695\u001b[0m  0.0956\n",
      "     13     \u001b[36m1116.3567\u001b[0m        \u001b[32m1.1207\u001b[0m  0.0948\n",
      "     14      \u001b[36m752.8919\u001b[0m        \u001b[32m1.0796\u001b[0m  0.1005\n",
      "     15      \u001b[36m508.2490\u001b[0m        1.1783  0.0989\n",
      "     16      \u001b[36m210.8273\u001b[0m        1.1711  0.0957\n",
      "     17      \u001b[36m110.4420\u001b[0m        1.1259  0.0950\n",
      "     18       \u001b[36m88.5458\u001b[0m        1.0956  0.0947\n",
      "     19       \u001b[36m37.0146\u001b[0m        \u001b[32m1.0673\u001b[0m  0.0971\n",
      "     20       \u001b[36m17.8957\u001b[0m        \u001b[32m1.0558\u001b[0m  0.0949\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m74391.9105\u001b[0m        \u001b[32m1.3716\u001b[0m  0.0971\n",
      "      2    \u001b[36m35926.0193\u001b[0m        \u001b[32m1.2538\u001b[0m  0.0963\n",
      "      3    \u001b[36m27642.4435\u001b[0m        \u001b[32m1.1971\u001b[0m  0.0970\n",
      "      4     \u001b[36m9408.6456\u001b[0m        \u001b[32m1.1427\u001b[0m  0.0965\n",
      "      5     \u001b[36m4988.9089\u001b[0m        \u001b[32m1.0984\u001b[0m  0.0981\n",
      "      6     \u001b[36m2748.3071\u001b[0m        \u001b[32m1.0604\u001b[0m  0.0978\n",
      "      7     \u001b[36m1230.5594\u001b[0m        \u001b[32m1.0459\u001b[0m  0.1449\n",
      "      8      \u001b[36m757.0086\u001b[0m        \u001b[32m1.0351\u001b[0m  0.0992\n",
      "      9      \u001b[36m375.9761\u001b[0m        \u001b[32m1.0316\u001b[0m  0.1016\n",
      "     10      \u001b[36m109.2712\u001b[0m        \u001b[32m1.0248\u001b[0m  0.1427\n",
      "     11       \u001b[36m21.1850\u001b[0m        \u001b[32m1.0186\u001b[0m  0.1116\n",
      "     12       \u001b[36m11.9247\u001b[0m        \u001b[32m1.0101\u001b[0m  0.0951\n",
      "     13        \u001b[36m2.8042\u001b[0m        \u001b[32m1.0073\u001b[0m  0.0974\n",
      "     14        \u001b[36m1.4562\u001b[0m        1.0073  0.1011\n",
      "     15        \u001b[36m1.0806\u001b[0m        1.0141  0.0989\n",
      "     16        \u001b[36m0.9900\u001b[0m        1.0120  0.0958\n",
      "     17        \u001b[36m0.9774\u001b[0m        \u001b[32m1.0071\u001b[0m  0.0968\n",
      "     18        \u001b[36m0.9688\u001b[0m        \u001b[32m1.0025\u001b[0m  0.0988\n",
      "     19        \u001b[36m0.9671\u001b[0m        1.0050  0.0974\n",
      "     20        \u001b[36m0.9601\u001b[0m        1.0068  0.0983\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m96195.9320\u001b[0m        \u001b[32m2.3345\u001b[0m  0.0962\n",
      "      2    \u001b[36m74354.5217\u001b[0m        \u001b[32m1.5548\u001b[0m  0.0963\n",
      "      3    \u001b[36m58754.3156\u001b[0m        \u001b[32m1.3647\u001b[0m  0.0960\n",
      "      4    \u001b[36m43517.5327\u001b[0m        \u001b[32m1.2122\u001b[0m  0.0967\n",
      "      5    \u001b[36m36796.0055\u001b[0m        \u001b[32m1.1871\u001b[0m  0.0950\n",
      "      6    \u001b[36m29474.9233\u001b[0m        \u001b[32m1.1553\u001b[0m  0.0962\n",
      "      7    \u001b[36m24822.9812\u001b[0m        1.1598  0.0958\n",
      "      8    \u001b[36m18569.3767\u001b[0m        \u001b[32m1.1361\u001b[0m  0.0966\n",
      "      9    \u001b[36m15578.2587\u001b[0m        \u001b[32m1.1003\u001b[0m  0.0950\n",
      "     10    \u001b[36m13306.5250\u001b[0m        1.1078  0.0951\n",
      "     11    \u001b[36m11944.9450\u001b[0m        \u001b[32m1.0856\u001b[0m  0.1186\n",
      "     12     \u001b[36m9069.4885\u001b[0m        \u001b[32m1.0485\u001b[0m  0.1272\n",
      "     13     \u001b[36m7860.2933\u001b[0m        \u001b[32m1.0375\u001b[0m  0.1023\n",
      "     14     \u001b[36m7034.5516\u001b[0m        1.0674  0.1043\n",
      "     15     \u001b[36m5799.1211\u001b[0m        1.0444  0.0974\n",
      "     16     \u001b[36m5482.0607\u001b[0m        \u001b[32m1.0338\u001b[0m  0.0965\n",
      "     17     \u001b[36m4349.6938\u001b[0m        \u001b[32m1.0123\u001b[0m  0.0999\n",
      "     18     5142.4262        \u001b[32m1.0079\u001b[0m  0.1103\n",
      "     19     \u001b[36m3539.9106\u001b[0m        \u001b[32m0.9885\u001b[0m  0.1575\n",
      "     20     \u001b[36m2923.4571\u001b[0m        \u001b[32m0.9584\u001b[0m  0.1101\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m19.5587\u001b[0m   \u001b[32m434777.5455\u001b[0m  0.1049\n",
      "      2        \u001b[36m8.1340\u001b[0m   460807.0353  0.1017\n",
      "      3        \u001b[36m4.6406\u001b[0m   495006.0589  0.1170\n",
      "      4        \u001b[36m2.9790\u001b[0m   461090.1156  0.1612\n",
      "      5        \u001b[36m2.3408\u001b[0m   \u001b[32m413751.5659\u001b[0m  0.1249\n",
      "      6        \u001b[36m1.9148\u001b[0m   \u001b[32m389784.8442\u001b[0m  0.1031\n",
      "      7        \u001b[36m1.7605\u001b[0m   \u001b[32m342182.2926\u001b[0m  0.1146\n",
      "      8        \u001b[36m1.6218\u001b[0m   \u001b[32m310498.9440\u001b[0m  0.1131\n",
      "      9        \u001b[36m1.4752\u001b[0m   \u001b[32m266264.2628\u001b[0m  0.1040\n",
      "     10        \u001b[36m1.4282\u001b[0m   \u001b[32m229414.4409\u001b[0m  0.1309\n",
      "     11        \u001b[36m1.3710\u001b[0m   \u001b[32m195863.1766\u001b[0m  0.1604\n",
      "     12        \u001b[36m1.3133\u001b[0m   \u001b[32m164425.4202\u001b[0m  0.1122\n",
      "     13        \u001b[36m1.2585\u001b[0m   \u001b[32m150634.9105\u001b[0m  0.1010\n",
      "     14        \u001b[36m1.2262\u001b[0m   \u001b[32m127170.7961\u001b[0m  0.1106\n",
      "     15        \u001b[36m1.1860\u001b[0m   \u001b[32m113309.6079\u001b[0m  0.1073\n",
      "     16        \u001b[36m1.1427\u001b[0m    \u001b[32m91267.0576\u001b[0m  0.1115\n",
      "     17        \u001b[36m1.1254\u001b[0m    \u001b[32m82221.2207\u001b[0m  0.1110\n",
      "     18        \u001b[36m1.0989\u001b[0m    \u001b[32m73407.9326\u001b[0m  0.1059\n",
      "     19        1.1083    \u001b[32m63515.7858\u001b[0m  0.1295\n",
      "     20        \u001b[36m1.0642\u001b[0m    \u001b[32m51733.3232\u001b[0m  0.1502\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m16875.2700\u001b[0m        \u001b[32m4.4485\u001b[0m  0.1222\n",
      "      2     \u001b[36m8860.0027\u001b[0m        \u001b[32m4.2672\u001b[0m  0.1034\n",
      "      3     \u001b[36m4723.3951\u001b[0m        \u001b[32m3.8365\u001b[0m  0.1069\n",
      "      4     \u001b[36m2631.4996\u001b[0m        \u001b[32m3.2911\u001b[0m  0.1018\n",
      "      5     \u001b[36m1190.7397\u001b[0m        \u001b[32m2.2028\u001b[0m  0.1052\n",
      "      6      \u001b[36m715.4132\u001b[0m        \u001b[32m1.7391\u001b[0m  0.1052\n",
      "      7      \u001b[36m223.9460\u001b[0m        \u001b[32m1.5003\u001b[0m  0.1084\n",
      "      8       \u001b[36m76.1912\u001b[0m        \u001b[32m1.3744\u001b[0m  0.1851\n",
      "      9       \u001b[36m32.2118\u001b[0m        \u001b[32m1.2985\u001b[0m  0.2046\n",
      "     10       \u001b[36m19.3283\u001b[0m        \u001b[32m1.2574\u001b[0m  0.1366\n",
      "     11       \u001b[36m13.4178\u001b[0m        \u001b[32m1.1951\u001b[0m  0.1096\n",
      "     12        \u001b[36m2.7551\u001b[0m        1.2514  0.1097\n",
      "     13        2.9377        1.3421  0.1098\n",
      "     14        3.4432        \u001b[32m1.1889\u001b[0m  0.1589\n",
      "     15        2.9979        1.2022  0.1103\n",
      "     16        \u001b[36m2.3190\u001b[0m        \u001b[32m1.1085\u001b[0m  0.1107\n",
      "     17        \u001b[36m2.1284\u001b[0m        \u001b[32m1.0839\u001b[0m  0.1109\n",
      "     18        \u001b[36m2.0538\u001b[0m        \u001b[32m1.0485\u001b[0m  0.1073\n",
      "     19        \u001b[36m1.9060\u001b[0m        \u001b[32m1.0219\u001b[0m  0.1084\n",
      "     20        \u001b[36m1.7770\u001b[0m        \u001b[32m0.9796\u001b[0m  0.1109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m30067.3909\u001b[0m       \u001b[32m10.0811\u001b[0m  0.1122\n",
      "      2     \u001b[36m9342.0588\u001b[0m        \u001b[32m2.7404\u001b[0m  0.1063\n",
      "      3     \u001b[36m8143.3976\u001b[0m        \u001b[32m1.7411\u001b[0m  0.1053\n",
      "      4      \u001b[36m153.5788\u001b[0m        \u001b[32m1.5246\u001b[0m  0.1229\n",
      "      5      264.1247        \u001b[32m1.3666\u001b[0m  0.1084\n",
      "      6       \u001b[36m21.9655\u001b[0m        \u001b[32m1.2653\u001b[0m  0.1050\n",
      "      7        \u001b[36m8.0050\u001b[0m        \u001b[32m1.1794\u001b[0m  0.1274\n",
      "      8        9.0540        \u001b[32m1.1054\u001b[0m  0.1507\n",
      "      9        \u001b[36m6.8760\u001b[0m        \u001b[32m1.0430\u001b[0m  0.1160\n",
      "     10        \u001b[36m5.7168\u001b[0m        \u001b[32m1.0187\u001b[0m  0.1088\n",
      "     11        \u001b[36m5.3529\u001b[0m        \u001b[32m0.9861\u001b[0m  0.1065\n",
      "     12        \u001b[36m5.0389\u001b[0m        \u001b[32m0.9593\u001b[0m  0.1071\n",
      "     13        \u001b[36m4.8737\u001b[0m        0.9701  0.1205\n",
      "     14        \u001b[36m4.6301\u001b[0m        \u001b[32m0.9393\u001b[0m  0.1068\n",
      "     15        \u001b[36m4.4412\u001b[0m        \u001b[32m0.9315\u001b[0m  0.1217\n",
      "     16        \u001b[36m4.2527\u001b[0m        \u001b[32m0.9278\u001b[0m  0.1109\n",
      "     17        \u001b[36m4.1084\u001b[0m        0.9344  0.1084\n",
      "     18        \u001b[36m4.0062\u001b[0m        \u001b[32m0.9183\u001b[0m  0.1089\n",
      "     19        4.9367        0.9244  0.1094\n",
      "     20        5.2450        \u001b[32m0.9102\u001b[0m  0.1063\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m18697.6843\u001b[0m        \u001b[32m3.4367\u001b[0m  0.1169\n",
      "      2     \u001b[36m9161.1630\u001b[0m        \u001b[32m2.3114\u001b[0m  0.1073\n",
      "      3     \u001b[36m4876.5396\u001b[0m        \u001b[32m2.2551\u001b[0m  0.1065\n",
      "      4     \u001b[36m1257.7123\u001b[0m        2.4297  0.1069\n",
      "      5      \u001b[36m346.2661\u001b[0m        2.3549  0.1241\n",
      "      6       \u001b[36m67.2239\u001b[0m        \u001b[32m2.1792\u001b[0m  0.1220\n",
      "      7     1190.2589        \u001b[32m2.1547\u001b[0m  0.1430\n",
      "      8       83.8983        2.2032  0.1213\n",
      "      9      356.6895        \u001b[32m2.0007\u001b[0m  0.1106\n",
      "     10       \u001b[36m15.9874\u001b[0m        \u001b[32m1.8743\u001b[0m  0.1075\n",
      "     11      216.1471        1.9168  0.1084\n",
      "     12       62.7217        1.8782  0.1095\n",
      "     13        \u001b[36m1.9651\u001b[0m        \u001b[32m1.7828\u001b[0m  0.1134\n",
      "     14        4.5673        \u001b[32m1.7705\u001b[0m  0.1014\n",
      "     15        \u001b[36m1.6703\u001b[0m        \u001b[32m1.7546\u001b[0m  0.1035\n",
      "     16        6.2430        \u001b[32m1.7190\u001b[0m  0.1080\n",
      "     17        1.7446        \u001b[32m1.7111\u001b[0m  0.1050\n",
      "     18        \u001b[36m1.1143\u001b[0m        \u001b[32m1.6831\u001b[0m  0.1056\n",
      "     19        \u001b[36m1.1094\u001b[0m        \u001b[32m1.6669\u001b[0m  0.1163\n",
      "     20        \u001b[36m1.0639\u001b[0m        \u001b[32m1.6517\u001b[0m  0.1070\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m21.5332\u001b[0m        \u001b[32m5.5597\u001b[0m  0.1055\n",
      "      2        \u001b[36m6.5898\u001b[0m        \u001b[32m2.6938\u001b[0m  0.1075\n",
      "      3        \u001b[36m3.2261\u001b[0m        \u001b[32m2.2192\u001b[0m  0.1061\n",
      "      4        \u001b[36m2.4606\u001b[0m        \u001b[32m1.9184\u001b[0m  0.1073\n",
      "      5        \u001b[36m2.2420\u001b[0m        \u001b[32m1.7055\u001b[0m  0.1063\n",
      "      6        \u001b[36m2.0723\u001b[0m        \u001b[32m1.5910\u001b[0m  0.1326\n",
      "      7        \u001b[36m1.9075\u001b[0m        \u001b[32m1.4986\u001b[0m  0.1395\n",
      "      8        \u001b[36m1.8111\u001b[0m        \u001b[32m1.4072\u001b[0m  0.1382\n",
      "      9        \u001b[36m1.7228\u001b[0m        \u001b[32m1.3833\u001b[0m  0.1151\n",
      "     10        \u001b[36m1.6260\u001b[0m        \u001b[32m1.3592\u001b[0m  0.1144\n",
      "     11        \u001b[36m1.5727\u001b[0m        \u001b[32m1.3021\u001b[0m  0.1103\n",
      "     12        \u001b[36m1.5117\u001b[0m        1.3079  0.1220\n",
      "     13        \u001b[36m1.4456\u001b[0m        \u001b[32m1.2674\u001b[0m  0.1476\n",
      "     14        1.4556        \u001b[32m1.2558\u001b[0m  0.1564\n",
      "     15        \u001b[36m1.3807\u001b[0m        \u001b[32m1.2471\u001b[0m  0.1218\n",
      "     16        \u001b[36m1.3710\u001b[0m        \u001b[32m1.2233\u001b[0m  0.1198\n",
      "     17        \u001b[36m1.3201\u001b[0m        1.2314  0.1374\n",
      "     18        \u001b[36m1.3141\u001b[0m        \u001b[32m1.2144\u001b[0m  0.1185\n",
      "     19        \u001b[36m1.2736\u001b[0m        \u001b[32m1.1768\u001b[0m  0.1104\n",
      "     20        \u001b[36m1.2459\u001b[0m        1.2088  0.1046\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1  \u001b[36m1533100.4592\u001b[0m        \u001b[32m3.9966\u001b[0m  0.1092\n",
      "      2  \u001b[36m1169606.1315\u001b[0m        \u001b[32m3.0336\u001b[0m  0.1127\n",
      "      3  \u001b[36m1041477.4356\u001b[0m        \u001b[32m2.5307\u001b[0m  0.1077\n",
      "      4   \u001b[36m831522.2894\u001b[0m        \u001b[32m2.0588\u001b[0m  0.1389\n",
      "      5   \u001b[36m638446.4876\u001b[0m        \u001b[32m1.6656\u001b[0m  0.1360\n",
      "      6   \u001b[36m557373.2548\u001b[0m        \u001b[32m1.5978\u001b[0m  0.1190\n",
      "      7   \u001b[36m510649.8110\u001b[0m        1.6060  0.1085\n",
      "      8   \u001b[36m336512.4951\u001b[0m        \u001b[32m1.5541\u001b[0m  0.1070\n",
      "      9   \u001b[36m302489.5434\u001b[0m        \u001b[32m1.4388\u001b[0m  0.1101\n",
      "     10   \u001b[36m222119.6626\u001b[0m        \u001b[32m1.4329\u001b[0m  0.1108\n",
      "     11   \u001b[36m205396.8079\u001b[0m        \u001b[32m1.4208\u001b[0m  0.1044\n",
      "     12   \u001b[36m140141.1672\u001b[0m        \u001b[32m1.4179\u001b[0m  0.1115\n",
      "     13   \u001b[36m114398.3745\u001b[0m        \u001b[32m1.4137\u001b[0m  0.1114\n",
      "     14    \u001b[36m91568.2002\u001b[0m        1.4180  0.1082\n",
      "     15    \u001b[36m81420.0180\u001b[0m        \u001b[32m1.4023\u001b[0m  0.1119\n",
      "     16    \u001b[36m65624.1182\u001b[0m        \u001b[32m1.3940\u001b[0m  0.1056\n",
      "     17    \u001b[36m59226.1898\u001b[0m        \u001b[32m1.3794\u001b[0m  0.1068\n",
      "     18    \u001b[36m45768.7236\u001b[0m        1.3867  0.1094\n",
      "     19    \u001b[36m40916.5258\u001b[0m        \u001b[32m1.3608\u001b[0m  0.1105\n",
      "     20    \u001b[36m33102.6560\u001b[0m        \u001b[32m1.3574\u001b[0m  0.1271\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m51411.0797\u001b[0m       \u001b[32m13.5096\u001b[0m  0.1076\n",
      "      2    \u001b[36m35728.0863\u001b[0m       \u001b[32m10.6083\u001b[0m  0.1047\n",
      "      3    \u001b[36m20713.4630\u001b[0m        \u001b[32m8.1825\u001b[0m  0.1105\n",
      "      4    \u001b[36m12751.3362\u001b[0m        \u001b[32m5.9306\u001b[0m  0.1673\n",
      "      5     \u001b[36m6669.9718\u001b[0m        \u001b[32m3.0724\u001b[0m  0.1200\n",
      "      6     \u001b[36m3968.3906\u001b[0m        \u001b[32m1.9559\u001b[0m  0.1103\n",
      "      7     \u001b[36m3501.7541\u001b[0m        \u001b[32m1.7956\u001b[0m  0.1097\n",
      "      8     \u001b[36m1000.6360\u001b[0m        \u001b[32m1.6339\u001b[0m  0.1101\n",
      "      9      \u001b[36m562.3308\u001b[0m        \u001b[32m1.4644\u001b[0m  0.1134\n",
      "     10      \u001b[36m240.7674\u001b[0m        \u001b[32m1.4513\u001b[0m  0.1120\n",
      "     11      \u001b[36m120.4822\u001b[0m        \u001b[32m1.3726\u001b[0m  0.1067\n",
      "     12       \u001b[36m71.7459\u001b[0m        \u001b[32m1.3035\u001b[0m  0.1088\n",
      "     13       \u001b[36m27.1431\u001b[0m        \u001b[32m1.2580\u001b[0m  0.1096\n",
      "     14       \u001b[36m12.4455\u001b[0m        \u001b[32m1.2387\u001b[0m  0.1082\n",
      "     15        \u001b[36m5.6189\u001b[0m        \u001b[32m1.2072\u001b[0m  0.1010\n",
      "     16        \u001b[36m2.8750\u001b[0m        \u001b[32m1.1740\u001b[0m  0.1047\n",
      "     17        \u001b[36m1.8953\u001b[0m        \u001b[32m1.1421\u001b[0m  0.1013\n",
      "     18        \u001b[36m1.7990\u001b[0m        1.1828  0.1032\n",
      "     19        \u001b[36m1.3780\u001b[0m        \u001b[32m1.0686\u001b[0m  0.1029\n",
      "     20        \u001b[36m1.3141\u001b[0m        1.1003  0.1040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m6183.0798\u001b[0m       \u001b[32m24.4248\u001b[0m  0.1018\n",
      "      2      \u001b[36m779.3289\u001b[0m       \u001b[32m14.3118\u001b[0m  0.1062\n",
      "      3      \u001b[36m221.2623\u001b[0m        \u001b[32m9.7710\u001b[0m  0.1004\n",
      "      4      \u001b[36m120.5411\u001b[0m        \u001b[32m6.7950\u001b[0m  0.1721\n",
      "      5       \u001b[36m28.3400\u001b[0m        \u001b[32m4.8285\u001b[0m  0.1103\n",
      "      6      223.4558        \u001b[32m4.0491\u001b[0m  0.1026\n",
      "      7       \u001b[36m18.2789\u001b[0m        \u001b[32m3.5849\u001b[0m  0.1033\n",
      "      8       46.3786        \u001b[32m3.0200\u001b[0m  0.1057\n",
      "      9       35.5719        \u001b[32m2.8179\u001b[0m  0.1038\n",
      "     10       22.9427        \u001b[32m2.5536\u001b[0m  0.1006\n",
      "     11        \u001b[36m6.2704\u001b[0m        \u001b[32m2.3860\u001b[0m  0.1029\n",
      "     12        \u001b[36m3.9930\u001b[0m        \u001b[32m2.2661\u001b[0m  0.0997\n",
      "     13        4.3453        \u001b[32m2.1603\u001b[0m  0.1015\n",
      "     14        4.7280        \u001b[32m2.0673\u001b[0m  0.1049\n",
      "     15        7.9637        \u001b[32m1.9730\u001b[0m  0.1034\n",
      "     16       17.5406        \u001b[32m1.8718\u001b[0m  0.1039\n",
      "     17       33.4420        \u001b[32m1.7981\u001b[0m  0.1051\n",
      "     18        6.4777        \u001b[32m1.7258\u001b[0m  0.1019\n",
      "     19       10.8723        \u001b[32m1.6972\u001b[0m  0.0993\n",
      "     20       57.9964        \u001b[32m1.6204\u001b[0m  0.1127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5758\u001b[0m        \u001b[32m1.2985\u001b[0m  0.1056\n",
      "      2        \u001b[36m1.4236\u001b[0m        \u001b[32m1.2400\u001b[0m  0.1042\n",
      "      3        \u001b[36m1.2280\u001b[0m        \u001b[32m1.1676\u001b[0m  0.1025\n",
      "      4        \u001b[36m1.1060\u001b[0m        \u001b[32m1.0970\u001b[0m  0.1516\n",
      "      5        \u001b[36m1.0527\u001b[0m        \u001b[32m1.0575\u001b[0m  0.1408\n",
      "      6        \u001b[36m1.0388\u001b[0m        \u001b[32m1.0434\u001b[0m  0.1180\n",
      "      7        \u001b[36m1.0054\u001b[0m        1.0559  0.1064\n",
      "      8        \u001b[36m0.9651\u001b[0m        \u001b[32m1.0087\u001b[0m  0.1059\n",
      "      9        \u001b[36m0.9539\u001b[0m        1.0466  0.1145\n",
      "     10        0.9729        1.0712  0.1205\n",
      "     11        \u001b[36m0.9238\u001b[0m        1.0103  0.1000\n",
      "     12        \u001b[36m0.9145\u001b[0m        \u001b[32m0.9951\u001b[0m  0.1028\n",
      "     13        \u001b[36m0.9108\u001b[0m        1.0044  0.1004\n",
      "     14        \u001b[36m0.9052\u001b[0m        \u001b[32m0.9888\u001b[0m  0.1031\n",
      "     15        \u001b[36m0.8906\u001b[0m        \u001b[32m0.9673\u001b[0m  0.1041\n",
      "     16        0.8959        0.9928  0.1039\n",
      "     17        \u001b[36m0.8821\u001b[0m        0.9971  0.1050\n",
      "     18        0.9276        0.9773  0.1054\n",
      "     19        0.8969        0.9970  0.1027\n",
      "     20        \u001b[36m0.8804\u001b[0m        0.9934  0.1050\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7605\u001b[0m    \u001b[32m52792.6089\u001b[0m  0.1010\n",
      "      2        \u001b[36m1.3657\u001b[0m    63500.1272  0.1031\n",
      "      3        \u001b[36m1.0967\u001b[0m    62733.9816  0.1009\n",
      "      4        \u001b[36m0.9997\u001b[0m    55195.1525  0.1204\n",
      "      5        \u001b[36m0.9613\u001b[0m    \u001b[32m44146.5548\u001b[0m  0.1415\n",
      "      6        \u001b[36m0.9321\u001b[0m    \u001b[32m37213.6414\u001b[0m  0.1130\n",
      "      7        \u001b[36m0.9081\u001b[0m    \u001b[32m35065.7485\u001b[0m  0.1145\n",
      "      8        \u001b[36m0.9048\u001b[0m    \u001b[32m32023.0498\u001b[0m  0.1030\n",
      "      9        \u001b[36m0.9017\u001b[0m    \u001b[32m26687.9400\u001b[0m  0.1052\n",
      "     10        \u001b[36m0.8837\u001b[0m    \u001b[32m25180.7770\u001b[0m  0.1070\n",
      "     11        0.8850    \u001b[32m23302.8462\u001b[0m  0.1060\n",
      "     12        \u001b[36m0.8715\u001b[0m    \u001b[32m22373.7964\u001b[0m  0.1026\n",
      "     13        0.8752    \u001b[32m22114.5847\u001b[0m  0.1035\n",
      "     14        0.8853    \u001b[32m18905.9863\u001b[0m  0.1021\n",
      "     15        0.9001    \u001b[32m16594.7232\u001b[0m  0.1021\n",
      "     16        \u001b[36m0.8519\u001b[0m    \u001b[32m14764.2486\u001b[0m  0.1090\n",
      "     17        \u001b[36m0.8392\u001b[0m    14905.0170  0.1020\n",
      "     18        \u001b[36m0.8370\u001b[0m    \u001b[32m13976.3442\u001b[0m  0.1056\n",
      "     19        \u001b[36m0.8282\u001b[0m    14001.4470  0.1062\n",
      "     20        0.8584    \u001b[32m12637.0211\u001b[0m  0.1044\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m78596.3401\u001b[0m       \u001b[32m19.2587\u001b[0m  0.1042\n",
      "      2    \u001b[36m64900.6990\u001b[0m        \u001b[32m8.5023\u001b[0m  0.1040\n",
      "      3    \u001b[36m50541.6122\u001b[0m        \u001b[32m5.5237\u001b[0m  0.1596\n",
      "      4    \u001b[36m38889.1527\u001b[0m        \u001b[32m3.9358\u001b[0m  0.1569\n",
      "      5    \u001b[36m35596.6639\u001b[0m        \u001b[32m3.0524\u001b[0m  0.1314\n",
      "      6    \u001b[36m27441.7827\u001b[0m        \u001b[32m2.4096\u001b[0m  0.1028\n",
      "      7    \u001b[36m22950.6536\u001b[0m        \u001b[32m2.0374\u001b[0m  0.1031\n",
      "      8    \u001b[36m19880.8503\u001b[0m        \u001b[32m1.7998\u001b[0m  0.1042\n",
      "      9    \u001b[36m17000.5621\u001b[0m        \u001b[32m1.5947\u001b[0m  0.1012\n",
      "     10    \u001b[36m14754.5096\u001b[0m        \u001b[32m1.4475\u001b[0m  0.1056\n",
      "     11    \u001b[36m12903.0764\u001b[0m        \u001b[32m1.4155\u001b[0m  0.1018\n",
      "     12    \u001b[36m11254.5074\u001b[0m        \u001b[32m1.2324\u001b[0m  0.1054\n",
      "     13    \u001b[36m10108.7234\u001b[0m        \u001b[32m1.2017\u001b[0m  0.1029\n",
      "     14     \u001b[36m8685.0260\u001b[0m        \u001b[32m1.1680\u001b[0m  0.1086\n",
      "     15     \u001b[36m7733.1027\u001b[0m        \u001b[32m1.1159\u001b[0m  0.1002\n",
      "     16     \u001b[36m7208.3993\u001b[0m        \u001b[32m1.0827\u001b[0m  0.1022\n",
      "     17     \u001b[36m6305.4160\u001b[0m        \u001b[32m1.0607\u001b[0m  0.1201\n",
      "     18     \u001b[36m5767.9688\u001b[0m        \u001b[32m1.0606\u001b[0m  0.1030\n",
      "     19     \u001b[36m5314.4821\u001b[0m        \u001b[32m1.0254\u001b[0m  0.1041\n",
      "     20     \u001b[36m4855.3180\u001b[0m        \u001b[32m1.0137\u001b[0m  0.1024\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m10.6406\u001b[0m    \u001b[32m69723.9608\u001b[0m  0.1086\n",
      "      2        \u001b[36m4.8680\u001b[0m    \u001b[32m65623.3320\u001b[0m  0.1065\n",
      "      3        \u001b[36m3.3910\u001b[0m    73874.5200  0.1049\n",
      "      4        \u001b[36m2.7443\u001b[0m    79537.1467  0.1254\n",
      "      5        \u001b[36m2.2607\u001b[0m    66462.2915  0.1355\n",
      "      6        \u001b[36m1.9637\u001b[0m    \u001b[32m59137.2034\u001b[0m  0.1225\n",
      "      7        \u001b[36m1.7990\u001b[0m    \u001b[32m53960.0945\u001b[0m  0.1038\n",
      "      8        \u001b[36m1.6616\u001b[0m    56172.7230  0.1041\n",
      "      9        \u001b[36m1.5978\u001b[0m    58976.2141  0.1036\n",
      "     10        \u001b[36m1.4784\u001b[0m    60786.2339  0.1049\n",
      "     11        \u001b[36m1.4156\u001b[0m    67269.4780  0.1045\n",
      "     12        \u001b[36m1.3442\u001b[0m    66139.2581  0.1099\n",
      "     13        \u001b[36m1.2872\u001b[0m    68422.9563  0.1072\n",
      "     14        \u001b[36m1.2520\u001b[0m    71361.0210  0.1053\n",
      "     15        \u001b[36m1.2162\u001b[0m    73528.0692  0.1059\n",
      "     16        \u001b[36m1.1645\u001b[0m    78021.7117  0.1139\n",
      "     17        \u001b[36m1.1409\u001b[0m    78013.5643  0.1078\n",
      "     18        \u001b[36m1.1134\u001b[0m    78720.5558  0.1122\n",
      "     19        \u001b[36m1.0837\u001b[0m    78843.3669  0.1137\n",
      "     20        \u001b[36m1.0783\u001b[0m    78683.8637  0.1059\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m3489.5489\u001b[0m       \u001b[32m22.1384\u001b[0m  0.1071\n",
      "      2      \u001b[36m961.7574\u001b[0m       \u001b[32m10.0730\u001b[0m  0.1061\n",
      "      3      \u001b[36m211.0506\u001b[0m        \u001b[32m5.2531\u001b[0m  0.1083\n",
      "      4       \u001b[36m11.7573\u001b[0m        \u001b[32m2.5773\u001b[0m  0.1400\n",
      "      5       15.9160        \u001b[32m1.6476\u001b[0m  0.1283\n",
      "      6        \u001b[36m5.1211\u001b[0m        \u001b[32m1.3919\u001b[0m  0.1043\n",
      "      7        6.5695        \u001b[32m1.2985\u001b[0m  0.1097\n",
      "      8        \u001b[36m3.4694\u001b[0m        \u001b[32m1.2336\u001b[0m  0.1012\n",
      "      9        3.8690        \u001b[32m1.1832\u001b[0m  0.1047\n",
      "     10        \u001b[36m2.8714\u001b[0m        \u001b[32m1.1437\u001b[0m  0.1070\n",
      "     11        \u001b[36m2.5785\u001b[0m        \u001b[32m1.1109\u001b[0m  0.1017\n",
      "     12        \u001b[36m2.1808\u001b[0m        \u001b[32m1.0748\u001b[0m  0.1028\n",
      "     13        2.1997        \u001b[32m1.0740\u001b[0m  0.1010\n",
      "     14        \u001b[36m1.9634\u001b[0m        \u001b[32m1.0128\u001b[0m  0.1022\n",
      "     15        \u001b[36m1.7347\u001b[0m        \u001b[32m0.9988\u001b[0m  0.1015\n",
      "     16        \u001b[36m1.6120\u001b[0m        \u001b[32m0.9932\u001b[0m  0.1161\n",
      "     17        \u001b[36m1.4955\u001b[0m        \u001b[32m0.9848\u001b[0m  0.1058\n",
      "     18        \u001b[36m1.4539\u001b[0m        \u001b[32m0.9661\u001b[0m  0.1025\n",
      "     19        \u001b[36m1.3668\u001b[0m        \u001b[32m0.9646\u001b[0m  0.1036\n",
      "     20        \u001b[36m1.3031\u001b[0m        \u001b[32m0.9579\u001b[0m  0.1022\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m125174.6797\u001b[0m       \u001b[32m10.0376\u001b[0m  0.1176\n",
      "      2    \u001b[36m31503.8734\u001b[0m        \u001b[32m6.9606\u001b[0m  0.1028\n",
      "      3     \u001b[36m5158.6497\u001b[0m        \u001b[32m4.7721\u001b[0m  0.1024\n",
      "      4      \u001b[36m213.6304\u001b[0m        \u001b[32m2.8984\u001b[0m  0.1189\n",
      "      5      274.3551        \u001b[32m1.9009\u001b[0m  0.1443\n",
      "      6      \u001b[36m175.3876\u001b[0m        \u001b[32m1.6754\u001b[0m  0.1099\n",
      "      7       \u001b[36m29.5283\u001b[0m        \u001b[32m1.5419\u001b[0m  0.1004\n",
      "      8       \u001b[36m18.0631\u001b[0m        \u001b[32m1.4226\u001b[0m  0.1078\n",
      "      9       \u001b[36m10.6863\u001b[0m        \u001b[32m1.3606\u001b[0m  0.1069\n",
      "     10        \u001b[36m2.4940\u001b[0m        \u001b[32m1.2650\u001b[0m  0.1048\n",
      "     11        \u001b[36m1.6423\u001b[0m        \u001b[32m1.1694\u001b[0m  0.1021\n",
      "     12        \u001b[36m1.4506\u001b[0m        \u001b[32m1.0814\u001b[0m  0.1050\n",
      "     13        \u001b[36m1.3291\u001b[0m        \u001b[32m0.9993\u001b[0m  0.1040\n",
      "     14       25.4073        1.0186  0.1042\n",
      "     15       34.4090        \u001b[32m0.9441\u001b[0m  0.1162\n",
      "     16        1.5684        0.9597  0.1062\n",
      "     17        1.4806        \u001b[32m0.9344\u001b[0m  0.1087\n",
      "     18        \u001b[36m1.1486\u001b[0m        0.9368  0.1078\n",
      "     19        1.3488        \u001b[32m0.8964\u001b[0m  0.1044\n",
      "     20        \u001b[36m1.1441\u001b[0m        0.9314  0.1005\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m10837.6166\u001b[0m        \u001b[32m5.3773\u001b[0m  0.0991\n",
      "      2     \u001b[36m4679.0718\u001b[0m        \u001b[32m3.5273\u001b[0m  0.1088\n",
      "      3     \u001b[36m1155.4223\u001b[0m        \u001b[32m2.8009\u001b[0m  0.1019\n",
      "      4      \u001b[36m224.1949\u001b[0m        \u001b[32m2.7482\u001b[0m  0.1029\n",
      "      5       \u001b[36m78.1046\u001b[0m        \u001b[32m2.5772\u001b[0m  0.1505\n",
      "      6       94.8501        \u001b[32m2.5527\u001b[0m  0.1281\n",
      "      7       \u001b[36m25.2556\u001b[0m        2.6377  0.1026\n",
      "      8        \u001b[36m7.6915\u001b[0m        2.7571  0.1118\n",
      "      9        \u001b[36m2.7642\u001b[0m        2.8368  0.1017\n",
      "     10        \u001b[36m1.7474\u001b[0m        2.9059  0.1020\n",
      "     11        \u001b[36m1.0722\u001b[0m        2.9221  0.1046\n",
      "     12        1.0758        2.9621  0.1027\n",
      "     13        \u001b[36m1.0205\u001b[0m        2.9785  0.1116\n",
      "     14        \u001b[36m1.0141\u001b[0m        2.9659  0.1021\n",
      "     15        \u001b[36m1.0056\u001b[0m        2.9852  0.1053\n",
      "     16        1.0068        2.9723  0.1032\n",
      "     17        \u001b[36m1.0018\u001b[0m        3.0099  0.1044\n",
      "     18        1.0040        2.9857  0.1037\n",
      "     19        1.0037        3.0213  0.1023\n",
      "     20        1.0018        2.9996  0.1035\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m167995.3723\u001b[0m     \u001b[32m3240.5752\u001b[0m  0.1068\n",
      "      2    \u001b[36m61516.4654\u001b[0m     \u001b[32m1676.1110\u001b[0m  0.1166\n",
      "      3    \u001b[36m10468.1641\u001b[0m      \u001b[32m929.3850\u001b[0m  0.1118\n",
      "      4     \u001b[36m2038.6122\u001b[0m      \u001b[32m570.7078\u001b[0m  0.1042\n",
      "      5      \u001b[36m635.9875\u001b[0m      \u001b[32m391.3928\u001b[0m  0.1486\n",
      "      6      \u001b[36m437.5883\u001b[0m      \u001b[32m284.1503\u001b[0m  0.1353\n",
      "      7      \u001b[36m215.8775\u001b[0m      \u001b[32m216.0146\u001b[0m  0.1120\n",
      "      8      \u001b[36m161.8481\u001b[0m      \u001b[32m171.1270\u001b[0m  0.1064\n",
      "      9      \u001b[36m127.5144\u001b[0m      \u001b[32m138.6475\u001b[0m  0.1061\n",
      "     10      \u001b[36m102.2195\u001b[0m      \u001b[32m111.6998\u001b[0m  0.1103\n",
      "     11       \u001b[36m82.1280\u001b[0m       \u001b[32m91.5662\u001b[0m  0.1302\n",
      "     12       \u001b[36m66.8757\u001b[0m       \u001b[32m75.7591\u001b[0m  0.1117\n",
      "     13       \u001b[36m55.2434\u001b[0m       \u001b[32m62.6140\u001b[0m  0.1115\n",
      "     14       \u001b[36m45.6172\u001b[0m       \u001b[32m52.4624\u001b[0m  0.1099\n",
      "     15       \u001b[36m38.1256\u001b[0m       \u001b[32m44.2111\u001b[0m  0.1070\n",
      "     16       \u001b[36m31.9948\u001b[0m       \u001b[32m37.7707\u001b[0m  0.1053\n",
      "     17       \u001b[36m27.1362\u001b[0m       \u001b[32m32.6726\u001b[0m  0.1066\n",
      "     18       \u001b[36m23.3763\u001b[0m       \u001b[32m28.3706\u001b[0m  0.1084\n",
      "     19       \u001b[36m20.3508\u001b[0m       \u001b[32m24.7918\u001b[0m  0.1071\n",
      "     20       \u001b[36m17.8376\u001b[0m       \u001b[32m21.9318\u001b[0m  0.1057\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m811.7379\u001b[0m      \u001b[32m155.5626\u001b[0m  0.1236\n",
      "      2      \u001b[36m206.9114\u001b[0m       \u001b[32m79.9992\u001b[0m  0.1048\n",
      "      3       \u001b[36m71.0452\u001b[0m       \u001b[32m42.8225\u001b[0m  0.1069\n",
      "      4       \u001b[36m39.0607\u001b[0m       \u001b[32m22.2926\u001b[0m  0.1271\n",
      "      5       \u001b[36m23.0567\u001b[0m       \u001b[32m10.8168\u001b[0m  0.1423\n",
      "      6       \u001b[36m11.2444\u001b[0m        \u001b[32m5.5583\u001b[0m  0.1281\n",
      "      7       \u001b[36m10.6850\u001b[0m        \u001b[32m3.4153\u001b[0m  0.1075\n",
      "      8        \u001b[36m5.4732\u001b[0m        \u001b[32m2.6768\u001b[0m  0.1261\n",
      "      9        6.4988        \u001b[32m2.3878\u001b[0m  0.1121\n",
      "     10        5.8310        \u001b[32m2.2063\u001b[0m  0.1287\n",
      "     11        \u001b[36m3.7944\u001b[0m        \u001b[32m2.0843\u001b[0m  0.1097\n",
      "     12        \u001b[36m3.6457\u001b[0m        \u001b[32m1.9710\u001b[0m  0.1118\n",
      "     13        3.8408        \u001b[32m1.8730\u001b[0m  0.1031\n",
      "     14        4.5800        \u001b[32m1.7872\u001b[0m  0.1070\n",
      "     15        \u001b[36m3.0931\u001b[0m        \u001b[32m1.7001\u001b[0m  0.1206\n",
      "     16       18.2632        \u001b[32m1.6780\u001b[0m  0.1174\n",
      "     17       36.7576        \u001b[32m1.5615\u001b[0m  0.1062\n",
      "     18      118.0674        \u001b[32m1.5234\u001b[0m  0.1060\n",
      "     19      151.3566        \u001b[32m1.4846\u001b[0m  0.1057\n",
      "     20       18.7557        \u001b[32m1.4455\u001b[0m  0.1024\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.7776\u001b[0m        \u001b[32m1.0476\u001b[0m  0.1221\n",
      "      2        \u001b[36m1.2214\u001b[0m        \u001b[32m0.9032\u001b[0m  0.1116\n",
      "      3        \u001b[36m1.1207\u001b[0m        \u001b[32m0.8771\u001b[0m  0.1350\n",
      "      4        \u001b[36m1.0522\u001b[0m        \u001b[32m0.8403\u001b[0m  0.1518\n",
      "      5        \u001b[36m1.0424\u001b[0m        0.8483  0.1218\n",
      "      6        \u001b[36m1.0244\u001b[0m        0.8425  0.1220\n",
      "      7        \u001b[36m1.0113\u001b[0m        0.8446  0.1136\n",
      "      8        \u001b[36m1.0034\u001b[0m        0.8444  0.1188\n",
      "      9        \u001b[36m0.9981\u001b[0m        0.8405  0.1205\n",
      "     10        \u001b[36m0.9922\u001b[0m        0.8530  0.1253\n",
      "     11        \u001b[36m0.9852\u001b[0m        0.8460  0.1080\n",
      "     12        \u001b[36m0.9799\u001b[0m        0.8423  0.1081\n",
      "     13        0.9879        0.8436  0.1070\n",
      "     14        0.9923        0.8497  0.1083\n",
      "     15        \u001b[36m0.9750\u001b[0m        0.8493  0.1081\n",
      "     16        \u001b[36m0.9735\u001b[0m        \u001b[32m0.8371\u001b[0m  0.1061\n",
      "     17        \u001b[36m0.9611\u001b[0m        0.8421  0.1081\n",
      "     18        \u001b[36m0.9595\u001b[0m        0.8512  0.1073\n",
      "     19        \u001b[36m0.9585\u001b[0m        0.8478  0.1089\n",
      "     20        \u001b[36m0.9562\u001b[0m        0.8432  0.1052\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m175263.3531\u001b[0m        \u001b[32m1.7573\u001b[0m  0.1078\n",
      "      2    \u001b[36m74983.2161\u001b[0m        \u001b[32m1.1913\u001b[0m  0.1345\n",
      "      3    \u001b[36m46053.1256\u001b[0m        1.2830  0.1474\n",
      "      4    \u001b[36m19494.4009\u001b[0m        \u001b[32m1.1231\u001b[0m  0.1250\n",
      "      5     \u001b[36m2632.1998\u001b[0m        \u001b[32m1.0478\u001b[0m  0.1041\n",
      "      6     \u001b[36m1555.4036\u001b[0m        \u001b[32m1.0434\u001b[0m  0.1116\n",
      "      7      \u001b[36m121.5533\u001b[0m        \u001b[32m1.0365\u001b[0m  0.1087\n",
      "      8        \u001b[36m1.1003\u001b[0m        \u001b[32m1.0272\u001b[0m  0.1091\n",
      "      9        4.1782        \u001b[32m1.0253\u001b[0m  0.1047\n",
      "     10        \u001b[36m1.0005\u001b[0m        \u001b[32m1.0185\u001b[0m  0.1012\n",
      "     11      972.7139        1.0417  0.1054\n",
      "     12      401.1140        \u001b[32m1.0084\u001b[0m  0.1212\n",
      "     13      104.9920        \u001b[32m0.9907\u001b[0m  0.1070\n",
      "     14       69.5396        \u001b[32m0.9832\u001b[0m  0.1110\n",
      "     15        8.6513        \u001b[32m0.9827\u001b[0m  0.1078\n",
      "     16       16.9420        \u001b[32m0.9722\u001b[0m  0.1093\n",
      "     17        8.4585        \u001b[32m0.9705\u001b[0m  0.1043\n",
      "     18        1.5649        \u001b[32m0.9618\u001b[0m  0.1171\n",
      "     19        1.4475        \u001b[32m0.9558\u001b[0m  0.1231\n",
      "     20        1.0038        0.9558  0.1132\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.5435\u001b[0m        \u001b[32m7.4665\u001b[0m  0.1200\n",
      "      2        \u001b[36m3.8797\u001b[0m        \u001b[32m4.0759\u001b[0m  0.1602\n",
      "      3        \u001b[36m3.5446\u001b[0m        \u001b[32m2.3984\u001b[0m  0.1092\n",
      "      4        \u001b[36m2.1101\u001b[0m        \u001b[32m2.3750\u001b[0m  0.1062\n",
      "      5        \u001b[36m1.9788\u001b[0m        \u001b[32m1.9494\u001b[0m  0.1042\n",
      "      6        \u001b[36m1.7554\u001b[0m        \u001b[32m1.7850\u001b[0m  0.1028\n",
      "      7        \u001b[36m1.6122\u001b[0m        \u001b[32m1.6427\u001b[0m  0.1005\n",
      "      8        \u001b[36m1.4891\u001b[0m        \u001b[32m1.4917\u001b[0m  0.1074\n",
      "      9        \u001b[36m1.3873\u001b[0m        \u001b[32m1.3969\u001b[0m  0.1027\n",
      "     10        \u001b[36m1.3218\u001b[0m        \u001b[32m1.2996\u001b[0m  0.1029\n",
      "     11        \u001b[36m1.2695\u001b[0m        \u001b[32m1.2749\u001b[0m  0.1069\n",
      "     12        \u001b[36m1.2061\u001b[0m        \u001b[32m1.2116\u001b[0m  0.1149\n",
      "     13        \u001b[36m1.1713\u001b[0m        \u001b[32m1.1938\u001b[0m  0.1147\n",
      "     14        \u001b[36m1.1408\u001b[0m        \u001b[32m1.1602\u001b[0m  0.1057\n",
      "     15        \u001b[36m1.0987\u001b[0m        \u001b[32m1.1457\u001b[0m  0.1083\n",
      "     16        \u001b[36m1.0727\u001b[0m        \u001b[32m1.1294\u001b[0m  0.1082\n",
      "     17        \u001b[36m1.0518\u001b[0m        1.1398  0.1075\n",
      "     18        \u001b[36m1.0182\u001b[0m        \u001b[32m1.0963\u001b[0m  0.1130\n",
      "     19        \u001b[36m1.0029\u001b[0m        \u001b[32m1.0891\u001b[0m  0.1094\n",
      "     20        \u001b[36m0.9880\u001b[0m        1.1031  0.1121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m268823.0104\u001b[0m        \u001b[32m2.8090\u001b[0m  0.1281\n",
      "      2   \u001b[36m180435.5933\u001b[0m        \u001b[32m1.8836\u001b[0m  0.1411\n",
      "      3    \u001b[36m98976.0240\u001b[0m        \u001b[32m1.6402\u001b[0m  0.1096\n",
      "      4    \u001b[36m47597.8296\u001b[0m        \u001b[32m1.5099\u001b[0m  0.1115\n",
      "      5    \u001b[36m36066.3586\u001b[0m        1.5549  0.1079\n",
      "      6    \u001b[36m11210.5381\u001b[0m        \u001b[32m1.4874\u001b[0m  0.1085\n",
      "      7     \u001b[36m4744.0869\u001b[0m        \u001b[32m1.4499\u001b[0m  0.1159\n",
      "      8     \u001b[36m2110.2122\u001b[0m        \u001b[32m1.4109\u001b[0m  0.1196\n",
      "      9      \u001b[36m852.3603\u001b[0m        \u001b[32m1.3686\u001b[0m  0.1047\n",
      "     10      \u001b[36m315.1543\u001b[0m        \u001b[32m1.3281\u001b[0m  0.1017\n",
      "     11       \u001b[36m31.4762\u001b[0m        \u001b[32m1.3016\u001b[0m  0.1160\n",
      "     12       54.5909        1.3224  0.1096\n",
      "     13        \u001b[36m3.4558\u001b[0m        \u001b[32m1.2980\u001b[0m  0.1163\n",
      "     14        3.5848        1.3047  0.1067\n",
      "     15        \u001b[36m2.1914\u001b[0m        \u001b[32m1.2871\u001b[0m  0.1496\n",
      "     16        \u001b[36m1.5128\u001b[0m        \u001b[32m1.2716\u001b[0m  0.1203\n",
      "     17        \u001b[36m1.1618\u001b[0m        \u001b[32m1.2550\u001b[0m  0.1239\n",
      "     18        \u001b[36m1.1409\u001b[0m        1.2651  0.1119\n",
      "     19        \u001b[36m1.1327\u001b[0m        \u001b[32m1.2550\u001b[0m  0.1105\n",
      "     20        \u001b[36m1.1271\u001b[0m        \u001b[32m1.2507\u001b[0m  0.1291\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m379727.1811\u001b[0m        \u001b[32m2.5037\u001b[0m  0.1362\n",
      "      2   \u001b[36m310341.2084\u001b[0m        \u001b[32m1.9126\u001b[0m  0.1592\n",
      "      3   \u001b[36m205013.8421\u001b[0m        \u001b[32m1.5646\u001b[0m  0.1105\n",
      "      4   \u001b[36m154776.7016\u001b[0m        \u001b[32m1.3778\u001b[0m  0.1104\n",
      "      5   \u001b[36m104893.1507\u001b[0m        \u001b[32m1.2560\u001b[0m  0.1099\n",
      "      6    \u001b[36m79848.1785\u001b[0m        \u001b[32m1.2307\u001b[0m  0.1088\n",
      "      7    \u001b[36m55221.3115\u001b[0m        \u001b[32m1.1567\u001b[0m  0.1114\n",
      "      8    \u001b[36m48080.6311\u001b[0m        1.1684  0.1068\n",
      "      9    \u001b[36m26101.9490\u001b[0m        1.1596  0.1032\n",
      "     10    \u001b[36m17579.5166\u001b[0m        1.1714  0.1187\n",
      "     11    \u001b[36m11740.3515\u001b[0m        \u001b[32m1.1489\u001b[0m  0.1174\n",
      "     12     \u001b[36m6153.7239\u001b[0m        1.1564  0.1119\n",
      "     13     \u001b[36m2514.9114\u001b[0m        \u001b[32m1.0955\u001b[0m  0.1052\n",
      "     14     \u001b[36m2341.7247\u001b[0m        1.1512  0.1088\n",
      "     15      \u001b[36m522.5366\u001b[0m        1.1726  0.1095\n",
      "     16      \u001b[36m108.7553\u001b[0m        1.1063  0.1038\n",
      "     17       \u001b[36m71.8657\u001b[0m        \u001b[32m1.0908\u001b[0m  0.1044\n",
      "     18       \u001b[36m10.6644\u001b[0m        \u001b[32m1.0881\u001b[0m  0.1015\n",
      "     19        \u001b[36m2.6982\u001b[0m        \u001b[32m1.0472\u001b[0m  0.1275\n",
      "     20        \u001b[36m1.8934\u001b[0m        \u001b[32m1.0239\u001b[0m  0.1468\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m11.9449\u001b[0m   \u001b[32m231114.9299\u001b[0m  0.1132\n",
      "      2        \u001b[36m6.9396\u001b[0m    \u001b[32m14556.8518\u001b[0m  0.1075\n",
      "      3        \u001b[36m4.9806\u001b[0m     \u001b[32m2402.4591\u001b[0m  0.1041\n",
      "      4        \u001b[36m3.5796\u001b[0m    52528.5709  0.1030\n",
      "      5        \u001b[36m2.3181\u001b[0m   140737.5861  0.1075\n",
      "      6        \u001b[36m1.7964\u001b[0m   210799.8232  0.1045\n",
      "      7        \u001b[36m1.5577\u001b[0m   303878.7816  0.1051\n",
      "      8        \u001b[36m1.3652\u001b[0m   507700.8889  0.1045\n",
      "      9        \u001b[36m1.2494\u001b[0m   766307.6363  0.1147\n",
      "     10        \u001b[36m1.1418\u001b[0m  1115061.4237  0.1291\n",
      "     11        \u001b[36m1.0748\u001b[0m  1558924.1300  0.1097\n",
      "     12        \u001b[36m1.0295\u001b[0m  1690191.0778  0.1028\n",
      "     13        \u001b[36m1.0040\u001b[0m  1940774.2262  0.1033\n",
      "     14        \u001b[36m0.9868\u001b[0m  2113215.7418  0.1020\n",
      "     15        \u001b[36m0.9703\u001b[0m  2078028.3296  0.1019\n",
      "     16        0.9776  2410496.7779  0.1036\n",
      "     17        0.9715  2404123.6099  0.1019\n",
      "     18        \u001b[36m0.9691\u001b[0m  2250519.1735  0.1029\n",
      "     19        \u001b[36m0.9622\u001b[0m  2467630.6074  0.1130\n",
      "     20        \u001b[36m0.9507\u001b[0m  2401494.6216  0.1496\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m189.8406\u001b[0m        \u001b[32m2.8245\u001b[0m  0.1113\n",
      "      2       \u001b[36m59.2212\u001b[0m        \u001b[32m2.7725\u001b[0m  0.1034\n",
      "      3       \u001b[36m18.6410\u001b[0m        \u001b[32m2.2368\u001b[0m  0.1049\n",
      "      4        \u001b[36m7.3448\u001b[0m        2.2541  0.1001\n",
      "      5        \u001b[36m2.9106\u001b[0m        2.4408  0.1025\n",
      "      6        \u001b[36m2.7719\u001b[0m        2.8434  0.1020\n",
      "      7        \u001b[36m1.5458\u001b[0m        2.9004  0.1038\n",
      "      8        \u001b[36m1.3202\u001b[0m        3.0357  0.1115\n",
      "      9        \u001b[36m1.1356\u001b[0m        3.0278  0.1048\n",
      "     10        \u001b[36m1.0585\u001b[0m        3.0121  0.1048\n",
      "     11        \u001b[36m1.0154\u001b[0m        3.0195  0.1107\n",
      "     12        \u001b[36m0.9815\u001b[0m        3.0472  0.1062\n",
      "     13        \u001b[36m0.9682\u001b[0m        3.0481  0.1015\n",
      "     14        \u001b[36m0.9608\u001b[0m        3.0820  0.1010\n",
      "     15        \u001b[36m0.9539\u001b[0m        3.0073  0.1064\n",
      "     16        \u001b[36m0.9484\u001b[0m        3.0102  0.1006\n",
      "     17        \u001b[36m0.9475\u001b[0m        2.9791  0.1028\n",
      "     18        \u001b[36m0.9472\u001b[0m        2.9581  0.1039\n",
      "     19        0.9492        2.9187  0.1077\n",
      "     20        \u001b[36m0.9414\u001b[0m        2.9096  0.1494\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.8256\u001b[0m   \u001b[32m101123.2507\u001b[0m  0.1224\n",
      "      2       \u001b[36m71.2057\u001b[0m   127581.7576  0.1055\n",
      "      3       \u001b[36m29.3310\u001b[0m    \u001b[32m35211.5806\u001b[0m  0.1054\n",
      "      4       \u001b[36m13.9687\u001b[0m     \u001b[32m4248.9069\u001b[0m  0.1211\n",
      "      5        \u001b[36m7.3430\u001b[0m      \u001b[32m805.3069\u001b[0m  0.1111\n",
      "      6        \u001b[36m4.5718\u001b[0m     2324.2861  0.1184\n",
      "      7        \u001b[36m3.2651\u001b[0m     4171.1222  0.1103\n",
      "      8        \u001b[36m2.6891\u001b[0m     2996.6455  0.1091\n",
      "      9        \u001b[36m2.4357\u001b[0m     3101.0564  0.1045\n",
      "     10        \u001b[36m2.2380\u001b[0m     3683.9540  0.1067\n",
      "     11        \u001b[36m2.1000\u001b[0m     4392.5099  0.1054\n",
      "     12        \u001b[36m2.0015\u001b[0m     5424.1134  0.1037\n",
      "     13        \u001b[36m1.9433\u001b[0m    10223.3850  0.1030\n",
      "     14        \u001b[36m1.8678\u001b[0m     9728.1446  0.1001\n",
      "     15        \u001b[36m1.7887\u001b[0m    15576.4209  0.1042\n",
      "     16        \u001b[36m1.7355\u001b[0m    20398.1796  0.1031\n",
      "     17        \u001b[36m1.6799\u001b[0m    25856.7925  0.1083\n",
      "     18        \u001b[36m1.6349\u001b[0m    30505.9531  0.1000\n",
      "     19        \u001b[36m1.5951\u001b[0m    36110.9023  0.1070\n",
      "     20        \u001b[36m1.5502\u001b[0m    48365.3650  0.1435\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m212834.7549\u001b[0m        \u001b[32m2.6671\u001b[0m  0.1260\n",
      "      2   \u001b[36m173999.6538\u001b[0m        \u001b[32m2.1564\u001b[0m  0.1018\n",
      "      3   \u001b[36m133704.7041\u001b[0m        \u001b[32m1.6105\u001b[0m  0.1032\n",
      "      4    \u001b[36m98803.2262\u001b[0m        \u001b[32m1.4111\u001b[0m  0.1096\n",
      "      5    \u001b[36m80726.4229\u001b[0m        \u001b[32m1.3430\u001b[0m  0.1064\n",
      "      6    \u001b[36m64650.8258\u001b[0m        \u001b[32m1.2918\u001b[0m  0.1029\n",
      "      7    \u001b[36m49888.2481\u001b[0m        \u001b[32m1.2628\u001b[0m  0.1044\n",
      "      8    \u001b[36m40017.2175\u001b[0m        1.2705  0.0993\n",
      "      9    \u001b[36m33819.1428\u001b[0m        \u001b[32m1.2478\u001b[0m  0.1062\n",
      "     10    \u001b[36m29107.3139\u001b[0m        1.2635  0.1040\n",
      "     11    \u001b[36m19136.2189\u001b[0m        \u001b[32m1.2285\u001b[0m  0.1021\n",
      "     12    \u001b[36m16110.4775\u001b[0m        1.2733  0.1054\n",
      "     13    \u001b[36m11741.0878\u001b[0m        1.2710  0.1047\n",
      "     14     \u001b[36m9565.0063\u001b[0m        1.2685  0.1025\n",
      "     15     \u001b[36m7076.2487\u001b[0m        1.2672  0.1100\n",
      "     16     \u001b[36m5094.6534\u001b[0m        1.2394  0.1048\n",
      "     17     \u001b[36m3790.8420\u001b[0m        \u001b[32m1.2241\u001b[0m  0.1224\n",
      "     18     \u001b[36m2756.8499\u001b[0m        \u001b[32m1.1973\u001b[0m  0.1059\n",
      "     19     \u001b[36m1917.8681\u001b[0m        \u001b[32m1.1791\u001b[0m  0.1056\n",
      "     20     \u001b[36m1447.0356\u001b[0m        \u001b[32m1.1705\u001b[0m  0.1416\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m377.1881\u001b[0m       \u001b[32m46.8507\u001b[0m  0.1472\n",
      "      2       \u001b[36m70.9538\u001b[0m        \u001b[32m9.2966\u001b[0m  0.1121\n",
      "      3       \u001b[36m33.0375\u001b[0m        \u001b[32m8.7567\u001b[0m  0.1137\n",
      "      4       \u001b[36m13.0839\u001b[0m        \u001b[32m4.3485\u001b[0m  0.1059\n",
      "      5        \u001b[36m9.5975\u001b[0m        \u001b[32m2.6846\u001b[0m  0.1090\n",
      "      6        \u001b[36m8.6041\u001b[0m        \u001b[32m2.0509\u001b[0m  0.1038\n",
      "      7        \u001b[36m6.9945\u001b[0m        \u001b[32m1.7935\u001b[0m  0.1065\n",
      "      8        \u001b[36m5.7645\u001b[0m        \u001b[32m1.6509\u001b[0m  0.1030\n",
      "      9        \u001b[36m5.1452\u001b[0m        \u001b[32m1.5479\u001b[0m  0.1030\n",
      "     10        \u001b[36m4.6360\u001b[0m        \u001b[32m1.4712\u001b[0m  0.1036\n",
      "     11        \u001b[36m4.1984\u001b[0m        \u001b[32m1.4407\u001b[0m  0.1060\n",
      "     12        \u001b[36m3.9434\u001b[0m        \u001b[32m1.4060\u001b[0m  0.1165\n",
      "     13        \u001b[36m3.5470\u001b[0m        \u001b[32m1.3505\u001b[0m  0.1052\n",
      "     14        \u001b[36m3.3193\u001b[0m        \u001b[32m1.3187\u001b[0m  0.1319\n",
      "     15        \u001b[36m3.1015\u001b[0m        1.3462  0.1121\n",
      "     16        \u001b[36m2.9259\u001b[0m        \u001b[32m1.2722\u001b[0m  0.1106\n",
      "     17        \u001b[36m2.6891\u001b[0m        \u001b[32m1.2540\u001b[0m  0.1087\n",
      "     18        \u001b[36m2.5569\u001b[0m        \u001b[32m1.2395\u001b[0m  0.1103\n",
      "     19        \u001b[36m2.3875\u001b[0m        \u001b[32m1.2012\u001b[0m  0.1049\n",
      "     20        \u001b[36m2.2416\u001b[0m        1.2097  0.1853\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m659368.4346\u001b[0m       \u001b[32m25.8532\u001b[0m  0.1116\n",
      "      2   \u001b[36m404266.1739\u001b[0m       \u001b[32m12.0473\u001b[0m  0.1025\n",
      "      3   \u001b[36m271166.7408\u001b[0m        \u001b[32m9.3654\u001b[0m  0.1044\n",
      "      4   \u001b[36m146672.3503\u001b[0m        \u001b[32m6.3710\u001b[0m  0.1085\n",
      "      5   \u001b[36m103581.8849\u001b[0m        \u001b[32m3.8831\u001b[0m  0.1088\n",
      "      6    \u001b[36m35927.2186\u001b[0m        \u001b[32m2.1593\u001b[0m  0.1103\n",
      "      7    \u001b[36m16294.7586\u001b[0m        \u001b[32m1.9592\u001b[0m  0.1075\n",
      "      8     \u001b[36m2614.9283\u001b[0m        \u001b[32m1.7782\u001b[0m  0.1097\n",
      "      9      \u001b[36m695.4200\u001b[0m        \u001b[32m1.7022\u001b[0m  0.1312\n",
      "     10      \u001b[36m217.8001\u001b[0m        \u001b[32m1.5807\u001b[0m  0.1071\n",
      "     11        \u001b[36m2.2377\u001b[0m        \u001b[32m1.5068\u001b[0m  0.1087\n",
      "     12        5.4889        \u001b[32m1.4593\u001b[0m  0.1190\n",
      "     13        3.9604        \u001b[32m1.4277\u001b[0m  0.1583\n",
      "     14        \u001b[36m1.6645\u001b[0m        \u001b[32m1.4047\u001b[0m  0.1238\n",
      "     15        \u001b[36m1.5940\u001b[0m        \u001b[32m1.3907\u001b[0m  0.1107\n",
      "     16        \u001b[36m1.5688\u001b[0m        \u001b[32m1.3788\u001b[0m  0.1265\n",
      "     17        \u001b[36m1.5544\u001b[0m        \u001b[32m1.3670\u001b[0m  0.1183\n",
      "     18        \u001b[36m1.5271\u001b[0m        \u001b[32m1.3560\u001b[0m  0.1309\n",
      "     19        \u001b[36m1.5153\u001b[0m        \u001b[32m1.3432\u001b[0m  0.1474\n",
      "     20        \u001b[36m1.5004\u001b[0m        \u001b[32m1.3342\u001b[0m  0.1126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m89.7142\u001b[0m       \u001b[32m36.8837\u001b[0m  0.1074\n",
      "      2       \u001b[36m16.7862\u001b[0m        \u001b[32m3.2223\u001b[0m  0.1135\n",
      "      3        \u001b[36m2.3372\u001b[0m        3.6315  0.1060\n",
      "      4        \u001b[36m2.0709\u001b[0m        \u001b[32m1.9696\u001b[0m  0.1089\n",
      "      5        \u001b[36m1.4991\u001b[0m        \u001b[32m1.6756\u001b[0m  0.1051\n",
      "      6        \u001b[36m1.3046\u001b[0m        \u001b[32m1.5665\u001b[0m  0.1097\n",
      "      7        \u001b[36m1.1802\u001b[0m        \u001b[32m1.4872\u001b[0m  0.1092\n",
      "      8        \u001b[36m1.1336\u001b[0m        \u001b[32m1.3922\u001b[0m  0.1091\n",
      "      9        \u001b[36m1.0882\u001b[0m        \u001b[32m1.3232\u001b[0m  0.1079\n",
      "     10        \u001b[36m1.0418\u001b[0m        \u001b[32m1.1952\u001b[0m  0.1096\n",
      "     11        \u001b[36m1.0190\u001b[0m        \u001b[32m1.1173\u001b[0m  0.1107\n",
      "     12        \u001b[36m0.9949\u001b[0m        \u001b[32m1.0905\u001b[0m  0.1055\n",
      "     13        \u001b[36m0.9904\u001b[0m        \u001b[32m1.0355\u001b[0m  0.1239\n",
      "     14        \u001b[36m0.9703\u001b[0m        1.0434  0.1165\n",
      "     15        \u001b[36m0.9681\u001b[0m        \u001b[32m0.9964\u001b[0m  0.1079\n",
      "     16        \u001b[36m0.9574\u001b[0m        0.9997  0.1063\n",
      "     17        \u001b[36m0.9556\u001b[0m        \u001b[32m0.9706\u001b[0m  0.1072\n",
      "     18        \u001b[36m0.9510\u001b[0m        0.9850  0.1680\n",
      "     19        0.9522        \u001b[32m0.9668\u001b[0m  0.1124\n",
      "     20        \u001b[36m0.9429\u001b[0m        0.9734  0.1233\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m205819.2611\u001b[0m        \u001b[32m2.6666\u001b[0m  0.1104\n",
      "      2   \u001b[36m147887.6815\u001b[0m        \u001b[32m2.1651\u001b[0m  0.1088\n",
      "      3   \u001b[36m111372.0654\u001b[0m        2.3253  0.1199\n",
      "      4    \u001b[36m62113.6501\u001b[0m        \u001b[32m2.1227\u001b[0m  0.1108\n",
      "      5    \u001b[36m43565.9880\u001b[0m        2.1470  0.1116\n",
      "      6    \u001b[36m26426.0485\u001b[0m        \u001b[32m2.0228\u001b[0m  0.1102\n",
      "      7    \u001b[36m18765.2759\u001b[0m        2.2353  0.1064\n",
      "      8     \u001b[36m9432.0920\u001b[0m        2.2451  0.1104\n",
      "      9     \u001b[36m5783.4932\u001b[0m        2.3725  0.1184\n",
      "     10     \u001b[36m3039.2074\u001b[0m        2.3346  0.1246\n",
      "     11     \u001b[36m1820.1438\u001b[0m        2.3144  0.1214\n",
      "     12     2542.8704        2.3208  0.1131\n",
      "     13      \u001b[36m351.9960\u001b[0m        2.1531  0.1080\n",
      "     14       \u001b[36m59.9724\u001b[0m        \u001b[32m1.8651\u001b[0m  0.1069\n",
      "     15       \u001b[36m25.8841\u001b[0m        \u001b[32m1.6534\u001b[0m  0.1086\n",
      "     16       \u001b[36m11.6907\u001b[0m        \u001b[32m1.5121\u001b[0m  0.1091\n",
      "     17        \u001b[36m5.1702\u001b[0m        \u001b[32m1.4056\u001b[0m  0.1719\n",
      "     18        \u001b[36m3.0598\u001b[0m        \u001b[32m1.3704\u001b[0m  0.1191\n",
      "     19        \u001b[36m2.4622\u001b[0m        \u001b[32m1.3152\u001b[0m  0.1110\n",
      "     20        \u001b[36m2.2204\u001b[0m        \u001b[32m1.2975\u001b[0m  0.1143\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m50842.9925\u001b[0m        \u001b[32m9.3768\u001b[0m  0.1066\n",
      "      2    \u001b[36m30737.4840\u001b[0m        \u001b[32m5.2360\u001b[0m  0.1180\n",
      "      3     \u001b[36m4443.8671\u001b[0m        \u001b[32m3.6961\u001b[0m  0.1090\n",
      "      4     \u001b[36m1086.9268\u001b[0m        \u001b[32m2.6002\u001b[0m  0.1084\n",
      "      5       \u001b[36m64.2891\u001b[0m        \u001b[32m1.8400\u001b[0m  0.1112\n",
      "      6        \u001b[36m2.0033\u001b[0m        \u001b[32m1.5785\u001b[0m  0.1114\n",
      "      7       16.9603        \u001b[32m1.5039\u001b[0m  0.1214\n",
      "      8        8.0467        \u001b[32m1.5022\u001b[0m  0.1105\n",
      "      9        \u001b[36m1.9143\u001b[0m        \u001b[32m1.4718\u001b[0m  0.1056\n",
      "     10        \u001b[36m1.8235\u001b[0m        \u001b[32m1.4535\u001b[0m  0.1253\n",
      "     11        \u001b[36m1.5087\u001b[0m        \u001b[32m1.4398\u001b[0m  0.1056\n",
      "     12        \u001b[36m1.4369\u001b[0m        \u001b[32m1.4297\u001b[0m  0.1085\n",
      "     13        \u001b[36m1.4041\u001b[0m        \u001b[32m1.4219\u001b[0m  0.1008\n",
      "     14        \u001b[36m1.3514\u001b[0m        \u001b[32m1.4095\u001b[0m  0.1018\n",
      "     15        \u001b[36m1.3190\u001b[0m        \u001b[32m1.3963\u001b[0m  0.1036\n",
      "     16        \u001b[36m1.2644\u001b[0m        \u001b[32m1.3885\u001b[0m  0.1755\n",
      "     17        \u001b[36m1.2396\u001b[0m        \u001b[32m1.3844\u001b[0m  0.1396\n",
      "     18        \u001b[36m1.1960\u001b[0m        \u001b[32m1.3691\u001b[0m  0.1105\n",
      "     19        \u001b[36m1.1690\u001b[0m        \u001b[32m1.3578\u001b[0m  0.1073\n",
      "     20        \u001b[36m1.1421\u001b[0m        \u001b[32m1.3496\u001b[0m  0.1099\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m71.6098\u001b[0m        \u001b[32m4.0481\u001b[0m  0.1134\n",
      "      2       \u001b[36m27.0503\u001b[0m        \u001b[32m3.0786\u001b[0m  0.1145\n",
      "      3       29.0116        \u001b[32m2.6609\u001b[0m  0.1107\n",
      "      4       \u001b[36m23.8524\u001b[0m        \u001b[32m2.0630\u001b[0m  0.1164\n",
      "      5        \u001b[36m7.8795\u001b[0m        \u001b[32m1.6344\u001b[0m  0.1148\n",
      "      6        \u001b[36m6.1194\u001b[0m        \u001b[32m1.3126\u001b[0m  0.1128\n",
      "      7        \u001b[36m5.2478\u001b[0m        \u001b[32m1.1981\u001b[0m  0.1072\n",
      "      8        \u001b[36m3.2386\u001b[0m        \u001b[32m1.1193\u001b[0m  0.1075\n",
      "      9        \u001b[36m2.1609\u001b[0m        \u001b[32m1.1150\u001b[0m  0.1064\n",
      "     10        2.6787        \u001b[32m1.0778\u001b[0m  0.1057\n",
      "     11        2.8800        1.0824  0.1077\n",
      "     12        7.2821        \u001b[32m1.0682\u001b[0m  0.1070\n",
      "     13        \u001b[36m1.6488\u001b[0m        \u001b[32m1.0427\u001b[0m  0.1059\n",
      "     14       14.9236        \u001b[32m1.0252\u001b[0m  0.1077\n",
      "     15       45.3444        1.0633  0.1344\n",
      "     16       78.1637        \u001b[32m1.0051\u001b[0m  0.1512\n",
      "     17       68.9487        1.0155  0.1176\n",
      "     18        5.5218        1.0223  0.1068\n",
      "     19        3.5734        \u001b[32m0.9932\u001b[0m  0.1167\n",
      "     20        \u001b[36m1.1828\u001b[0m        1.0020  0.1089\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m142.5731\u001b[0m       \u001b[32m90.5246\u001b[0m  0.1038\n",
      "      2       \u001b[36m55.9225\u001b[0m       \u001b[32m39.6172\u001b[0m  0.1124\n",
      "      3       \u001b[36m25.9979\u001b[0m       \u001b[32m18.7419\u001b[0m  0.1154\n",
      "      4       \u001b[36m12.9926\u001b[0m       \u001b[32m10.4474\u001b[0m  0.1089\n",
      "      5        \u001b[36m8.0256\u001b[0m        \u001b[32m7.4579\u001b[0m  0.1089\n",
      "      6        \u001b[36m6.2052\u001b[0m        \u001b[32m6.0408\u001b[0m  0.1116\n",
      "      7        \u001b[36m5.1842\u001b[0m        \u001b[32m5.0484\u001b[0m  0.1113\n",
      "      8        \u001b[36m4.3810\u001b[0m        \u001b[32m4.2675\u001b[0m  0.1091\n",
      "      9        \u001b[36m3.7655\u001b[0m        \u001b[32m3.5638\u001b[0m  0.1061\n",
      "     10        \u001b[36m3.3189\u001b[0m        \u001b[32m3.2460\u001b[0m  0.1081\n",
      "     11        \u001b[36m2.9563\u001b[0m        \u001b[32m2.8082\u001b[0m  0.1089\n",
      "     12        \u001b[36m2.6855\u001b[0m        \u001b[32m2.5692\u001b[0m  0.1152\n",
      "     13        \u001b[36m2.4684\u001b[0m        \u001b[32m2.1134\u001b[0m  0.1061\n",
      "     14        \u001b[36m2.2996\u001b[0m        \u001b[32m1.9490\u001b[0m  0.1056\n",
      "     15        \u001b[36m2.1685\u001b[0m        \u001b[32m1.7895\u001b[0m  0.1721\n",
      "     16        \u001b[36m2.0423\u001b[0m        \u001b[32m1.7119\u001b[0m  0.1288\n",
      "     17        \u001b[36m1.9418\u001b[0m        \u001b[32m1.6174\u001b[0m  0.1075\n",
      "     18        \u001b[36m1.8479\u001b[0m        \u001b[32m1.5441\u001b[0m  0.1084\n",
      "     19        \u001b[36m1.7759\u001b[0m        \u001b[32m1.4451\u001b[0m  0.1211\n",
      "     20        \u001b[36m1.7250\u001b[0m        1.5327  0.1226\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m179220.0314\u001b[0m       \u001b[32m15.1108\u001b[0m  0.1122\n",
      "      2   \u001b[36m150115.2774\u001b[0m        \u001b[32m9.4549\u001b[0m  0.1057\n",
      "      3   \u001b[36m115871.4871\u001b[0m        \u001b[32m6.4860\u001b[0m  0.1104\n",
      "      4    \u001b[36m99283.1693\u001b[0m        \u001b[32m4.2022\u001b[0m  0.1080\n",
      "      5    \u001b[36m76814.7020\u001b[0m        \u001b[32m2.9701\u001b[0m  0.1071\n",
      "      6    \u001b[36m59609.3615\u001b[0m        \u001b[32m2.0008\u001b[0m  0.1076\n",
      "      7    \u001b[36m30219.5179\u001b[0m        \u001b[32m1.7020\u001b[0m  0.1063\n",
      "      8    \u001b[36m20021.4869\u001b[0m        \u001b[32m1.4224\u001b[0m  0.1215\n",
      "      9     \u001b[36m8720.1656\u001b[0m        \u001b[32m1.3552\u001b[0m  0.1096\n",
      "     10     \u001b[36m3293.7401\u001b[0m        \u001b[32m1.3490\u001b[0m  0.1140\n",
      "     11     \u001b[36m1841.6171\u001b[0m        1.3507  0.1047\n",
      "     12     \u001b[36m1450.6664\u001b[0m        1.3930  0.1051\n",
      "     13      \u001b[36m463.6487\u001b[0m        1.4066  0.1052\n",
      "     14      \u001b[36m238.9641\u001b[0m        1.4086  0.1410\n",
      "     15       \u001b[36m81.8573\u001b[0m        1.4087  0.1397\n",
      "     16       \u001b[36m26.2860\u001b[0m        1.3986  0.1097\n",
      "     17        \u001b[36m8.6877\u001b[0m        1.3848  0.1199\n",
      "     18        \u001b[36m4.4390\u001b[0m        1.3785  0.1122\n",
      "     19        \u001b[36m2.9665\u001b[0m        1.3739  0.1083\n",
      "     20        \u001b[36m1.5933\u001b[0m        1.3629  0.1052\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.6634\u001b[0m      \u001b[32m436.8362\u001b[0m  0.1073\n",
      "      2       \u001b[36m66.5474\u001b[0m     3051.7467  0.1068\n",
      "      3       \u001b[36m12.1862\u001b[0m     4697.4470  0.1043\n",
      "      4        \u001b[36m5.7290\u001b[0m     2163.3861  0.1043\n",
      "      5        \u001b[36m4.1575\u001b[0m      760.6480  0.1081\n",
      "      6        \u001b[36m3.3918\u001b[0m      572.0630  0.1082\n",
      "      7        \u001b[36m2.9491\u001b[0m      479.8625  0.1112\n",
      "      8        \u001b[36m2.6726\u001b[0m      \u001b[32m390.3262\u001b[0m  0.1081\n",
      "      9        \u001b[36m2.4411\u001b[0m      \u001b[32m299.6626\u001b[0m  0.1073\n",
      "     10        \u001b[36m2.2358\u001b[0m      300.7541  0.1087\n",
      "     11        \u001b[36m2.0277\u001b[0m      304.4822  0.1080\n",
      "     12        \u001b[36m1.7913\u001b[0m      427.9170  0.1082\n",
      "     13        \u001b[36m1.6319\u001b[0m      521.4247  0.1149\n",
      "     14        \u001b[36m1.5564\u001b[0m      563.7979  0.1712\n",
      "     15        \u001b[36m1.5010\u001b[0m      553.9286  0.1409\n",
      "     16        \u001b[36m1.4601\u001b[0m      557.6318  0.1284\n",
      "     17        \u001b[36m1.4143\u001b[0m      522.8362  0.1090\n",
      "     18        \u001b[36m1.3821\u001b[0m      549.9034  0.1084\n",
      "     19        \u001b[36m1.3530\u001b[0m      523.6296  0.1111\n",
      "     20        \u001b[36m1.3325\u001b[0m      499.0855  0.1091\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m19.0159\u001b[0m    \u001b[32m92742.1043\u001b[0m  0.1081\n",
      "      2        \u001b[36m8.9891\u001b[0m    \u001b[32m21447.3454\u001b[0m  0.1075\n",
      "      3        \u001b[36m6.2844\u001b[0m    \u001b[32m12018.5331\u001b[0m  0.1085\n",
      "      4        \u001b[36m4.2331\u001b[0m    \u001b[32m10157.9051\u001b[0m  0.1086\n",
      "      5        \u001b[36m2.4486\u001b[0m     \u001b[32m9413.4799\u001b[0m  0.1083\n",
      "      6        \u001b[36m1.5340\u001b[0m     \u001b[32m9105.0837\u001b[0m  0.1057\n",
      "      7        \u001b[36m1.1363\u001b[0m     \u001b[32m8936.4026\u001b[0m  0.1090\n",
      "      8        \u001b[36m0.9916\u001b[0m     \u001b[32m8854.5541\u001b[0m  0.1037\n",
      "      9        \u001b[36m0.9336\u001b[0m     \u001b[32m8822.8713\u001b[0m  0.1091\n",
      "     10        0.9377     \u001b[32m8820.6862\u001b[0m  0.1060\n",
      "     11        \u001b[36m0.9062\u001b[0m     \u001b[32m8801.7956\u001b[0m  0.1023\n",
      "     12        \u001b[36m0.8973\u001b[0m     8804.2687  0.1098\n",
      "     13        \u001b[36m0.8904\u001b[0m     8804.2832  0.1301\n",
      "     14        0.8944     \u001b[32m8800.2428\u001b[0m  0.1525\n",
      "     15        0.8915     8815.7429  0.1245\n",
      "     16        \u001b[36m0.8830\u001b[0m     8806.7110  0.1143\n",
      "     17        \u001b[36m0.8746\u001b[0m     \u001b[32m8792.5307\u001b[0m  0.1061\n",
      "     18        \u001b[36m0.8743\u001b[0m     \u001b[32m8783.0200\u001b[0m  0.1108\n",
      "     19        0.8841     \u001b[32m8772.6415\u001b[0m  0.1088\n",
      "     20        \u001b[36m0.8709\u001b[0m     \u001b[32m8755.3886\u001b[0m  0.1083\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m327121.4865\u001b[0m       \u001b[32m32.9801\u001b[0m  0.1157\n",
      "      2   \u001b[36m245876.5701\u001b[0m        \u001b[32m7.6678\u001b[0m  0.1091\n",
      "      3   \u001b[36m209972.5303\u001b[0m        \u001b[32m5.1849\u001b[0m  0.1109\n",
      "      4   \u001b[36m153608.3268\u001b[0m        \u001b[32m3.7798\u001b[0m  0.1278\n",
      "      5   \u001b[36m126560.2697\u001b[0m        \u001b[32m2.9685\u001b[0m  0.1163\n",
      "      6   \u001b[36m107059.9088\u001b[0m        \u001b[32m2.5157\u001b[0m  0.1135\n",
      "      7    \u001b[36m81300.6774\u001b[0m        \u001b[32m2.2399\u001b[0m  0.1106\n",
      "      8    \u001b[36m68017.4382\u001b[0m        \u001b[32m2.0790\u001b[0m  0.1128\n",
      "      9    \u001b[36m58959.5318\u001b[0m        \u001b[32m1.9960\u001b[0m  0.1234\n",
      "     10    \u001b[36m45291.8383\u001b[0m        \u001b[32m1.8799\u001b[0m  0.1127\n",
      "     11    \u001b[36m41169.2233\u001b[0m        \u001b[32m1.7843\u001b[0m  0.1078\n",
      "     12    \u001b[36m33326.3683\u001b[0m        \u001b[32m1.7034\u001b[0m  0.1257\n",
      "     13    \u001b[36m25357.0041\u001b[0m        1.7196  0.1528\n",
      "     14    \u001b[36m20763.5871\u001b[0m        \u001b[32m1.6063\u001b[0m  0.1212\n",
      "     15    \u001b[36m16511.6416\u001b[0m        \u001b[32m1.5265\u001b[0m  0.1111\n",
      "     16    \u001b[36m14980.1219\u001b[0m        \u001b[32m1.4925\u001b[0m  0.1089\n",
      "     17    \u001b[36m10936.5086\u001b[0m        \u001b[32m1.4391\u001b[0m  0.1111\n",
      "     18     \u001b[36m9963.2726\u001b[0m        1.5794  0.1142\n",
      "     19     \u001b[36m6977.6039\u001b[0m        1.4498  0.1112\n",
      "     20     \u001b[36m6119.1883\u001b[0m        1.4440  0.1099\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m11796.8968\u001b[0m        \u001b[32m5.5499\u001b[0m  0.1102\n",
      "      2     \u001b[36m1017.5474\u001b[0m        \u001b[32m2.0032\u001b[0m  0.1084\n",
      "      3       \u001b[36m24.0728\u001b[0m        \u001b[32m0.9724\u001b[0m  0.1045\n",
      "      4        \u001b[36m2.9793\u001b[0m        \u001b[32m0.8175\u001b[0m  0.1060\n",
      "      5       35.4228        \u001b[32m0.7950\u001b[0m  0.1090\n",
      "      6       47.1966        0.8106  0.1270\n",
      "      7        9.6848        0.7992  0.1151\n",
      "      8        6.9181        \u001b[32m0.7815\u001b[0m  0.1045\n",
      "      9        9.2892        \u001b[32m0.7796\u001b[0m  0.1059\n",
      "     10        5.4368        \u001b[32m0.7641\u001b[0m  0.1066\n",
      "     11        \u001b[36m1.9457\u001b[0m        \u001b[32m0.7596\u001b[0m  0.1214\n",
      "     12        2.3309        \u001b[32m0.7432\u001b[0m  0.1713\n",
      "     13        4.4628        \u001b[32m0.7415\u001b[0m  0.1221\n",
      "     14        8.3256        \u001b[32m0.7340\u001b[0m  0.1128\n",
      "     15       24.4398        \u001b[32m0.7281\u001b[0m  0.1125\n",
      "     16       89.7067        \u001b[32m0.7276\u001b[0m  0.1074\n",
      "     17      295.7097        \u001b[32m0.7000\u001b[0m  0.1099\n",
      "     18        6.1828        0.7898  0.1202\n",
      "     19      824.0388        0.8195  0.1095\n",
      "     20     1696.4875        0.7479  0.1079\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m30561.3499\u001b[0m       \u001b[32m20.6328\u001b[0m  0.1075\n",
      "      2     \u001b[36m7903.7239\u001b[0m       \u001b[32m12.0738\u001b[0m  0.1112\n",
      "      3     \u001b[36m6589.1881\u001b[0m        \u001b[32m7.3087\u001b[0m  0.1172\n",
      "      4      \u001b[36m605.0975\u001b[0m        \u001b[32m5.9658\u001b[0m  0.1136\n",
      "      5      \u001b[36m125.5361\u001b[0m        \u001b[32m4.2997\u001b[0m  0.1104\n",
      "      6      238.9512        \u001b[32m3.2566\u001b[0m  0.1053\n",
      "      7        \u001b[36m4.2819\u001b[0m        \u001b[32m2.9265\u001b[0m  0.1082\n",
      "      8        4.4529        \u001b[32m2.7526\u001b[0m  0.1042\n",
      "      9        \u001b[36m3.5535\u001b[0m        \u001b[32m2.5904\u001b[0m  0.1047\n",
      "     10        \u001b[36m3.5425\u001b[0m        \u001b[32m2.4728\u001b[0m  0.1096\n",
      "     11        \u001b[36m2.7897\u001b[0m        \u001b[32m2.3908\u001b[0m  0.1380\n",
      "     12        2.8078        \u001b[32m2.3134\u001b[0m  0.1438\n",
      "     13        \u001b[36m2.6283\u001b[0m        \u001b[32m2.2444\u001b[0m  0.1669\n",
      "     14        \u001b[36m2.6141\u001b[0m        \u001b[32m2.1855\u001b[0m  0.1128\n",
      "     15        \u001b[36m2.4585\u001b[0m        \u001b[32m2.1293\u001b[0m  0.1129\n",
      "     16        \u001b[36m2.3963\u001b[0m        \u001b[32m2.0753\u001b[0m  0.1138\n",
      "     17        \u001b[36m2.3382\u001b[0m        \u001b[32m2.0274\u001b[0m  0.1239\n",
      "     18        \u001b[36m2.2830\u001b[0m        \u001b[32m1.9836\u001b[0m  0.1064\n",
      "     19        \u001b[36m2.2281\u001b[0m        \u001b[32m1.9372\u001b[0m  0.1061\n",
      "     20        \u001b[36m2.1744\u001b[0m        \u001b[32m1.8882\u001b[0m  0.1093\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m6.1981\u001b[0m    \u001b[32m64975.4090\u001b[0m  0.1174\n",
      "      2        \u001b[36m2.0674\u001b[0m     \u001b[32m4346.4193\u001b[0m  0.1064\n",
      "      3        \u001b[36m1.7857\u001b[0m      \u001b[32m325.7279\u001b[0m  0.1041\n",
      "      4        \u001b[36m1.5307\u001b[0m     1625.8678  0.1096\n",
      "      5        \u001b[36m1.3923\u001b[0m    11459.2123  0.1041\n",
      "      6        \u001b[36m1.2974\u001b[0m    27099.1076  0.1038\n",
      "      7        \u001b[36m1.2177\u001b[0m    43533.8217  0.1085\n",
      "      8        \u001b[36m1.1614\u001b[0m    69070.7469  0.1051\n",
      "      9        \u001b[36m1.1238\u001b[0m    99420.7138  0.1016\n",
      "     10        \u001b[36m1.0818\u001b[0m   119175.2389  0.1333\n",
      "     11        \u001b[36m1.0600\u001b[0m   129640.3722  0.1459\n",
      "     12        \u001b[36m1.0243\u001b[0m   168413.4254  0.1169\n",
      "     13        \u001b[36m1.0041\u001b[0m   179082.5890  0.1044\n",
      "     14        \u001b[36m0.9917\u001b[0m   200102.3814  0.1043\n",
      "     15        \u001b[36m0.9667\u001b[0m   204081.6211  0.1071\n",
      "     16        \u001b[36m0.9533\u001b[0m   215416.3384  0.1156\n",
      "     17        \u001b[36m0.9429\u001b[0m   217902.3948  0.1095\n",
      "     18        \u001b[36m0.9335\u001b[0m   229623.3203  0.1089\n",
      "     19        \u001b[36m0.9284\u001b[0m   225437.8052  0.1140\n",
      "     20        \u001b[36m0.9157\u001b[0m   236758.0671  0.1067\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m156.5076\u001b[0m       \u001b[32m67.4937\u001b[0m  0.1019\n",
      "      2       \u001b[36m31.7864\u001b[0m       \u001b[32m10.3875\u001b[0m  0.1010\n",
      "      3        \u001b[36m5.3536\u001b[0m        \u001b[32m3.2301\u001b[0m  0.1165\n",
      "      4        \u001b[36m2.0995\u001b[0m        \u001b[32m2.6039\u001b[0m  0.1056\n",
      "      5        \u001b[36m1.8272\u001b[0m        \u001b[32m2.2690\u001b[0m  0.1062\n",
      "      6        \u001b[36m1.5654\u001b[0m        \u001b[32m2.1925\u001b[0m  0.1031\n",
      "      7        \u001b[36m1.4491\u001b[0m        \u001b[32m2.0071\u001b[0m  0.1023\n",
      "      8        \u001b[36m1.3769\u001b[0m        \u001b[32m1.9554\u001b[0m  0.1113\n",
      "      9        \u001b[36m1.3165\u001b[0m        \u001b[32m1.8749\u001b[0m  0.1071\n",
      "     10        \u001b[36m1.2718\u001b[0m        \u001b[32m1.8067\u001b[0m  0.1495\n",
      "     11        \u001b[36m1.2340\u001b[0m        \u001b[32m1.7148\u001b[0m  0.1375\n",
      "     12        \u001b[36m1.2216\u001b[0m        \u001b[32m1.6177\u001b[0m  0.1089\n",
      "     13        \u001b[36m1.1971\u001b[0m        1.6313  0.1040\n",
      "     14        \u001b[36m1.1882\u001b[0m        1.6188  0.1057\n",
      "     15        \u001b[36m1.1416\u001b[0m        \u001b[32m1.5650\u001b[0m  0.1049\n",
      "     16        \u001b[36m1.1186\u001b[0m        \u001b[32m1.5439\u001b[0m  0.1183\n",
      "     17        \u001b[36m1.1049\u001b[0m        \u001b[32m1.4918\u001b[0m  0.1038\n",
      "     18        \u001b[36m1.0949\u001b[0m        \u001b[32m1.4825\u001b[0m  0.1031\n",
      "     19        \u001b[36m1.0833\u001b[0m        \u001b[32m1.4505\u001b[0m  0.1051\n",
      "     20        \u001b[36m1.0776\u001b[0m        \u001b[32m1.4387\u001b[0m  0.1049\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m29962.1861\u001b[0m       \u001b[32m21.6151\u001b[0m  0.1039\n",
      "      2    \u001b[36m16369.4921\u001b[0m        \u001b[32m9.9553\u001b[0m  0.1032\n",
      "      3    \u001b[36m12773.6554\u001b[0m        \u001b[32m6.4584\u001b[0m  0.1027\n",
      "      4     \u001b[36m5998.4291\u001b[0m        \u001b[32m4.1933\u001b[0m  0.1037\n",
      "      5     \u001b[36m4224.0219\u001b[0m        \u001b[32m2.9002\u001b[0m  0.1038\n",
      "      6     4370.7988        \u001b[32m2.5408\u001b[0m  0.1061\n",
      "      7     \u001b[36m1887.8325\u001b[0m        3.1735  0.1032\n",
      "      8      \u001b[36m455.7062\u001b[0m        3.1943  0.1054\n",
      "      9       \u001b[36m24.8969\u001b[0m        2.7675  0.1243\n",
      "     10       \u001b[36m11.8834\u001b[0m        \u001b[32m2.4405\u001b[0m  0.1343\n",
      "     11        \u001b[36m3.9132\u001b[0m        \u001b[32m2.2181\u001b[0m  0.1484\n",
      "     12        \u001b[36m2.9621\u001b[0m        \u001b[32m2.0671\u001b[0m  0.1139\n",
      "     13        \u001b[36m2.6575\u001b[0m        \u001b[32m1.9360\u001b[0m  0.1088\n",
      "     14        \u001b[36m2.3629\u001b[0m        \u001b[32m1.8226\u001b[0m  0.1184\n",
      "     15        \u001b[36m2.2043\u001b[0m        \u001b[32m1.7668\u001b[0m  0.1102\n",
      "     16        \u001b[36m2.1244\u001b[0m        \u001b[32m1.7052\u001b[0m  0.1045\n",
      "     17        \u001b[36m2.0668\u001b[0m        \u001b[32m1.6706\u001b[0m  0.1045\n",
      "     18        \u001b[36m2.0199\u001b[0m        \u001b[32m1.6260\u001b[0m  0.1035\n",
      "     19        \u001b[36m1.9519\u001b[0m        \u001b[32m1.6069\u001b[0m  0.1056\n",
      "     20        \u001b[36m1.9078\u001b[0m        \u001b[32m1.5899\u001b[0m  0.1052\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1521.6196\u001b[0m       \u001b[32m23.2713\u001b[0m  0.1035\n",
      "      2       \u001b[36m37.8681\u001b[0m        \u001b[32m6.8768\u001b[0m  0.1064\n",
      "      3      244.6511        \u001b[32m2.4169\u001b[0m  0.1085\n",
      "      4        \u001b[36m4.8320\u001b[0m        \u001b[32m1.6951\u001b[0m  0.1057\n",
      "      5      256.7723        \u001b[32m1.4755\u001b[0m  0.1028\n",
      "      6        4.9422        \u001b[32m1.4180\u001b[0m  0.1069\n",
      "      7        \u001b[36m4.7362\u001b[0m        \u001b[32m1.3882\u001b[0m  0.1042\n",
      "      8        \u001b[36m2.1300\u001b[0m        \u001b[32m1.3864\u001b[0m  0.1019\n",
      "      9        \u001b[36m1.3736\u001b[0m        \u001b[32m1.3827\u001b[0m  0.1020\n",
      "     10        \u001b[36m1.1887\u001b[0m        \u001b[32m1.3671\u001b[0m  0.1211\n",
      "     11        \u001b[36m1.1795\u001b[0m        \u001b[32m1.3645\u001b[0m  0.1381\n",
      "     12        \u001b[36m1.1653\u001b[0m        \u001b[32m1.3524\u001b[0m  0.1204\n",
      "     13        \u001b[36m1.1607\u001b[0m        1.3534  0.1075\n",
      "     14        \u001b[36m1.1501\u001b[0m        \u001b[32m1.3382\u001b[0m  0.1248\n",
      "     15        \u001b[36m1.1402\u001b[0m        1.3421  0.1069\n",
      "     16        \u001b[36m1.1296\u001b[0m        \u001b[32m1.3307\u001b[0m  0.1047\n",
      "     17        \u001b[36m1.1232\u001b[0m        \u001b[32m1.3182\u001b[0m  0.1045\n",
      "     18        \u001b[36m1.1143\u001b[0m        \u001b[32m1.3130\u001b[0m  0.1074\n",
      "     19        \u001b[36m1.1073\u001b[0m        1.3135  0.1022\n",
      "     20        \u001b[36m1.1030\u001b[0m        \u001b[32m1.3012\u001b[0m  0.1083\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m10.1915\u001b[0m        \u001b[32m2.9366\u001b[0m  0.1060\n",
      "      2        \u001b[36m2.9182\u001b[0m        \u001b[32m2.1802\u001b[0m  0.1059\n",
      "      3        \u001b[36m2.0557\u001b[0m        \u001b[32m1.5184\u001b[0m  0.1325\n",
      "      4        \u001b[36m1.7181\u001b[0m        \u001b[32m1.3179\u001b[0m  0.1530\n",
      "      5        \u001b[36m1.4803\u001b[0m        \u001b[32m1.2156\u001b[0m  0.1087\n",
      "      6        \u001b[36m1.3748\u001b[0m        \u001b[32m1.1703\u001b[0m  0.1064\n",
      "      7        \u001b[36m1.3088\u001b[0m        \u001b[32m1.1159\u001b[0m  0.1062\n",
      "      8        \u001b[36m1.2326\u001b[0m        \u001b[32m1.0803\u001b[0m  0.1041\n",
      "      9        \u001b[36m1.1704\u001b[0m        \u001b[32m1.0602\u001b[0m  0.1346\n",
      "     10        \u001b[36m1.1307\u001b[0m        \u001b[32m1.0291\u001b[0m  0.1632\n",
      "     11        \u001b[36m1.0930\u001b[0m        \u001b[32m1.0127\u001b[0m  0.1147\n",
      "     12        \u001b[36m1.0664\u001b[0m        \u001b[32m0.9995\u001b[0m  0.1042\n",
      "     13        \u001b[36m1.0516\u001b[0m        1.0034  0.1027\n",
      "     14        \u001b[36m1.0275\u001b[0m        \u001b[32m0.9744\u001b[0m  0.1047\n",
      "     15        \u001b[36m1.0129\u001b[0m        0.9860  0.1077\n",
      "     16        1.0207        \u001b[32m0.9649\u001b[0m  0.1040\n",
      "     17        \u001b[36m0.9965\u001b[0m        \u001b[32m0.9607\u001b[0m  0.1027\n",
      "     18        \u001b[36m0.9789\u001b[0m        \u001b[32m0.9513\u001b[0m  0.1086\n",
      "     19        \u001b[36m0.9714\u001b[0m        0.9576  0.1225\n",
      "     20        \u001b[36m0.9564\u001b[0m        \u001b[32m0.9435\u001b[0m  0.1035\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m48.6803\u001b[0m       \u001b[32m26.0885\u001b[0m  0.1063\n",
      "      2       \u001b[36m19.5752\u001b[0m       \u001b[32m10.5529\u001b[0m  0.1022\n",
      "      3        \u001b[36m8.3536\u001b[0m        \u001b[32m5.0036\u001b[0m  0.1033\n",
      "      4        \u001b[36m4.4458\u001b[0m        \u001b[32m3.1246\u001b[0m  0.1057\n",
      "      5        \u001b[36m2.6843\u001b[0m        \u001b[32m2.0313\u001b[0m  0.1022\n",
      "      6        \u001b[36m1.8711\u001b[0m        \u001b[32m1.5791\u001b[0m  0.1283\n",
      "      7        \u001b[36m1.5001\u001b[0m        \u001b[32m1.3549\u001b[0m  0.1162\n",
      "      8        \u001b[36m1.2860\u001b[0m        \u001b[32m1.2513\u001b[0m  0.1090\n",
      "      9        \u001b[36m1.1924\u001b[0m        \u001b[32m1.1887\u001b[0m  0.1391\n",
      "     10        \u001b[36m1.1407\u001b[0m        \u001b[32m1.1551\u001b[0m  0.1379\n",
      "     11        \u001b[36m1.1022\u001b[0m        \u001b[32m1.0961\u001b[0m  0.1087\n",
      "     12        \u001b[36m1.0738\u001b[0m        \u001b[32m1.0738\u001b[0m  0.1059\n",
      "     13        \u001b[36m1.0550\u001b[0m        \u001b[32m1.0677\u001b[0m  0.1060\n",
      "     14        \u001b[36m1.0433\u001b[0m        \u001b[32m1.0291\u001b[0m  0.1071\n",
      "     15        \u001b[36m1.0337\u001b[0m        \u001b[32m1.0173\u001b[0m  0.1101\n",
      "     16        \u001b[36m1.0299\u001b[0m        \u001b[32m1.0050\u001b[0m  0.1199\n",
      "     17        \u001b[36m1.0196\u001b[0m        \u001b[32m0.9861\u001b[0m  0.1115\n",
      "     18        \u001b[36m1.0186\u001b[0m        \u001b[32m0.9797\u001b[0m  0.1044\n",
      "     19        1.0310        \u001b[32m0.9735\u001b[0m  0.1135\n",
      "     20        1.0327        \u001b[32m0.9628\u001b[0m  0.1045\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m173312.9573\u001b[0m       \u001b[32m92.6066\u001b[0m  0.1064\n",
      "      2   \u001b[36m121155.6784\u001b[0m       \u001b[32m23.8153\u001b[0m  0.1055\n",
      "      3    \u001b[36m85310.5284\u001b[0m       \u001b[32m12.2255\u001b[0m  0.1064\n",
      "      4    \u001b[36m70534.4720\u001b[0m       13.0268  0.1082\n",
      "      5    \u001b[36m47014.2709\u001b[0m        \u001b[32m9.2451\u001b[0m  0.1345\n",
      "      6    \u001b[36m30563.4355\u001b[0m        \u001b[32m6.3381\u001b[0m  0.1068\n",
      "      7    \u001b[36m23025.5979\u001b[0m        \u001b[32m4.9417\u001b[0m  0.1059\n",
      "      8    \u001b[36m13602.8045\u001b[0m        \u001b[32m3.8591\u001b[0m  0.1081\n",
      "      9     \u001b[36m9908.2874\u001b[0m        \u001b[32m3.3524\u001b[0m  0.1704\n",
      "     10     \u001b[36m6319.5963\u001b[0m        \u001b[32m2.9197\u001b[0m  0.1196\n",
      "     11     \u001b[36m3707.5311\u001b[0m        \u001b[32m2.5925\u001b[0m  0.1086\n",
      "     12     \u001b[36m2144.0650\u001b[0m        \u001b[32m2.3409\u001b[0m  0.1086\n",
      "     13     \u001b[36m1520.6152\u001b[0m        \u001b[32m2.2211\u001b[0m  0.1171\n",
      "     14      \u001b[36m698.6152\u001b[0m        \u001b[32m2.1099\u001b[0m  0.1114\n",
      "     15      \u001b[36m417.9039\u001b[0m        \u001b[32m2.0462\u001b[0m  0.1067\n",
      "     16      \u001b[36m210.6830\u001b[0m        \u001b[32m1.9233\u001b[0m  0.1047\n",
      "     17       \u001b[36m91.0842\u001b[0m        \u001b[32m1.8372\u001b[0m  0.1028\n",
      "     18       \u001b[36m62.1584\u001b[0m        \u001b[32m1.8117\u001b[0m  0.1049\n",
      "     19       \u001b[36m25.5010\u001b[0m        \u001b[32m1.7535\u001b[0m  0.1027\n",
      "     20       \u001b[36m11.5807\u001b[0m        \u001b[32m1.7250\u001b[0m  0.1046\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m8889.8908\u001b[0m        \u001b[32m6.2079\u001b[0m  0.1022\n",
      "      2     \u001b[36m5468.0008\u001b[0m        \u001b[32m4.4145\u001b[0m  0.1152\n",
      "      3     \u001b[36m2945.2578\u001b[0m        \u001b[32m2.0844\u001b[0m  0.1141\n",
      "      4     \u001b[36m1652.5714\u001b[0m        \u001b[32m1.7693\u001b[0m  0.1051\n",
      "      5      \u001b[36m768.6567\u001b[0m        \u001b[32m1.4792\u001b[0m  0.1070\n",
      "      6      \u001b[36m426.1470\u001b[0m        \u001b[32m1.4182\u001b[0m  0.1055\n",
      "      7      \u001b[36m167.6656\u001b[0m        \u001b[32m1.3338\u001b[0m  0.1038\n",
      "      8       \u001b[36m42.2237\u001b[0m        \u001b[32m1.2623\u001b[0m  0.1268\n",
      "      9        \u001b[36m7.5076\u001b[0m        \u001b[32m1.2354\u001b[0m  0.1665\n",
      "     10        \u001b[36m3.1139\u001b[0m        \u001b[32m1.2350\u001b[0m  0.1263\n",
      "     11        \u001b[36m1.4704\u001b[0m        1.2353  0.1088\n",
      "     12        \u001b[36m1.0325\u001b[0m        1.2400  0.1124\n",
      "     13        \u001b[36m0.9959\u001b[0m        1.2385  0.1107\n",
      "     14        \u001b[36m0.9885\u001b[0m        1.2430  0.1062\n",
      "     15        0.9957        1.2394  0.1050\n",
      "     16        \u001b[36m0.9798\u001b[0m        1.2445  0.1058\n",
      "     17        \u001b[36m0.9773\u001b[0m        1.2357  0.1094\n",
      "     18        \u001b[36m0.9703\u001b[0m        1.2381  0.1110\n",
      "     19        \u001b[36m0.9699\u001b[0m        \u001b[32m1.2307\u001b[0m  0.1105\n",
      "     20        0.9817        \u001b[32m1.2263\u001b[0m  0.1204\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m6845.0329\u001b[0m        \u001b[32m3.6338\u001b[0m  0.1048\n",
      "      2     \u001b[36m4432.8511\u001b[0m        \u001b[32m2.2681\u001b[0m  0.1017\n",
      "      3     \u001b[36m3321.1333\u001b[0m        \u001b[32m1.9674\u001b[0m  0.1072\n",
      "      4     \u001b[36m2223.2172\u001b[0m        \u001b[32m1.8340\u001b[0m  0.1059\n",
      "      5     \u001b[36m1551.0959\u001b[0m        \u001b[32m1.6663\u001b[0m  0.1040\n",
      "      6     \u001b[36m1162.7367\u001b[0m        \u001b[32m1.5708\u001b[0m  0.1032\n",
      "      7      \u001b[36m789.4937\u001b[0m        \u001b[32m1.5163\u001b[0m  0.1058\n",
      "      8      \u001b[36m478.9354\u001b[0m        \u001b[32m1.4513\u001b[0m  0.1597\n",
      "      9      \u001b[36m333.7245\u001b[0m        \u001b[32m1.4264\u001b[0m  0.1159\n",
      "     10      \u001b[36m268.2411\u001b[0m        \u001b[32m1.3166\u001b[0m  0.1075\n",
      "     11      \u001b[36m120.8699\u001b[0m        \u001b[32m1.2159\u001b[0m  0.1016\n",
      "     12       \u001b[36m64.4870\u001b[0m        \u001b[32m1.1362\u001b[0m  0.1252\n",
      "     13       \u001b[36m36.1534\u001b[0m        \u001b[32m1.1206\u001b[0m  0.1039\n",
      "     14       \u001b[36m28.2648\u001b[0m        \u001b[32m1.0593\u001b[0m  0.1043\n",
      "     15        \u001b[36m7.4443\u001b[0m        \u001b[32m1.0275\u001b[0m  0.1038\n",
      "     16        \u001b[36m5.1486\u001b[0m        1.0303  0.1067\n",
      "     17        \u001b[36m2.3933\u001b[0m        \u001b[32m0.9745\u001b[0m  0.1101\n",
      "     18        \u001b[36m1.5092\u001b[0m        0.9876  0.1089\n",
      "     19        \u001b[36m1.3037\u001b[0m        \u001b[32m0.9604\u001b[0m  0.1097\n",
      "     20        \u001b[36m1.2760\u001b[0m        \u001b[32m0.9473\u001b[0m  0.1091\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m15291.1618\u001b[0m        \u001b[32m3.9023\u001b[0m  0.1046\n",
      "      2     \u001b[36m3572.0062\u001b[0m        \u001b[32m3.8399\u001b[0m  0.1039\n",
      "      3      \u001b[36m839.6206\u001b[0m        \u001b[32m3.7330\u001b[0m  0.1033\n",
      "      4       \u001b[36m45.3563\u001b[0m        \u001b[32m3.0699\u001b[0m  0.1041\n",
      "      5       55.9583        \u001b[32m2.4194\u001b[0m  0.1068\n",
      "      6       \u001b[36m27.6360\u001b[0m        \u001b[32m2.1538\u001b[0m  0.1098\n",
      "      7        \u001b[36m5.6558\u001b[0m        \u001b[32m1.9469\u001b[0m  0.1078\n",
      "      8        6.1079        \u001b[32m1.8391\u001b[0m  0.1655\n",
      "      9        \u001b[36m4.8806\u001b[0m        \u001b[32m1.6955\u001b[0m  0.1193\n",
      "     10        \u001b[36m4.5006\u001b[0m        \u001b[32m1.5711\u001b[0m  0.1104\n",
      "     11        \u001b[36m4.2804\u001b[0m        \u001b[32m1.4430\u001b[0m  0.1063\n",
      "     12        \u001b[36m3.8848\u001b[0m        \u001b[32m1.3667\u001b[0m  0.1053\n",
      "     13        \u001b[36m3.6156\u001b[0m        \u001b[32m1.2824\u001b[0m  0.1126\n",
      "     14        \u001b[36m3.3938\u001b[0m        \u001b[32m1.2290\u001b[0m  0.1102\n",
      "     15        \u001b[36m3.1604\u001b[0m        \u001b[32m1.1576\u001b[0m  0.1198\n",
      "     16        \u001b[36m3.0489\u001b[0m        \u001b[32m1.1063\u001b[0m  0.1230\n",
      "     17        \u001b[36m2.7599\u001b[0m        \u001b[32m1.0822\u001b[0m  0.1056\n",
      "     18        \u001b[36m2.6281\u001b[0m        1.1067  0.1012\n",
      "     19        \u001b[36m2.4767\u001b[0m        \u001b[32m1.0640\u001b[0m  0.1042\n",
      "     20        \u001b[36m2.3214\u001b[0m        \u001b[32m1.0482\u001b[0m  0.1061\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m9563.5407\u001b[0m       \u001b[32m15.0461\u001b[0m  0.1026\n",
      "      2     \u001b[36m5539.6492\u001b[0m        \u001b[32m2.3300\u001b[0m  0.1034\n",
      "      3      \u001b[36m737.0435\u001b[0m        \u001b[32m1.9562\u001b[0m  0.1048\n",
      "      4     2552.7962        1.9983  0.1085\n",
      "      5      \u001b[36m622.9321\u001b[0m        \u001b[32m1.6197\u001b[0m  0.1098\n",
      "      6       \u001b[36m32.9137\u001b[0m        \u001b[32m1.5698\u001b[0m  0.1058\n",
      "      7        \u001b[36m1.6904\u001b[0m        \u001b[32m1.4788\u001b[0m  0.1561\n",
      "      8       73.7413        \u001b[32m1.4398\u001b[0m  0.1771\n",
      "      9       57.2095        \u001b[32m1.3995\u001b[0m  0.1169\n",
      "     10       39.3947        \u001b[32m1.3644\u001b[0m  0.1151\n",
      "     11       13.7493        \u001b[32m1.3351\u001b[0m  0.1061\n",
      "     12        5.6675        \u001b[32m1.2971\u001b[0m  0.1135\n",
      "     13        2.2868        \u001b[32m1.2590\u001b[0m  0.1146\n",
      "     14        \u001b[36m1.3943\u001b[0m        \u001b[32m1.2362\u001b[0m  0.1081\n",
      "     15        1.4647        \u001b[32m1.2107\u001b[0m  0.1095\n",
      "     16        \u001b[36m1.3625\u001b[0m        \u001b[32m1.1759\u001b[0m  0.1045\n",
      "     17        \u001b[36m1.2872\u001b[0m        \u001b[32m1.1488\u001b[0m  0.1087\n",
      "     18        \u001b[36m1.1748\u001b[0m        \u001b[32m1.1176\u001b[0m  0.1076\n",
      "     19        1.6780        \u001b[32m1.1005\u001b[0m  0.1304\n",
      "     20        \u001b[36m1.1593\u001b[0m        \u001b[32m1.0781\u001b[0m  0.1123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1580.3236\u001b[0m     \u001b[32m1006.9460\u001b[0m  0.1042\n",
      "      2      \u001b[36m558.9294\u001b[0m      \u001b[32m320.2784\u001b[0m  0.1079\n",
      "      3      \u001b[36m162.5718\u001b[0m       \u001b[32m63.6567\u001b[0m  0.1050\n",
      "      4       \u001b[36m27.5786\u001b[0m       \u001b[32m10.0514\u001b[0m  0.1060\n",
      "      5       \u001b[36m10.8542\u001b[0m        \u001b[32m5.4114\u001b[0m  0.1120\n",
      "      6        \u001b[36m5.7959\u001b[0m        \u001b[32m3.3183\u001b[0m  0.1090\n",
      "      7        \u001b[36m3.4149\u001b[0m        \u001b[32m2.5540\u001b[0m  0.1720\n",
      "      8        \u001b[36m2.2933\u001b[0m        \u001b[32m1.5483\u001b[0m  0.1261\n",
      "      9        \u001b[36m1.6819\u001b[0m        \u001b[32m1.2510\u001b[0m  0.1390\n",
      "     10        \u001b[36m1.4657\u001b[0m        \u001b[32m1.1210\u001b[0m  0.1199\n",
      "     11        \u001b[36m1.3907\u001b[0m        \u001b[32m1.0471\u001b[0m  0.1191\n",
      "     12        \u001b[36m1.3046\u001b[0m        1.0879  0.1188\n",
      "     13        \u001b[36m1.2795\u001b[0m        \u001b[32m1.0196\u001b[0m  0.1133\n",
      "     14        \u001b[36m1.2599\u001b[0m        \u001b[32m0.9427\u001b[0m  0.1110\n",
      "     15        \u001b[36m1.2286\u001b[0m        \u001b[32m0.9255\u001b[0m  0.1093\n",
      "     16        \u001b[36m1.2059\u001b[0m        \u001b[32m0.9118\u001b[0m  0.1114\n",
      "     17        1.2077        \u001b[32m0.9050\u001b[0m  0.1078\n",
      "     18        \u001b[36m1.1960\u001b[0m        0.9156  0.1093\n",
      "     19        \u001b[36m1.1656\u001b[0m        0.9141  0.1506\n",
      "     20        \u001b[36m1.1619\u001b[0m        \u001b[32m0.8744\u001b[0m  0.1183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1294.1149\u001b[0m        \u001b[32m1.3260\u001b[0m  0.1323\n",
      "      2      \u001b[36m631.6823\u001b[0m        \u001b[32m1.0861\u001b[0m  0.1124\n",
      "      3      \u001b[36m434.7797\u001b[0m        1.1376  0.1058\n",
      "      4      \u001b[36m233.4895\u001b[0m        1.1157  0.1110\n",
      "      5      \u001b[36m152.1218\u001b[0m        \u001b[32m1.0731\u001b[0m  0.1687\n",
      "      6       \u001b[36m95.4479\u001b[0m        \u001b[32m0.9990\u001b[0m  0.1245\n",
      "      7       \u001b[36m61.7419\u001b[0m        \u001b[32m0.9042\u001b[0m  0.1117\n",
      "      8       \u001b[36m40.8826\u001b[0m        \u001b[32m0.8888\u001b[0m  0.1124\n",
      "      9       \u001b[36m28.9729\u001b[0m        \u001b[32m0.8517\u001b[0m  0.1086\n",
      "     10       \u001b[36m22.3939\u001b[0m        0.8655  0.1149\n",
      "     11       \u001b[36m19.6247\u001b[0m        \u001b[32m0.8365\u001b[0m  0.1053\n",
      "     12       \u001b[36m12.3185\u001b[0m        \u001b[32m0.8342\u001b[0m  0.1068\n",
      "     13        \u001b[36m8.1941\u001b[0m        0.8396  0.1114\n",
      "     14        \u001b[36m6.7954\u001b[0m        0.8349  0.1147\n",
      "     15        \u001b[36m5.5206\u001b[0m        0.8381  0.1110\n",
      "     16        \u001b[36m3.7911\u001b[0m        \u001b[32m0.8334\u001b[0m  0.1120\n",
      "     17        \u001b[36m3.1906\u001b[0m        0.8374  0.1102\n",
      "     18        \u001b[36m2.3863\u001b[0m        0.8352  0.1114\n",
      "     19        \u001b[36m1.9637\u001b[0m        \u001b[32m0.8173\u001b[0m  0.1101\n",
      "     20        \u001b[36m1.7454\u001b[0m        0.8586  0.1139\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m518851.6162\u001b[0m      \u001b[32m109.3230\u001b[0m  0.1150\n",
      "      2   \u001b[36m374076.2416\u001b[0m       \u001b[32m13.7368\u001b[0m  0.1104\n",
      "      3   \u001b[36m292577.1730\u001b[0m        \u001b[32m3.0279\u001b[0m  0.1349\n",
      "      4   \u001b[36m233152.2006\u001b[0m        3.7560  0.1783\n",
      "      5   \u001b[36m160773.2040\u001b[0m        \u001b[32m2.7835\u001b[0m  0.1197\n",
      "      6   \u001b[36m116368.1198\u001b[0m        \u001b[32m2.5719\u001b[0m  0.1090\n",
      "      7    \u001b[36m92189.1984\u001b[0m        \u001b[32m2.4356\u001b[0m  0.1126\n",
      "      8    \u001b[36m57409.8906\u001b[0m        \u001b[32m2.2898\u001b[0m  0.1115\n",
      "      9    \u001b[36m47948.5774\u001b[0m        \u001b[32m2.2748\u001b[0m  0.1122\n",
      "     10    \u001b[36m27687.2801\u001b[0m        2.3182  0.1122\n",
      "     11    \u001b[36m19026.6448\u001b[0m        2.4699  0.1087\n",
      "     12    \u001b[36m11958.3879\u001b[0m        2.6316  0.1072\n",
      "     13     \u001b[36m7672.5458\u001b[0m        2.7286  0.1280\n",
      "     14     \u001b[36m4315.5573\u001b[0m        2.7620  0.1158\n",
      "     15     \u001b[36m2756.0804\u001b[0m        2.8205  0.1083\n",
      "     16     \u001b[36m1487.3760\u001b[0m        2.8653  0.1157\n",
      "     17     \u001b[36m1170.2013\u001b[0m        2.9995  0.1087\n",
      "     18      \u001b[36m377.0389\u001b[0m        2.9571  0.1033\n",
      "     19      \u001b[36m247.5768\u001b[0m        2.9763  0.1026\n",
      "     20       \u001b[36m77.2439\u001b[0m        2.9039  0.1110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m11.9512\u001b[0m   \u001b[32m396424.1490\u001b[0m  0.1177\n",
      "      2        \u001b[36m4.7613\u001b[0m   \u001b[32m238857.9657\u001b[0m  0.1158\n",
      "      3        \u001b[36m2.5079\u001b[0m   \u001b[32m141538.0703\u001b[0m  0.1633\n",
      "      4        \u001b[36m1.7341\u001b[0m   \u001b[32m111306.0584\u001b[0m  0.1249\n",
      "      5        \u001b[36m1.4033\u001b[0m    \u001b[32m98486.1982\u001b[0m  0.1294\n",
      "      6        \u001b[36m1.2491\u001b[0m    \u001b[32m92452.9018\u001b[0m  0.1113\n",
      "      7        \u001b[36m1.1436\u001b[0m    \u001b[32m85795.5524\u001b[0m  0.1117\n",
      "      8        \u001b[36m1.0752\u001b[0m    87189.4049  0.1118\n",
      "      9        \u001b[36m1.0320\u001b[0m    90304.6562  0.1216\n",
      "     10        \u001b[36m0.9985\u001b[0m    \u001b[32m85506.5588\u001b[0m  0.1325\n",
      "     11        \u001b[36m0.9779\u001b[0m    \u001b[32m82663.2313\u001b[0m  0.1554\n",
      "     12        \u001b[36m0.9699\u001b[0m    \u001b[32m75985.4698\u001b[0m  0.1261\n",
      "     13        \u001b[36m0.9577\u001b[0m    79751.3424  0.1399\n",
      "     14        \u001b[36m0.9516\u001b[0m    76775.1807  0.1241\n",
      "     15        \u001b[36m0.9459\u001b[0m    78086.8433  0.1084\n",
      "     16        \u001b[36m0.9415\u001b[0m    \u001b[32m72098.5637\u001b[0m  0.1300\n",
      "     17        \u001b[36m0.9316\u001b[0m    74292.9517  0.1140\n",
      "     18        \u001b[36m0.9306\u001b[0m    \u001b[32m71797.2576\u001b[0m  0.1158\n",
      "     19        \u001b[36m0.9273\u001b[0m    \u001b[32m67984.5759\u001b[0m  0.1081\n",
      "     20        \u001b[36m0.9236\u001b[0m    72308.5788  0.1199\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4552\u001b[0m        \u001b[32m1.7968\u001b[0m  0.1441\n",
      "      2        \u001b[36m1.6396\u001b[0m        \u001b[32m1.4817\u001b[0m  0.1171\n",
      "      3        \u001b[36m1.4582\u001b[0m        \u001b[32m1.4224\u001b[0m  0.1256\n",
      "      4        \u001b[36m1.3701\u001b[0m        \u001b[32m1.3638\u001b[0m  0.1109\n",
      "      5        \u001b[36m1.2939\u001b[0m        \u001b[32m1.3526\u001b[0m  0.1158\n",
      "      6        \u001b[36m1.2578\u001b[0m        \u001b[32m1.3231\u001b[0m  0.1393\n",
      "      7        \u001b[36m1.2186\u001b[0m        1.3318  0.1099\n",
      "      8        \u001b[36m1.1876\u001b[0m        \u001b[32m1.3101\u001b[0m  0.1106\n",
      "      9        \u001b[36m1.1537\u001b[0m        \u001b[32m1.3029\u001b[0m  0.1057\n",
      "     10        \u001b[36m1.1514\u001b[0m        \u001b[32m1.2796\u001b[0m  0.1121\n",
      "     11        \u001b[36m1.1279\u001b[0m        1.2905  0.1129\n",
      "     12        \u001b[36m1.0943\u001b[0m        1.2883  0.1183\n",
      "     13        \u001b[36m1.0915\u001b[0m        \u001b[32m1.2762\u001b[0m  0.1162\n",
      "     14        \u001b[36m1.0690\u001b[0m        \u001b[32m1.2710\u001b[0m  0.1147\n",
      "     15        \u001b[36m1.0523\u001b[0m        \u001b[32m1.2580\u001b[0m  0.1095\n",
      "     16        \u001b[36m1.0464\u001b[0m        1.2600  0.1127\n",
      "     17        \u001b[36m1.0257\u001b[0m        1.2628  0.1083\n",
      "     18        \u001b[36m1.0198\u001b[0m        1.2677  0.1164\n",
      "     19        \u001b[36m1.0104\u001b[0m        \u001b[32m1.2467\u001b[0m  0.1433\n",
      "     20        \u001b[36m0.9990\u001b[0m        1.2658  0.1441\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1630.1642\u001b[0m      \u001b[32m138.7554\u001b[0m  0.1166\n",
      "      2      \u001b[36m839.8282\u001b[0m       \u001b[32m36.4357\u001b[0m  0.1121\n",
      "      3      \u001b[36m220.6379\u001b[0m       \u001b[32m10.9046\u001b[0m  0.1168\n",
      "      4      \u001b[36m137.8022\u001b[0m        \u001b[32m4.3693\u001b[0m  0.1054\n",
      "      5       \u001b[36m62.5575\u001b[0m        \u001b[32m3.3809\u001b[0m  0.1151\n",
      "      6       \u001b[36m33.4238\u001b[0m        \u001b[32m3.1136\u001b[0m  0.1134\n",
      "      7       \u001b[36m19.2497\u001b[0m        \u001b[32m2.6919\u001b[0m  0.1105\n",
      "      8        \u001b[36m9.9216\u001b[0m        \u001b[32m2.3801\u001b[0m  0.1401\n",
      "      9        \u001b[36m7.8661\u001b[0m        \u001b[32m2.1378\u001b[0m  0.1137\n",
      "     10        \u001b[36m3.6581\u001b[0m        \u001b[32m1.9453\u001b[0m  0.1196\n",
      "     11        \u001b[36m2.5917\u001b[0m        \u001b[32m1.7833\u001b[0m  0.1044\n",
      "     12        \u001b[36m2.0140\u001b[0m        \u001b[32m1.6596\u001b[0m  0.1122\n",
      "     13        \u001b[36m1.6645\u001b[0m        \u001b[32m1.5624\u001b[0m  0.1065\n",
      "     14        \u001b[36m1.4599\u001b[0m        \u001b[32m1.4959\u001b[0m  0.1061\n",
      "     15        \u001b[36m1.3594\u001b[0m        \u001b[32m1.4573\u001b[0m  0.1067\n",
      "     16        \u001b[36m1.3006\u001b[0m        \u001b[32m1.4210\u001b[0m  0.1086\n",
      "     17        \u001b[36m1.2643\u001b[0m        \u001b[32m1.3920\u001b[0m  0.1019\n",
      "     18        \u001b[36m1.2328\u001b[0m        \u001b[32m1.3718\u001b[0m  0.1467\n",
      "     19        \u001b[36m1.2083\u001b[0m        \u001b[32m1.3520\u001b[0m  0.1799\n",
      "     20        \u001b[36m1.1850\u001b[0m        \u001b[32m1.3344\u001b[0m  0.1281\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5925\u001b[0m        \u001b[32m1.6441\u001b[0m  0.1075\n",
      "      2        \u001b[36m1.6323\u001b[0m        \u001b[32m1.4001\u001b[0m  0.1026\n",
      "      3        \u001b[36m1.3773\u001b[0m        \u001b[32m1.3217\u001b[0m  0.1070\n",
      "      4        \u001b[36m1.2210\u001b[0m        \u001b[32m1.2303\u001b[0m  0.1044\n",
      "      5        \u001b[36m1.1045\u001b[0m        \u001b[32m1.1897\u001b[0m  0.1075\n",
      "      6        \u001b[36m1.0345\u001b[0m        \u001b[32m1.1891\u001b[0m  0.1291\n",
      "      7        \u001b[36m0.9853\u001b[0m        \u001b[32m1.1684\u001b[0m  0.1056\n",
      "      8        \u001b[36m0.9760\u001b[0m        \u001b[32m1.1571\u001b[0m  0.1100\n",
      "      9        \u001b[36m0.9538\u001b[0m        1.1811  0.1078\n",
      "     10        \u001b[36m0.9458\u001b[0m        1.1611  0.1213\n",
      "     11        \u001b[36m0.9149\u001b[0m        \u001b[32m1.1440\u001b[0m  0.1071\n",
      "     12        0.9172        \u001b[32m1.1342\u001b[0m  0.1059\n",
      "     13        \u001b[36m0.9094\u001b[0m        \u001b[32m1.1333\u001b[0m  0.1101\n",
      "     14        \u001b[36m0.8914\u001b[0m        1.1431  0.1076\n",
      "     15        \u001b[36m0.8873\u001b[0m        1.1347  0.1119\n",
      "     16        0.8932        \u001b[32m1.1307\u001b[0m  0.1047\n",
      "     17        \u001b[36m0.8799\u001b[0m        1.1639  0.1055\n",
      "     18        0.9015        1.1425  0.1079\n",
      "     19        0.9130        1.1407  0.1683\n",
      "     20        0.8862        1.1500  0.1241\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.5986\u001b[0m        \u001b[32m2.8277\u001b[0m  0.1086\n",
      "      2     2640.0563        \u001b[32m2.3276\u001b[0m  0.1081\n",
      "      3      164.3234        \u001b[32m2.1433\u001b[0m  0.1198\n",
      "      4      410.9077        \u001b[32m2.0354\u001b[0m  0.1149\n",
      "      5      467.9338        \u001b[32m1.9874\u001b[0m  0.1044\n",
      "      6      291.3622        \u001b[32m1.9346\u001b[0m  0.1062\n",
      "      7       \u001b[36m57.9758\u001b[0m        1.9442  0.2872\n",
      "      8       64.4429        1.9360  0.1393\n",
      "      9       \u001b[36m31.2861\u001b[0m        1.9598  0.1237\n",
      "     10       \u001b[36m16.7107\u001b[0m        1.9737  0.1149\n",
      "     11      164.4602        1.9584  0.1330\n",
      "     12      302.9242        2.0100  0.1120\n",
      "     13      643.8748        2.0559  0.1274\n",
      "     14     1819.1779        1.9951  0.1160\n",
      "     15     2361.1139        2.0946  0.1211\n",
      "     16     3339.5263        2.0350  0.1942\n",
      "     17     1628.9756        2.0943  0.1326\n",
      "     18      372.9054        2.0371  0.1873\n",
      "     19     2639.0007        2.0052  0.1598\n",
      "     20       20.1377        1.9912  0.1340\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m20698.0665\u001b[0m       \u001b[32m30.6480\u001b[0m  0.1061\n",
      "      2    \u001b[36m13510.6474\u001b[0m       \u001b[32m13.0636\u001b[0m  0.1099\n",
      "      3     \u001b[36m8523.5302\u001b[0m        \u001b[32m6.6612\u001b[0m  0.1129\n",
      "      4     \u001b[36m7162.9819\u001b[0m        \u001b[32m4.0176\u001b[0m  0.1071\n",
      "      5     \u001b[36m4213.3584\u001b[0m        \u001b[32m2.9954\u001b[0m  0.1088\n",
      "      6     \u001b[36m2916.0607\u001b[0m        \u001b[32m2.6378\u001b[0m  0.1061\n",
      "      7     \u001b[36m2021.7543\u001b[0m        \u001b[32m2.4388\u001b[0m  0.1060\n",
      "      8     \u001b[36m1610.2009\u001b[0m        \u001b[32m2.4148\u001b[0m  0.1092\n",
      "      9     \u001b[36m1088.7989\u001b[0m        \u001b[32m2.2329\u001b[0m  0.1073\n",
      "     10      \u001b[36m812.1731\u001b[0m        \u001b[32m2.0565\u001b[0m  0.1074\n",
      "     11      \u001b[36m509.3620\u001b[0m        \u001b[32m1.9793\u001b[0m  0.1078\n",
      "     12      \u001b[36m317.9013\u001b[0m        \u001b[32m1.9538\u001b[0m  0.1296\n",
      "     13       \u001b[36m25.1910\u001b[0m        \u001b[32m1.7445\u001b[0m  0.1020\n",
      "     14       \u001b[36m14.0651\u001b[0m        \u001b[32m1.6516\u001b[0m  0.1740\n",
      "     15        \u001b[36m1.3887\u001b[0m        \u001b[32m1.6478\u001b[0m  0.1197\n",
      "     16        \u001b[36m1.3048\u001b[0m        \u001b[32m1.6137\u001b[0m  0.1089\n",
      "     17        \u001b[36m1.2367\u001b[0m        \u001b[32m1.5949\u001b[0m  0.1086\n",
      "     18        \u001b[36m1.0681\u001b[0m        1.6474  0.1131\n",
      "     19        \u001b[36m1.0339\u001b[0m        1.6995  0.1093\n",
      "     20        \u001b[36m1.0147\u001b[0m        1.7425  0.1079\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m16.7182\u001b[0m        \u001b[32m4.5578\u001b[0m  0.1066\n",
      "      2        \u001b[36m4.4426\u001b[0m        \u001b[32m3.6679\u001b[0m  0.1070\n",
      "      3        \u001b[36m2.6175\u001b[0m        \u001b[32m2.7400\u001b[0m  0.1035\n",
      "      4       74.3233        \u001b[32m2.0956\u001b[0m  0.1065\n",
      "      5        \u001b[36m1.9049\u001b[0m        \u001b[32m1.8441\u001b[0m  0.1062\n",
      "      6        \u001b[36m1.4336\u001b[0m        \u001b[32m1.7903\u001b[0m  0.1058\n",
      "      7        \u001b[36m1.3192\u001b[0m        \u001b[32m1.6598\u001b[0m  0.1053\n",
      "      8        \u001b[36m1.2358\u001b[0m        \u001b[32m1.5701\u001b[0m  0.1042\n",
      "      9        \u001b[36m1.2145\u001b[0m        \u001b[32m1.5045\u001b[0m  0.1041\n",
      "     10        \u001b[36m1.1757\u001b[0m        \u001b[32m1.4811\u001b[0m  0.1485\n",
      "     11        \u001b[36m1.1460\u001b[0m        \u001b[32m1.4103\u001b[0m  0.1122\n",
      "     12        \u001b[36m1.1288\u001b[0m        \u001b[32m1.3796\u001b[0m  0.1384\n",
      "     13        \u001b[36m1.1084\u001b[0m        \u001b[32m1.3584\u001b[0m  0.1694\n",
      "     14        \u001b[36m1.0955\u001b[0m        \u001b[32m1.3488\u001b[0m  0.1195\n",
      "     15        \u001b[36m1.0909\u001b[0m        \u001b[32m1.3245\u001b[0m  0.1093\n",
      "     16        \u001b[36m1.0569\u001b[0m        1.3458  0.1065\n",
      "     17        \u001b[36m1.0552\u001b[0m        \u001b[32m1.2998\u001b[0m  0.1139\n",
      "     18        \u001b[36m1.0395\u001b[0m        1.3118  0.1107\n",
      "     19        \u001b[36m1.0319\u001b[0m        \u001b[32m1.2875\u001b[0m  0.1110\n",
      "     20        \u001b[36m1.0190\u001b[0m        1.2945  0.1095\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m27.8398\u001b[0m        \u001b[32m6.8444\u001b[0m  0.1033\n",
      "      2        \u001b[36m3.1981\u001b[0m        \u001b[32m2.2285\u001b[0m  0.1053\n",
      "      3        \u001b[36m1.8151\u001b[0m        2.4828  0.1056\n",
      "      4        \u001b[36m1.6369\u001b[0m        \u001b[32m1.8112\u001b[0m  0.1060\n",
      "      5        \u001b[36m1.3802\u001b[0m        \u001b[32m1.5943\u001b[0m  0.1115\n",
      "      6        \u001b[36m1.2738\u001b[0m        \u001b[32m1.4896\u001b[0m  0.1069\n",
      "      7        \u001b[36m1.2010\u001b[0m        \u001b[32m1.4185\u001b[0m  0.1120\n",
      "      8        \u001b[36m1.1637\u001b[0m        \u001b[32m1.3775\u001b[0m  0.1107\n",
      "      9        \u001b[36m1.1393\u001b[0m        \u001b[32m1.3483\u001b[0m  0.1071\n",
      "     10        \u001b[36m1.1271\u001b[0m        \u001b[32m1.3306\u001b[0m  0.1144\n",
      "     11        \u001b[36m1.1043\u001b[0m        \u001b[32m1.2984\u001b[0m  0.1040\n",
      "     12        \u001b[36m1.1001\u001b[0m        \u001b[32m1.2913\u001b[0m  0.1703\n",
      "     13        \u001b[36m1.0792\u001b[0m        \u001b[32m1.2836\u001b[0m  0.1395\n",
      "     14        \u001b[36m1.0691\u001b[0m        \u001b[32m1.2725\u001b[0m  0.1077\n",
      "     15        \u001b[36m1.0598\u001b[0m        \u001b[32m1.2619\u001b[0m  0.1094\n",
      "     16        \u001b[36m1.0499\u001b[0m        \u001b[32m1.2501\u001b[0m  0.1076\n",
      "     17        \u001b[36m1.0427\u001b[0m        \u001b[32m1.2431\u001b[0m  0.1184\n",
      "     18        \u001b[36m1.0393\u001b[0m        1.2553  0.1139\n",
      "     19        \u001b[36m1.0250\u001b[0m        \u001b[32m1.2368\u001b[0m  0.1151\n",
      "     20        \u001b[36m1.0172\u001b[0m        \u001b[32m1.2325\u001b[0m  0.1129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m6.6698\u001b[0m        \u001b[32m2.0830\u001b[0m  0.1018\n",
      "      2        \u001b[36m3.1379\u001b[0m        2.3272  0.1101\n",
      "      3        \u001b[36m2.3282\u001b[0m        \u001b[32m2.0302\u001b[0m  0.1097\n",
      "      4        \u001b[36m1.8261\u001b[0m        \u001b[32m1.9637\u001b[0m  0.1069\n",
      "      5        \u001b[36m1.6831\u001b[0m        \u001b[32m1.9283\u001b[0m  0.1120\n",
      "      6        \u001b[36m1.5519\u001b[0m        \u001b[32m1.8378\u001b[0m  0.1300\n",
      "      7        \u001b[36m1.4593\u001b[0m        \u001b[32m1.7738\u001b[0m  0.1160\n",
      "      8        \u001b[36m1.3490\u001b[0m        \u001b[32m1.6558\u001b[0m  0.1133\n",
      "      9        \u001b[36m1.2342\u001b[0m        \u001b[32m1.5219\u001b[0m  0.1134\n",
      "     10        \u001b[36m1.1437\u001b[0m        \u001b[32m1.4559\u001b[0m  0.1231\n",
      "     11        \u001b[36m1.0872\u001b[0m        \u001b[32m1.3742\u001b[0m  0.1407\n",
      "     12        \u001b[36m1.0443\u001b[0m        \u001b[32m1.3251\u001b[0m  0.1603\n",
      "     13        \u001b[36m0.9975\u001b[0m        \u001b[32m1.2843\u001b[0m  0.1219\n",
      "     14        \u001b[36m0.9737\u001b[0m        \u001b[32m1.2595\u001b[0m  0.1157\n",
      "     15        \u001b[36m0.9615\u001b[0m        \u001b[32m1.2343\u001b[0m  0.1135\n",
      "     16        \u001b[36m0.9455\u001b[0m        \u001b[32m1.2095\u001b[0m  0.1204\n",
      "     17        \u001b[36m0.9340\u001b[0m        \u001b[32m1.1960\u001b[0m  0.1123\n",
      "     18        \u001b[36m0.9258\u001b[0m        \u001b[32m1.1868\u001b[0m  0.1134\n",
      "     19        0.9308        \u001b[32m1.1842\u001b[0m  0.1072\n",
      "     20        \u001b[36m0.9107\u001b[0m        \u001b[32m1.1656\u001b[0m  0.1117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m176103.2637\u001b[0m        \u001b[32m3.0240\u001b[0m  0.1125\n",
      "      2   \u001b[36m108474.7622\u001b[0m        3.2793  0.1255\n",
      "      3    \u001b[36m64589.5840\u001b[0m        3.1492  0.1355\n",
      "      4    \u001b[36m22606.6544\u001b[0m        \u001b[32m2.4983\u001b[0m  0.1100\n",
      "      5     \u001b[36m6070.2584\u001b[0m        \u001b[32m1.8970\u001b[0m  0.1072\n",
      "      6     \u001b[36m2626.6959\u001b[0m        \u001b[32m1.6568\u001b[0m  0.1072\n",
      "      7      \u001b[36m455.8549\u001b[0m        \u001b[32m1.5451\u001b[0m  0.1103\n",
      "      8       \u001b[36m33.2766\u001b[0m        \u001b[32m1.4718\u001b[0m  0.1096\n",
      "      9        \u001b[36m1.9482\u001b[0m        \u001b[32m1.4400\u001b[0m  0.1058\n",
      "     10        \u001b[36m1.2538\u001b[0m        \u001b[32m1.4191\u001b[0m  0.1731\n",
      "     11        1.3248        1.4203  0.1348\n",
      "     12        \u001b[36m1.1887\u001b[0m        \u001b[32m1.4132\u001b[0m  0.1263\n",
      "     13        \u001b[36m1.1267\u001b[0m        1.4161  0.1086\n",
      "     14        \u001b[36m1.0708\u001b[0m        1.4226  0.1105\n",
      "     15        \u001b[36m1.0581\u001b[0m        1.4204  0.1058\n",
      "     16        \u001b[36m1.0509\u001b[0m        1.4286  0.1080\n",
      "     17        \u001b[36m1.0371\u001b[0m        1.4294  0.1138\n",
      "     18        \u001b[36m1.0291\u001b[0m        1.4236  0.1147\n",
      "     19        \u001b[36m1.0224\u001b[0m        1.4321  0.1212\n",
      "     20        \u001b[36m1.0179\u001b[0m        1.4299  0.1201\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.7993\u001b[0m   \u001b[32m916069.4104\u001b[0m  0.1114\n",
      "      2        \u001b[36m7.0091\u001b[0m   \u001b[32m218357.7277\u001b[0m  0.1088\n",
      "      3        \u001b[36m2.7691\u001b[0m    \u001b[32m23136.1883\u001b[0m  0.1048\n",
      "      4        \u001b[36m2.1343\u001b[0m     \u001b[32m5975.8649\u001b[0m  0.1039\n",
      "      5        \u001b[36m1.9072\u001b[0m    48267.5435  0.1043\n",
      "      6        \u001b[36m1.7500\u001b[0m   164471.3949  0.1083\n",
      "      7        \u001b[36m1.6064\u001b[0m   364374.7429  0.1184\n",
      "      8        \u001b[36m1.4858\u001b[0m   611657.1773  0.1072\n",
      "      9        \u001b[36m1.3794\u001b[0m   952719.4359  0.1462\n",
      "     10        \u001b[36m1.2923\u001b[0m  1462907.5300  0.1311\n",
      "     11        \u001b[36m1.2230\u001b[0m  1981475.2031  0.1268\n",
      "     12        \u001b[36m1.1561\u001b[0m  2678914.9344  0.1086\n",
      "     13        \u001b[36m1.1071\u001b[0m  3641292.7930  0.1081\n",
      "     14        \u001b[36m1.0656\u001b[0m  4633586.5642  0.1081\n",
      "     15        \u001b[36m1.0368\u001b[0m  5719342.0295  0.1103\n",
      "     16        \u001b[36m1.0035\u001b[0m  6741647.3502  0.1196\n",
      "     17        \u001b[36m0.9836\u001b[0m  7522195.2403  0.1198\n",
      "     18        \u001b[36m0.9609\u001b[0m  8392811.5510  0.1078\n",
      "     19        \u001b[36m0.9485\u001b[0m  8868791.7855  0.1092\n",
      "     20        \u001b[36m0.9349\u001b[0m  9492380.8124  0.1104\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m113790.3864\u001b[0m       \u001b[32m21.5247\u001b[0m  0.1451\n",
      "      2    \u001b[36m98144.8914\u001b[0m        \u001b[32m7.1991\u001b[0m  0.1101\n",
      "      3    \u001b[36m80708.9857\u001b[0m        \u001b[32m3.2373\u001b[0m  0.1075\n",
      "      4    \u001b[36m74150.7963\u001b[0m        \u001b[32m2.1254\u001b[0m  0.1143\n",
      "      5    \u001b[36m58735.1369\u001b[0m        \u001b[32m1.8290\u001b[0m  0.1062\n",
      "      6    \u001b[36m53308.1434\u001b[0m        1.8560  0.1050\n",
      "      7    \u001b[36m44484.9168\u001b[0m        1.9179  0.1073\n",
      "      8    \u001b[36m40733.5707\u001b[0m        2.0170  0.1470\n",
      "      9    \u001b[36m35011.2235\u001b[0m        1.9757  0.1390\n",
      "     10    \u001b[36m29945.1063\u001b[0m        1.9684  0.1241\n",
      "     11    \u001b[36m28170.4515\u001b[0m        1.9254  0.1044\n",
      "     12    \u001b[36m24295.5218\u001b[0m        1.9721  0.1098\n",
      "     13    \u001b[36m21255.1410\u001b[0m        1.8980  0.1119\n",
      "     14    \u001b[36m19333.4155\u001b[0m        1.8488  0.1141\n",
      "     15    \u001b[36m17404.1102\u001b[0m        1.8388  0.1134\n",
      "     16    \u001b[36m15941.0239\u001b[0m        \u001b[32m1.8074\u001b[0m  0.1143\n",
      "     17    \u001b[36m14433.0088\u001b[0m        \u001b[32m1.7900\u001b[0m  0.1044\n",
      "     18    \u001b[36m12898.6046\u001b[0m        1.7996  0.1060\n",
      "     19    \u001b[36m11749.1331\u001b[0m        \u001b[32m1.7791\u001b[0m  0.1064\n",
      "     20    \u001b[36m10689.2875\u001b[0m        \u001b[32m1.7352\u001b[0m  0.1069\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m6637.4666\u001b[0m      \u001b[32m157.6564\u001b[0m  0.1098\n",
      "      2    45855.7361       \u001b[32m63.8369\u001b[0m  0.1086\n",
      "      3      \u001b[36m935.7791\u001b[0m       \u001b[32m42.2911\u001b[0m  0.1082\n",
      "      4     5527.6275       \u001b[32m22.9147\u001b[0m  0.1098\n",
      "      5     1200.8688       \u001b[32m10.6181\u001b[0m  0.1062\n",
      "      6    19520.5004        \u001b[32m7.9863\u001b[0m  0.1065\n",
      "      7     6806.4838        \u001b[32m7.9362\u001b[0m  0.1253\n",
      "      8      \u001b[36m193.0620\u001b[0m        \u001b[32m5.7071\u001b[0m  0.1685\n",
      "      9     4417.9112        \u001b[32m5.3256\u001b[0m  0.1340\n",
      "     10     1326.4725        \u001b[32m4.9247\u001b[0m  0.1268\n",
      "     11       \u001b[36m53.3067\u001b[0m        \u001b[32m4.4189\u001b[0m  0.1178\n",
      "     12       \u001b[36m18.5427\u001b[0m        \u001b[32m4.1763\u001b[0m  0.1133\n",
      "     13       50.9713        \u001b[32m3.8552\u001b[0m  0.1083\n",
      "     14        \u001b[36m6.3618\u001b[0m        \u001b[32m3.5910\u001b[0m  0.1086\n",
      "     15       25.2429        \u001b[32m3.4051\u001b[0m  0.1071\n",
      "     16        \u001b[36m2.5062\u001b[0m        \u001b[32m3.2784\u001b[0m  0.1070\n",
      "     17       36.6660        \u001b[32m3.0908\u001b[0m  0.1088\n",
      "     18       39.0771        \u001b[32m3.0130\u001b[0m  0.1050\n",
      "     19       40.1282        \u001b[32m2.8589\u001b[0m  0.1068\n",
      "     20       55.3460        \u001b[32m2.7916\u001b[0m  0.1072\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1  \u001b[36m1574889.5035\u001b[0m       \u001b[32m61.2921\u001b[0m  0.1047\n",
      "      2  \u001b[36m1307430.9848\u001b[0m       \u001b[32m27.6986\u001b[0m  0.1061\n",
      "      3   \u001b[36m983588.3566\u001b[0m       \u001b[32m12.5329\u001b[0m  0.1054\n",
      "      4   \u001b[36m694900.3062\u001b[0m        \u001b[32m5.9357\u001b[0m  0.1064\n",
      "      5   \u001b[36m507546.5310\u001b[0m        \u001b[32m3.5559\u001b[0m  0.1047\n",
      "      6   \u001b[36m391558.1764\u001b[0m        \u001b[32m3.0276\u001b[0m  0.1053\n",
      "      7   \u001b[36m213156.1379\u001b[0m        \u001b[32m2.5055\u001b[0m  0.1705\n",
      "      8   \u001b[36m174860.0766\u001b[0m        3.0203  0.1564\n",
      "      9    \u001b[36m81253.5668\u001b[0m        2.8148  0.1221\n",
      "     10    \u001b[36m50831.5291\u001b[0m        2.9289  0.1130\n",
      "     11    \u001b[36m26773.6648\u001b[0m        2.7215  0.1102\n",
      "     12    \u001b[36m12072.2292\u001b[0m        \u001b[32m2.4249\u001b[0m  0.1084\n",
      "     13     \u001b[36m8170.0575\u001b[0m        \u001b[32m2.3018\u001b[0m  0.1091\n",
      "     14     \u001b[36m2149.8019\u001b[0m        \u001b[32m2.1014\u001b[0m  0.1102\n",
      "     15      \u001b[36m786.3380\u001b[0m        \u001b[32m1.9554\u001b[0m  0.1040\n",
      "     16     1936.5598        \u001b[32m1.9107\u001b[0m  0.1123\n",
      "     17       \u001b[36m11.2973\u001b[0m        1.9334  0.1115\n",
      "     18        \u001b[36m6.1175\u001b[0m        \u001b[32m1.9002\u001b[0m  0.1094\n",
      "     19        \u001b[36m5.8285\u001b[0m        \u001b[32m1.8638\u001b[0m  0.1070\n",
      "     20        6.3958        \u001b[32m1.8373\u001b[0m  0.1124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1  \u001b[36m2636976.6361\u001b[0m        \u001b[32m1.6958\u001b[0m  0.1118\n",
      "      2  \u001b[36m2343006.7662\u001b[0m        \u001b[32m1.6044\u001b[0m  0.1077\n",
      "      3  \u001b[36m2019346.5729\u001b[0m        \u001b[32m1.4801\u001b[0m  0.1079\n",
      "      4  \u001b[36m1656422.5103\u001b[0m        1.4939  0.1049\n",
      "      5  \u001b[36m1499969.4309\u001b[0m        1.5348  0.1166\n",
      "      6  \u001b[36m1253509.4112\u001b[0m        1.5993  0.1718\n",
      "      7  \u001b[36m1151220.3840\u001b[0m        1.5655  0.1616\n",
      "      8   \u001b[36m958779.9640\u001b[0m        1.5995  0.1416\n",
      "      9   \u001b[36m892673.1336\u001b[0m        1.6429  0.1192\n",
      "     10   \u001b[36m750793.2998\u001b[0m        1.6068  0.1082\n",
      "     11   \u001b[36m698608.3259\u001b[0m        1.6366  0.1187\n",
      "     12   \u001b[36m589727.9235\u001b[0m        1.5983  0.1189\n",
      "     13   \u001b[36m554012.3188\u001b[0m        1.6906  0.1096\n",
      "     14   \u001b[36m463015.1491\u001b[0m        1.6850  0.1060\n",
      "     15   \u001b[36m424713.2461\u001b[0m        1.7177  0.1097\n",
      "     16   \u001b[36m382413.1969\u001b[0m        1.8856  0.1238\n",
      "     17   \u001b[36m339991.6073\u001b[0m        1.8978  0.1099\n",
      "     18   \u001b[36m289672.7101\u001b[0m        1.8696  0.1037\n",
      "     19   \u001b[36m263736.1774\u001b[0m        2.0454  0.1088\n",
      "     20   \u001b[36m235924.6427\u001b[0m        1.9406  0.1076\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m79000.3991\u001b[0m        \u001b[32m1.8652\u001b[0m  0.1133\n",
      "      2    \u001b[36m53754.9598\u001b[0m        \u001b[32m1.2497\u001b[0m  0.1135\n",
      "      3    \u001b[36m44890.0127\u001b[0m        \u001b[32m1.0674\u001b[0m  0.1158\n",
      "      4    \u001b[36m27536.9339\u001b[0m        \u001b[32m1.0006\u001b[0m  0.1094\n",
      "      5    \u001b[36m18794.5517\u001b[0m        \u001b[32m0.9762\u001b[0m  0.1576\n",
      "      6    \u001b[36m13510.2280\u001b[0m        \u001b[32m0.9672\u001b[0m  0.1266\n",
      "      7    \u001b[36m11192.1780\u001b[0m        \u001b[32m0.9582\u001b[0m  0.1120\n",
      "      8     \u001b[36m6150.9444\u001b[0m        \u001b[32m0.9381\u001b[0m  0.1105\n",
      "      9     \u001b[36m3755.2447\u001b[0m        \u001b[32m0.9345\u001b[0m  0.1176\n",
      "     10     \u001b[36m2578.1013\u001b[0m        \u001b[32m0.9340\u001b[0m  0.1089\n",
      "     11     \u001b[36m1393.5547\u001b[0m        \u001b[32m0.9309\u001b[0m  0.1105\n",
      "     12      \u001b[36m887.1647\u001b[0m        0.9474  0.1071\n",
      "     13      \u001b[36m496.9831\u001b[0m        0.9382  0.1091\n",
      "     14      \u001b[36m257.1061\u001b[0m        0.9591  0.1043\n",
      "     15      \u001b[36m123.2925\u001b[0m        0.9488  0.1092\n",
      "     16      \u001b[36m114.1162\u001b[0m        0.9756  0.1100\n",
      "     17       \u001b[36m27.3096\u001b[0m        0.9781  0.1117\n",
      "     18        \u001b[36m6.4867\u001b[0m        1.0131  0.1132\n",
      "     19        \u001b[36m3.2476\u001b[0m        1.0092  0.1116\n",
      "     20        \u001b[36m2.2834\u001b[0m        1.0343  0.1140\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m207170.7257\u001b[0m        \u001b[32m1.9267\u001b[0m  0.1557\n",
      "      2   \u001b[36m127327.0185\u001b[0m        2.2046  0.1077\n",
      "      3    \u001b[36m88073.9236\u001b[0m        2.5720  0.1208\n",
      "      4    \u001b[36m40414.2828\u001b[0m        2.8353  0.1688\n",
      "      5    \u001b[36m20054.8588\u001b[0m        3.0588  0.1210\n",
      "      6     \u001b[36m9851.4088\u001b[0m        3.2079  0.1113\n",
      "      7     \u001b[36m4315.3469\u001b[0m        3.3462  0.1075\n",
      "      8     \u001b[36m1529.7491\u001b[0m        3.3159  0.1091\n",
      "      9      \u001b[36m446.8971\u001b[0m        3.2077  0.1089\n",
      "     10      \u001b[36m252.5779\u001b[0m        2.9943  0.1079\n",
      "     11       \u001b[36m22.2501\u001b[0m        2.7913  0.1067\n",
      "     12       \u001b[36m18.2357\u001b[0m        2.6269  0.1073\n",
      "     13       \u001b[36m17.9600\u001b[0m        2.5247  0.1130\n",
      "     14       \u001b[36m17.7433\u001b[0m        2.3721  0.1098\n",
      "     15       \u001b[36m17.4444\u001b[0m        2.2788  0.1103\n",
      "     16       \u001b[36m17.2700\u001b[0m        2.1898  0.1116\n",
      "     17       \u001b[36m17.0631\u001b[0m        2.1082  0.1111\n",
      "     18       \u001b[36m16.7415\u001b[0m        2.0243  0.1115\n",
      "     19       \u001b[36m15.8015\u001b[0m        1.9794  0.1028\n",
      "     20       \u001b[36m14.7537\u001b[0m        1.9660  0.1075\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m54.0333\u001b[0m   \u001b[32m159185.2068\u001b[0m  0.1094\n",
      "      2       \u001b[36m21.0127\u001b[0m   \u001b[32m148906.8735\u001b[0m  0.1281\n",
      "      3        \u001b[36m9.1549\u001b[0m   \u001b[32m120807.1038\u001b[0m  0.1728\n",
      "      4        \u001b[36m4.2777\u001b[0m   170618.8752  0.1277\n",
      "      5        \u001b[36m2.5661\u001b[0m   241918.0323  0.1079\n",
      "      6        \u001b[36m1.9757\u001b[0m   388610.2467  0.1099\n",
      "      7        \u001b[36m1.7254\u001b[0m   489167.4766  0.1073\n",
      "      8        \u001b[36m1.5540\u001b[0m   604053.4618  0.1107\n",
      "      9        \u001b[36m1.4474\u001b[0m   708512.1030  0.1120\n",
      "     10        \u001b[36m1.3825\u001b[0m   818454.9179  0.1126\n",
      "     11        \u001b[36m1.2995\u001b[0m   964418.4112  0.1152\n",
      "     12        \u001b[36m1.2503\u001b[0m  1088862.4225  0.1136\n",
      "     13        \u001b[36m1.1969\u001b[0m  1223209.9073  0.1080\n",
      "     14        \u001b[36m1.1534\u001b[0m  1323303.3887  0.1168\n",
      "     15        \u001b[36m1.1269\u001b[0m  1391921.5349  0.1089\n",
      "     16        \u001b[36m1.0830\u001b[0m  1499455.3273  0.1136\n",
      "     17        \u001b[36m1.0632\u001b[0m  1504274.5716  0.1049\n",
      "     18        \u001b[36m1.0427\u001b[0m  1517341.9546  0.1052\n",
      "     19        \u001b[36m1.0269\u001b[0m  1565266.3257  0.1068\n",
      "     20        1.0315  1554252.9720  0.1260\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2618.8503\u001b[0m      \u001b[32m625.5080\u001b[0m  0.1133\n",
      "      2      \u001b[36m654.7721\u001b[0m      \u001b[32m290.3553\u001b[0m  0.1490\n",
      "      3      \u001b[36m298.4987\u001b[0m      \u001b[32m119.6604\u001b[0m  0.1384\n",
      "      4       \u001b[36m89.4503\u001b[0m       \u001b[32m40.0846\u001b[0m  0.1080\n",
      "      5       \u001b[36m24.9163\u001b[0m       \u001b[32m12.7750\u001b[0m  0.1103\n",
      "      6        \u001b[36m7.6270\u001b[0m        \u001b[32m6.7388\u001b[0m  0.1092\n",
      "      7        \u001b[36m5.4360\u001b[0m        \u001b[32m6.0295\u001b[0m  0.1105\n",
      "      8        \u001b[36m4.6806\u001b[0m        \u001b[32m5.5738\u001b[0m  0.1076\n",
      "      9        \u001b[36m4.4512\u001b[0m        \u001b[32m4.5895\u001b[0m  0.1148\n",
      "     10        \u001b[36m3.4606\u001b[0m        \u001b[32m4.0294\u001b[0m  0.1067\n",
      "     11     1646.4054        \u001b[32m3.6684\u001b[0m  0.1167\n",
      "     12      543.6111        \u001b[32m3.2805\u001b[0m  0.1112\n",
      "     13      204.3374        \u001b[32m3.0625\u001b[0m  0.1084\n",
      "     14      296.4690        \u001b[32m2.7311\u001b[0m  0.1095\n",
      "     15       65.3994        \u001b[32m2.5035\u001b[0m  0.1058\n",
      "     16      162.1158        \u001b[32m2.3580\u001b[0m  0.1080\n",
      "     17        \u001b[36m1.8910\u001b[0m        \u001b[32m2.1260\u001b[0m  0.1102\n",
      "     18        2.3141        \u001b[32m1.9518\u001b[0m  0.1275\n",
      "     19        \u001b[36m1.8459\u001b[0m        \u001b[32m1.8860\u001b[0m  0.1084\n",
      "     20        \u001b[36m1.6012\u001b[0m        \u001b[32m1.7513\u001b[0m  0.1083\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1   \u001b[36m339702.5642\u001b[0m        \u001b[32m6.3851\u001b[0m  0.1380\n",
      "      2   \u001b[36m185569.9217\u001b[0m        \u001b[32m4.5024\u001b[0m  0.1653\n",
      "      3   \u001b[36m146215.2990\u001b[0m        \u001b[32m2.7005\u001b[0m  0.1165\n",
      "      4    \u001b[36m69925.8135\u001b[0m        \u001b[32m1.8769\u001b[0m  0.1151\n",
      "      5    \u001b[36m31232.2234\u001b[0m        \u001b[32m1.4752\u001b[0m  0.1097\n",
      "      6     \u001b[36m8723.0356\u001b[0m        \u001b[32m1.2651\u001b[0m  0.1241\n",
      "      7     \u001b[36m4349.1755\u001b[0m        \u001b[32m1.1613\u001b[0m  0.1144\n",
      "      8     \u001b[36m3673.3324\u001b[0m        \u001b[32m1.1483\u001b[0m  0.1071\n",
      "      9     \u001b[36m1027.0922\u001b[0m        1.2814  0.1116\n",
      "     10      \u001b[36m379.1812\u001b[0m        1.1745  0.1067\n",
      "     11      \u001b[36m134.9230\u001b[0m        \u001b[32m1.1449\u001b[0m  0.1056\n",
      "     12        \u001b[36m5.4507\u001b[0m        1.1456  0.1073\n",
      "     13       10.8506        1.1598  0.1092\n",
      "     14        \u001b[36m1.0588\u001b[0m        1.1681  0.1069\n",
      "     15        1.0592        1.1565  0.1064\n",
      "     16        \u001b[36m0.9993\u001b[0m        1.1553  0.1252\n",
      "     17        \u001b[36m0.9863\u001b[0m        1.1587  0.1119\n",
      "     18        \u001b[36m0.9754\u001b[0m        1.1578  0.1074\n",
      "     19        \u001b[36m0.9693\u001b[0m        1.1532  0.1047\n",
      "     20        \u001b[36m0.9644\u001b[0m        1.1534  0.1167\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1    \u001b[36m18887.0517\u001b[0m       \u001b[32m12.9831\u001b[0m  0.1599\n",
      "      2    \u001b[36m11989.6828\u001b[0m        \u001b[32m2.4739\u001b[0m  0.1130\n",
      "      3     \u001b[36m9148.5857\u001b[0m        \u001b[32m1.9153\u001b[0m  0.1173\n",
      "      4     \u001b[36m7516.6995\u001b[0m        2.5158  0.1078\n",
      "      5     \u001b[36m5064.7155\u001b[0m        2.4046  0.1166\n",
      "      6     \u001b[36m4328.8512\u001b[0m        2.6027  0.1088\n",
      "      7     \u001b[36m3050.9358\u001b[0m        2.3263  0.1019\n",
      "      8     \u001b[36m2531.9772\u001b[0m        2.2158  0.1055\n",
      "      9     \u001b[36m1933.7751\u001b[0m        2.1128  0.1072\n",
      "     10     \u001b[36m1490.1437\u001b[0m        \u001b[32m1.8164\u001b[0m  0.1055\n",
      "     11     \u001b[36m1293.8812\u001b[0m        \u001b[32m1.7240\u001b[0m  0.1070\n",
      "     12      \u001b[36m981.1009\u001b[0m        \u001b[32m1.4574\u001b[0m  0.1157\n",
      "     13      \u001b[36m902.8050\u001b[0m        1.5730  0.1072\n",
      "     14      \u001b[36m634.1664\u001b[0m        \u001b[32m1.3794\u001b[0m  0.1232\n",
      "     15      \u001b[36m597.7183\u001b[0m        \u001b[32m1.3293\u001b[0m  0.1161\n",
      "     16      \u001b[36m446.0314\u001b[0m        \u001b[32m1.2771\u001b[0m  0.1100\n",
      "     17      \u001b[36m349.1897\u001b[0m        1.3036  0.1068\n",
      "     18      \u001b[36m288.7308\u001b[0m        \u001b[32m1.2220\u001b[0m  0.1051\n",
      "     19      \u001b[36m256.1834\u001b[0m        1.3469  0.1084\n",
      "     20      \u001b[36m199.5551\u001b[0m        1.3127  0.1602\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rng</th>\n",
       "      <th>condition</th>\n",
       "      <th>model_performance</th>\n",
       "      <th>p_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.245532</td>\n",
       "      <td>0.045208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.048639</td>\n",
       "      <td>0.695896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>-0.048959</td>\n",
       "      <td>0.693993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>-0.081683</td>\n",
       "      <td>0.511104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.090634</td>\n",
       "      <td>0.465748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>-0.033826</td>\n",
       "      <td>0.785820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.179587</td>\n",
       "      <td>0.145899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.957979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.120309</td>\n",
       "      <td>0.332164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.090471</td>\n",
       "      <td>0.466554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rng condition  model_performance    p_vals\n",
       "0     0   pytorch           0.245532  0.045208\n",
       "1     1   pytorch           0.048639  0.695896\n",
       "2     2   pytorch          -0.048959  0.693993\n",
       "3     3   pytorch          -0.081683  0.511104\n",
       "4     4   pytorch           0.090634  0.465748\n",
       "..  ...       ...                ...       ...\n",
       "95   95   pytorch          -0.033826  0.785820\n",
       "96   96   pytorch           0.179587  0.145899\n",
       "97   97   pytorch           0.006560  0.957979\n",
       "98   98   pytorch           0.120309  0.332164\n",
       "99   99   pytorch           0.090471  0.466554\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powerkit.run_selected_condition('pytorch', rngs, n_jobs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
