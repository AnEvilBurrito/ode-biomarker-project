{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The following scripts analyses the performance of different models on a given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in CCLE data\n",
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in original ccle data\n",
    "loading_code = 'generic-gdsc-1-FGFR_0939-LN_IC50-fgfr4_ccle_dynamic_features_v2-true-Unnamed: 0'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"ModelSelectionFGFR4\"\n",
    "exp_id = \"test\"\n",
    "fixed_random_seed = 42 # -1 for no seed\n",
    "save_figure = False\n",
    "save_data = True\n",
    "show_figure = False\n",
    "\n",
    "all_models = ['SVR', 'RandomForestRegressor', 'XGBRegressor', 'MLPRegressor', 'KNeighborsRegressor', 'ElasticNet']\n",
    "\n",
    "if not os.path.exists(f'{path_loader.get_data_path()}data/results/{folder_name}'):\n",
    "    os.makedirs(f'{path_loader.get_data_path()}data/results/{folder_name}')\n",
    "\n",
    "file_save_path = f'{path_loader.get_data_path()}data/results/{folder_name}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit = Powerkit(feature_data, label_data)\n",
    "rngs = list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_func(X_train, y_train, rng, model_used, **kwargs):\n",
    "    k = X_train.shape[1]\n",
    "    selected_features, scores = f_regression_select(X_train, y_train, k)\n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(X_train, y_train, selected_features)\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}\n",
    "\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['SVR', 'RandomForestRegressor', 'XGBRegressor', 'MLPRegressor', 'KNeighborsRegressor', 'ElasticNet']\n",
    "for model_used in all_models:\n",
    "    powerkit.add_condition(model_used, False, pipeline_func, {'model_used': model_used}, eval_func, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models: \n",
    "    print(f'Running {model_used}...')\n",
    "    df = powerkit.run_selected_condition(model_used, rngs, 16, True)\n",
    "    if save_data:\n",
    "        print(f'Saved {model_used} to path')\n",
    "        df.to_pickle(f'{file_save_path}{exp_id}_{model_used}_simple.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Results from Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "df_list = []\n",
    "for model_used in all_models:\n",
    "    df = pd.read_pickle(f'{file_save_path}{exp_id}_{model_used}_simple.pkl')\n",
    "    df_list.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the dataframes\n",
    "df_all = pd.concat(df_list, axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualisation import plot_box_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "plot_box_plot(df_all, 'condition', 'model_performance', 'Model Performance', 'Model'\n",
    "              ,'Pearson Correlation', ax=ax, tick_fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2 - Simple Feature Selection\n",
    "\n",
    "To observe the effect of feature size on model performance, simple f-regression is used as a filtering step to allow only half of the features to pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit = Powerkit(feature_data, label_data)\n",
    "rngs = list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_func(X_train, y_train, rng, model_used, **kwargs):\n",
    "    k = X_train.shape[1]\n",
    "    selected_features, scores = f_regression_select(X_train, y_train, int(k/2))\n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(X_train, y_train, selected_features)\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}\n",
    "\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models:\n",
    "    powerkit.add_condition(model_used, False, pipeline_func, {'model_used': model_used}, eval_func, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models: \n",
    "    print(f'Running {model_used}...')\n",
    "    df = powerkit.run_selected_condition(model_used, rngs, 16, True)\n",
    "    if save_data:\n",
    "        print(f'Saved {model_used} to path')\n",
    "        df.to_pickle(f'{file_save_path}{exp_id}_{model_used}_control_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df_list = []\n",
    "for model_used in all_models:\n",
    "    df = pd.read_pickle(f'{file_save_path}{exp_id}_{model_used}_simple.pkl')\n",
    "    df_list.append(df)\n",
    "# join all the dataframes\n",
    "df_all = pd.concat(df_list, axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualisation import plot_box_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "plot_box_plot(df_all, 'condition', 'model_performance', 'Model Performance With Simple Feature Selection', 'Model'\n",
    "              ,'Pearson Correlation', ax=ax, tick_fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3 - Ensemble Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit = Powerkit(feature_data, label_data)\n",
    "rngs = list(range(100))\n",
    "\n",
    "all_methods = [f_regression_select, pearson_corr_select, mutual_information_select, relieff_select, rf_select]\n",
    "all_kwargs = [{}, {'return_all': False}, {}, {}, {'random_state': 42}]\n",
    "all_ways = ['one_way', 'two_way', 'one_way', 'one_way', 'one_way']\n",
    "# features, scores = ensemble_percentile_threshold(feature_data, label_data, -1, all_methods, all_kwargs, all_ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_args = {'all_methods': all_methods, 'all_kwargs': all_kwargs, 'all_ways': all_ways}\n",
    "\n",
    "def ensemble_pipeline_func(X_train, y_train, rng, model_used, all_methods, all_kwargs, all_ways, **kwargs):\n",
    "    selected_features, scores = ensemble_percentile_threshold(X_train, y_train, -1, all_methods, all_kwargs, all_ways, n_jobs=4)\n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(X_train, y_train, selected_features)\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}\n",
    "\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models:\n",
    "    powerkit.add_condition(model_used, True, pipeline_func, {'model_used': model_used}, eval_func, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models:\n",
    "    print(f'Running {model_used}...')\n",
    "    df = powerkit.run_selected_condition(model_used, rngs, 4, True)\n",
    "    if save_data:\n",
    "        print(f'Saved {model_used} to path')\n",
    "        df.to_pickle(f'{file_save_path}{exp_id}_{model_used}_ensemble.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3 - A more efficient implementation\n",
    "\n",
    "Processes can be made more efficient by pre-computing the ensemble feature selection step. The pre-computed ensemble features from different spliting seeds can then be applied to each model.\n",
    "\n",
    "Subsequently, the rng seed can be matched later in the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs feature selection \n",
    "\n",
    "powerkit = Powerkit(feature_data, label_data)\n",
    "rngs = list(range(100))\n",
    "# lite version \n",
    "all_methods = [f_regression_select, pearson_corr_select, mutual_information_select]\n",
    "all_kwargs = [{}, {'return_all': False}, {}]\n",
    "all_ways = ['one_way', 'two_way', 'one_way']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import pickle \n",
    "\n",
    "feature_selection_data = {}\n",
    "\n",
    "for i, rng in enumerate(rngs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, test_size=0.1, random_state=rng)\n",
    "    try: \n",
    "        selected_features, scores = ensemble_percentile_threshold(X_train, y_train, -1, all_methods, all_kwargs, all_ways, n_jobs=-1)\n",
    "        print(f'Finished {i+1} of {len(rngs)}')\n",
    "        feature_selection_data[rng] = (selected_features, scores)\n",
    "    except Exception as e:\n",
    "        print(f'Failed {i+1} of {len(rngs)} with error: {e}')\n",
    "        continue\n",
    "    \n",
    "with open(f'{file_save_path}{exp_id}_ensemble_feature_selection_split_data_100.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_selection_data, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load in the feature selection data\n",
    "with open(f'{file_save_path}{exp_id}_ensemble_feature_selection_split_data_100.pkl', 'rb') as f:\n",
    "    feature_selection_data = pickle.load(f)\n",
    "    \n",
    "print(f'Size of feature selection data: {len(feature_selection_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_pipeline_func_pre_compute(X_train, y_train, rng, model_used, feature_selection_data, **kwargs):\n",
    "    if rng in feature_selection_data:\n",
    "        selected_features, scores = feature_selection_data[rng]\n",
    "    else: \n",
    "        return {'model': None}\n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(X_train, y_train, selected_features)\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}\n",
    "    \n",
    "def eval_func_pre_compute(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    if pipeline_components['model'] is None:\n",
    "        return {'model_performance': 0, 'p_vals': 0, 'feature_importance': 0}\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models:\n",
    "    pipeline_args = {'model_used': model_used, 'feature_selection_data': feature_selection_data}\n",
    "    powerkit.add_condition(model_used, True, ensemble_pipeline_func_pre_compute, pipeline_args, eval_func_pre_compute, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_used in all_models:\n",
    "    print(f'Running {model_used}...')\n",
    "    df = powerkit.run_selected_condition(model_used, rngs, 16, True)\n",
    "    if save_data:\n",
    "        print(f'Saved {model_used} to path')\n",
    "        df.to_pickle(f'{file_save_path}{exp_id}_{model_used}_ensemble.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df_list = []\n",
    "for model_used in all_models:\n",
    "    df = pd.read_pickle(f'{file_save_path}{exp_id}_{model_used}_ensemble.pkl')\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(df_list, axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column with the best model performance\n",
    "\n",
    "best_model = df_all.groupby('condition')['model_performance'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.iloc[67+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualisation import plot_box_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "plot_box_plot(df_all, 'condition', 'model_performance', 'Model Performance With Ensemble Feature Selection', 'Model'\n",
    "              ,'Pearson Correlation', ax=ax, tick_fontsize=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
