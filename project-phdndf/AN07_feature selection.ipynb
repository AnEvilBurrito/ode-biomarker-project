{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection by Filtering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in CCLE data\n",
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in original ccle data\n",
    "loading_code = 'generic-gdsc-1-FGFR_0939-LN_IC50-fgfr4_ccle_dynamic_features-true-Row'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from toolkit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"FeatureSelectionFGFR4\"\n",
    "exp_id = \"range1000_nml\"\n",
    "fixed_random_seed = 42 # -1 for no seed\n",
    "save_figure = True\n",
    "save_data = True\n",
    "show_figure = True  \n",
    "\n",
    "feature_size = feature_data.shape[1]\n",
    "key_args = {'random_state': fixed_random_seed}\n",
    "\n",
    "if not os.path.exists(f'{path_loader.get_data_path()}data/results/{folder_name}'):\n",
    "    os.makedirs(f'{path_loader.get_data_path()}data/results/{folder_name}')\n",
    "\n",
    "file_save_path = f'{path_loader.get_data_path()}data/results/{folder_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(selected_indices, scores, feature_data): \n",
    "    # if selected_indices are not indices but labels, skip the label conversion step \n",
    "    if not isinstance(selected_indices[0], int) and not isinstance(selected_indices[0], np.int64):\n",
    "        labels = selected_indices\n",
    "    else:\n",
    "        labels = feature_data.columns[selected_indices]\n",
    "    df = pd.DataFrame({'Selected': selected_indices, 'Scores': scores}, index=labels)\n",
    "    sorted_df = df.sort_values(by='Scores', ascending=False)\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolkit Method Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_methods = [f_regression_select, pearson_corr_select]\n",
    "all_kwargs = [{}, {'return_all': False}]\n",
    "all_ways = ['one_way', 'two_way']\n",
    "features, scores = ensemble_percentile_threshold(feature_data, label_data, -1, all_methods, all_kwargs, all_ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "while i < len(features): \n",
    "    print(f'Features {i}: {features[i]}, Scores: {scores[i]}')\n",
    "    i += 1 \n",
    "    \n",
    "print(f'Total number of features: {len(features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_selected, rf_score = rf_select(feature_data, label_data, feature_size, **key_args)\n",
    "rf_select_df = build_dataframe(rf_selected, rf_score, feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_select_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relieff_selected, relieff_score = relieff_select(feature_data, label_data, feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relieff_select_df = build_dataframe(relieff_selected, relieff_score, feature_data)\n",
    "relieff_select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selected, f_score = f_regression_select(feature_data, label_data, feature_size)\n",
    "f_df = build_dataframe(f_selected, f_score, feature_data)\n",
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_kwargs = {'return_all': True}\n",
    "pearson_selected, pearson_score, pearson_pvals = pearson_corr_select(feature_data, label_data, feature_size, **pearson_kwargs)\n",
    "pearson_df = build_dataframe(pearson_selected, pearson_score, feature_data)\n",
    "pearson_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_selected, mutual_score = mutual_information_select(feature_data, label_data, feature_size)\n",
    "mutual_df = build_dataframe(mutual_selected, mutual_score, feature_data)\n",
    "mutual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two options to ensemble the results of multiple different feature selection results \n",
    "1. To propose a universal k-cutoff rank number for all feature selection methods and then union the features \n",
    "    e.g. Gather FS Method 1's top 100 features and Gather FS Method 2's top 100 features and union them \n",
    "2. To ensemble the rank value of each feature from each method first, and then limit the number of features either using a k-cutoff rank number, or a k-cutoff threshold for the ensemble score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal K-Cutoff Rank Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50 \n",
    "\n",
    "def ensemble_k_rank_select(k: int, selection_methods: list, method_kwargs: list, feature_data, label_data):\n",
    "    all_dfs = []\n",
    "    assert len(selection_methods) == len(method_kwargs), 'Number of methods and method_kwargs must be equal'\n",
    "    for idx, method in enumerate(selection_methods):\n",
    "        selected, score = method(feature_data, label_data, feature_size, **method_kwargs[idx])\n",
    "        df = build_dataframe(selected, score, feature_data)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    all_labels = union_df_labels(k,all_dfs)\n",
    "    return all_labels\n",
    "\n",
    "def union_df_labels(k, df_list):\n",
    "    all_labels = []\n",
    "    for df in df_list:\n",
    "        k_best_labels = df.index.tolist()[:k]\n",
    "        all_labels.extend(k_best_labels)\n",
    "    \n",
    "    all_labels = list(set(all_labels))\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_methods = [rf_select, relieff_select, f_regression_select, pearson_corr_select, mutual_information_select]\n",
    "# method_kwargs = [{'random_state': 42}, {}, {}, {}, {}]\n",
    "# ensemble_feature_list = ensemble_k_rank_select(k, select_methods, method_kwargs, feature_data, label_data)\n",
    "\n",
    "df_list = [rf_select_df, relieff_select_df, f_df, pearson_df, mutual_df]\n",
    "ensemble_labels = union_df_labels(k,df_list)\n",
    "ensemble_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Curve Analysis\n",
    "\n",
    "based on a k ranging from 0 to n, we can visualise a line plot to the number of ensemble features selected vs the k value\n",
    "\n",
    "if there is zero overlap, we can expect ensemble features's size to always be k * no. methods \n",
    "However, if the overlap is strong, ensemble's feature size should approach k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "max_k = 200\n",
    "df_list = [rf_select_df, relieff_select_df, f_df, pearson_df, mutual_df]\n",
    "ensemble_features_length_list = []\n",
    "k_list = list(range(1, max_k+1))\n",
    "for k in k_list:\n",
    "    ensemble_labels = union_df_labels(k,df_list)\n",
    "    ensemble_features_length_list.append(len(ensemble_labels))\n",
    "\n",
    "\n",
    "# set plot size \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_list, ensemble_features_length_list, label='Actual')\n",
    "plt.plot(k_list, k_list, linestyle='--', color='black', label='Low Threshold')\n",
    "plt.plot(k_list, [k*len(df_list) if k*len(df_list) <= feature_size else feature_size for k in k_list], linestyle='--', color='red', label='High Threshold')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Overlap K-Curve Analysis for Ensemble Feature Selection')\n",
    "plt.legend()\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_ensemble_k_curve.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sungyoung's statistical method \n",
    "\n",
    "By generating imputed samples which has shuffled targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = list(range(1, 1000))    \n",
    "shuffled_label_data = [label_data.sample(frac=1, random_state=seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = feature_data.shape[1]\n",
    "key_args = {'random_state': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selected, f_score = f_regression_select(feature_data, shuffled_label_data[0], feature_size)\n",
    "f_df = build_dataframe(f_selected, f_score, feature_data)\n",
    "f_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for all shuffled label data, join the scores and plot histogram distribution of scores \n",
    "\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import seaborn as sns  # type: ignore\n",
    "from joblib import Parallel, delayed, cpu_count # type: ignore\n",
    "\n",
    "\n",
    "def get_shuffled_scores(shuffled_label_data, feature_data, selection_method, method_kwargs, verbose=1, n_jobs=1):\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = cpu_count()\n",
    "    if n_jobs == 1:\n",
    "        all_scores = []\n",
    "        for i, label_data in enumerate(shuffled_label_data):\n",
    "            selected, score = selection_method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "            all_scores.extend(score)\n",
    "            if verbose == 1: \n",
    "                print(f'Finished {i+1} out of {len(shuffled_label_data)}')\n",
    "    else: \n",
    "        # use joblib to parallelize the process\n",
    "        def run_one(label_data):\n",
    "            selected, score = selection_method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "            return score\n",
    "        # process shuffled data 20 at a time, so that we can see progress, then concatenate at the end\n",
    "        divide_n = 20\n",
    "        if n_jobs > 20:\n",
    "            divide_n = 40\n",
    "        shuffled_label_data_chunks = [shuffled_label_data[i:i + divide_n] for i in range(0, len(shuffled_label_data), divide_n)]\n",
    "        all_scores = []\n",
    "        for i, chunk in enumerate(shuffled_label_data_chunks):\n",
    "            scores = Parallel(n_jobs=n_jobs)(delayed(run_one)(label_data) for label_data in chunk)\n",
    "            all_scores.extend([score for sublist in scores for score in sublist])\n",
    "            if verbose == 1: \n",
    "                print(f'Finished {i+1} out of {len(shuffled_label_data_chunks)} chunks')\n",
    "                \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = f_regression_select\n",
    "method_string = 'f_regression'\n",
    "method_kwargs = {}\n",
    "all_scores = get_shuffled_scores(shuffled_label_data, feature_data, method, method_kwargs, n_jobs=1)\n",
    "\n",
    "# set plot size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_scores, bins=20, kde=True)\n",
    "plt.xlabel(f'{method_string} Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{method_string} Score Distribution for Shuffled Label Data')\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_{method_string}_shuffled_label_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top 5% of scores as the threshold for feature selection\n",
    "# remove nan first\n",
    "all_scores = [score for score in all_scores if not np.isnan(score)]\n",
    "threshold = np.percentile(all_scores, 95)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected, scores = method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "df = build_dataframe(selected, scores, feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with scores above the threshold\n",
    "df_above_threshold = df[df['Scores'] > threshold]\n",
    "df_above_threshold\n",
    "# save f_df_above_threshold to pkl file \n",
    "if save_data: df_above_threshold.to_pickle(f'{file_save_path}{exp_id}_{method_string}_above_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_above_threshold.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = pearson_corr_select\n",
    "method_string = 'pearson_correlation'\n",
    "method_kwargs = {'return_all': False}\n",
    "\n",
    "all_scores = get_shuffled_scores(shuffled_label_data, feature_data, method, method_kwargs)\n",
    "\n",
    "\n",
    "all_scores = [score for score in all_scores if not np.isnan(score)]\n",
    "upper_threshold = np.percentile(all_scores, 97.5)\n",
    "lower_threshold = np.percentile(all_scores, 2.5)\n",
    "print(upper_threshold, lower_threshold)\n",
    "\n",
    "\n",
    "# set plot size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_scores, bins=20, kde=True)\n",
    "plt.xlabel(f'{method_string} Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{method_string} Score Distribution for Shuffled Label Data')\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_{method_string}_shuffled_label_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected, scores = method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "df = build_dataframe(selected, scores, feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with scores above the threshold\n",
    "# filter nan values\n",
    "df = df.dropna()\n",
    "df_above_threshold = df[df['Scores'] > upper_threshold]\n",
    "df_below_threshold = df[df['Scores'] < lower_threshold]\n",
    "# join the two dataframes\n",
    "df_above_threshold = pd.concat([df_above_threshold, df_below_threshold])\n",
    "\n",
    "# save f_df_above_threshold to pkl file \n",
    "if save_data: df_above_threshold.to_pickle(f'{file_save_path}{exp_id}_{method_string}_above_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_above_threshold.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for all shuffled label data, join the scores and plot histogram distribution of scores \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "method = mutual_information_select\n",
    "method_string = 'mutual_information'\n",
    "method_kwargs = {}\n",
    "\n",
    "all_scores = get_shuffled_scores(shuffled_label_data, feature_data, method, method_kwargs, verbose=1, n_jobs=-1)\n",
    "\n",
    "# set plot size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_scores, bins=20, kde=True)\n",
    "plt.xlabel(f'{method_string} Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{method_string} Score Distribution for Shuffled Label Data')\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_{method_string}_shuffled_label_data.png')\n",
    "if save_data:\n",
    "    # use pickle to save all_scores\n",
    "    with open(f'{file_save_path}{exp_id}_{method_string}_all_shuffled_label_data.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)\n",
    "    print(f'Saved {exp_id}_{method_string}_all_shuffled_label_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relieff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for all shuffled label data, join the scores and plot histogram distribution of scores \n",
    "\n",
    "import pickle \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "method = relieff_select\n",
    "method_string = 'relieff'\n",
    "method_kwargs = {}\n",
    "\n",
    "all_scores = get_shuffled_scores(shuffled_label_data, feature_data, method, method_kwargs, verbose=1, n_jobs=-1)\n",
    "\n",
    "# set plot size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_scores, bins=20, kde=True)\n",
    "plt.xlabel(f'{method_string} Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{method_string} Score Distribution for Shuffled Label Data')\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_{method_string}_shuffled_label_data.png')\n",
    "if save_data: \n",
    "    # use pickle to save all_scores \n",
    "    with open(f'{file_save_path}{exp_id}_{method_string}_all_shuffled_label_data.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)\n",
    "    print(f'Saved {exp_id}_{method_string}_all_shuffled_label_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for all shuffled label data, join the scores and plot histogram distribution of scores \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "method = rf_select\n",
    "method_string = 'random_forest'\n",
    "method_kwargs = {}\n",
    "\n",
    "all_scores = get_shuffled_scores(shuffled_label_data, feature_data, method, method_kwargs, n_jobs=-1)\n",
    "\n",
    "# set plot size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_scores, bins=20, kde=True)\n",
    "plt.xlabel(f'{method_string} Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{method_string} Score Distribution for Shuffled Label Data')\n",
    "if show_figure: plt.show()\n",
    "if save_figure: plt.savefig(f'{file_save_path}{exp_id}_{method_string}_shuffled_label_data.png')\n",
    "if save_data: \n",
    "    # use pickle to save all_scores \n",
    "    with open(f'{file_save_path}{exp_id}_{method_string}_all_shuffled_label_data.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)\n",
    "    print(f'Saved {exp_id}_{method_string}_all_shuffled_label_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pickled Shuffled Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "selection_method_strings = ['mutual_information', 'relieff', 'random_forest']\n",
    "all_shuffled_label_data = []\n",
    "\n",
    "for method_str in selection_method_strings:\n",
    "    with open(f'{file_save_path}{exp_id}_{method_str}_all_shuffled_label_data.pkl', 'rb') as f:\n",
    "        all_shuffled_label_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutual information \n",
    "thresholds = []\n",
    "all_scores = all_shuffled_label_data[0]\n",
    "all_scores = [score for score in all_scores if not np.isnan(score)]\n",
    "threshold = np.percentile(all_scores, 95)\n",
    "thresholds.append(threshold)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = mutual_information_select\n",
    "method_string = 'mutual_information'\n",
    "method_kwargs = {}\n",
    " \n",
    "selected, scores = method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "df = build_dataframe(selected, scores, feature_data)\n",
    "# select features with scores above the threshold\n",
    "df_above_threshold = df[df['Scores'] > threshold]\n",
    "df_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_above_threshold.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save f_df_above_threshold to pkl file\n",
    "if save_data: df_above_threshold.to_pickle(f'{file_save_path}{exp_id}_{method_string}_above_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relieff\n",
    "\n",
    "all_scores = all_shuffled_label_data[1]\n",
    "all_scores = [score for score in all_scores if not np.isnan(score)]\n",
    "threshold = np.percentile(all_scores, 95)\n",
    "thresholds.append(threshold)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = relieff_select\n",
    "method_string = 'relieff'\n",
    "method_kwargs = {}\n",
    " \n",
    "selected, scores = method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "df = build_dataframe(selected, scores, feature_data)\n",
    "# select features with scores above the threshold\n",
    "df_above_threshold = df[df['Scores'] > threshold]\n",
    "df_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_above_threshold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data: df_above_threshold.to_pickle(f'{file_save_path}{exp_id}_{method_string}_above_threshold.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = all_shuffled_label_data[2]\n",
    "all_scores = [score for score in all_scores if not np.isnan(score)]\n",
    "threshold = np.percentile(all_scores, 95)\n",
    "thresholds.append(threshold)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = rf_select\n",
    "method_string = 'random_forest'\n",
    "method_kwargs = {'random_state': 42}    \n",
    " \n",
    "selected, scores = method(feature_data, label_data, feature_size, **method_kwargs)\n",
    "df = build_dataframe(selected, scores, feature_data)\n",
    "# select features with scores above the threshold\n",
    "df_above_threshold = df[df['Scores'] > threshold]\n",
    "df_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_above_threshold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data: df_above_threshold.to_pickle(f'{file_save_path}{exp_id}_{method_string}_above_threshold.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amTORC2_tfc', 'aGAB1_max', 'aGAB1_tsv', 'aShp2_max', 'aShp2_tsv', 'aShp2_auc', 'aShp2_median', 'aGAB1_median', 'aGAB1_auc', 'aShp2_min', 'FOXO_tmax', 'aShp2_init', 'FOXO_tmin', 'FOXO_max', 'aGAB1_min', 'FOXO_init', 'aRas_min', 'aShp2_tfc', 'aGrb2_tfc', 'aRas_tfc', 'aGAB1_tfc', 'FOXO_tfc', 'aGAB1_init', 'aRas_auc', 'aPI3K_tfc', 'pIRS_tfc', 'aPDK1_tfc', 'amTORC1_tfc', 'aRas_median', 'aRas_max', 'aRas_tsv', 'aRas_init', 'PIP3_tsv', 'pS6K_init', 'amTORC2_median', 'amTORC2_init', 'amTORC2_min', 'amTORC2_auc', 'amTORC2_max', 'pS6K_tfc', 'FOXO_tsv', 'aSos_min', 'pFGFR4_tfc', 'pFGFR4_min', 'pFGFR4_init', 'SPRY2_tfc', 'aGAB2_tfc']\n",
      "['pS6K_min', 'PIP3_ttsv', 'FOXO_min', 'FOXO_auc', 'PTP_tfc', 'PIP3_init', 'PIP3_max', 'pSPRY2_min', 'pMEK_init', 'pMEK_max', 'aRaf_auc', 'aPDK1_min', 'pERK_median', 'aPDK1_init', 'PTP_median', 'FOXO_median', 'amTORC1_ttsv', 'PIP3_median', 'pAkt_auc', 'amTORC1_min', 'PIP3_min', 'pERBB_init', 'pIGFR_init', 'pIGFR_max', 'pERK_max', 'pERK_init', 'pFGFR4_auc', 'SPRY2_max', 'SPRY2_tsv', 'pFGFR4_median', 'PIP3_tfc', 'pIGFR_tfc', 'pFRS2_tfc', 'aCbl_tfc', 'aPDK1_ttsv', 'pMEK_tsv', 'pFRS2_init', 'pMEK_auc', 'pFRS2_min', 'pMEK_min', 'aSos_init']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from pkl file\n",
    "methods = ['f_regression', 'pearson_correlation', 'mutual_information', 'relieff', 'random_forest']\n",
    "df_above_threshold_list = []\n",
    "for method in methods:\n",
    "    df_above_threshold = pd.read_pickle(f'{file_save_path}{exp_id}_{method}_above_threshold.pkl')\n",
    "    df_above_threshold_list.append(df_above_threshold)\n",
    "\n",
    "union_labels = []\n",
    "for df in df_above_threshold_list:\n",
    "    union_labels.extend(df.index.tolist())\n",
    "\n",
    "# find out which labels are duplicated\n",
    "from collections import Counter\n",
    "label_counts = Counter(union_labels)\n",
    "duplicated_labels = [label for label, count in label_counts.items() if count > 1]\n",
    "print(duplicated_labels)\n",
    "unique_labels = [label for label, count in label_counts.items() if count == 1]\n",
    "print(unique_labels)\n",
    "\n",
    "union_labels = list(set(union_labels))\n",
    "len(union_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = feature_data.columns.tolist()\n",
    "dataset = []\n",
    "for df in df_above_threshold_list:\n",
    "    boolean_list = [0] *  feature_data.shape[1]\n",
    "    for label in df.index.tolist():\n",
    "        boolean_list[all_features.index(label)] = 1\n",
    "    dataset.append(boolean_list)\n",
    "selected_features_df = pd.DataFrame(dataset, columns=all_features, index=methods)\n",
    "selected_features_df = selected_features_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df.to_csv(f'{file_save_path}{exp_id}_ensemble_selected_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
