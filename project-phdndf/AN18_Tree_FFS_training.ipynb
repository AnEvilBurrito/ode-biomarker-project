{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in CCLE data\n",
    "from PathLoader import PathLoader\n",
    "from DataLink import DataLink\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "data_link = DataLink(path_loader, 'data_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_used = 32\n",
    "normalised = True\n",
    "save_model = True\n",
    "fixed_random_seed = 42  # -1 for no seed, NOT IN USE\n",
    "save_figure = False\n",
    "save_data = True\n",
    "show_figure = False\n",
    "rngs = list(range(100)) # for stable rng, for pure random, a random set can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic_data_code = 'fgfr4_ccle_dynamic_features_v2'\n",
    "# drug_code = 'gdsc-1-FGFR_0939'\n",
    "# match_rules_data_code = 'fgfr4_model_ccle_match_rules'\n",
    "# folder_name = \"FGFR4-combined-model-training\"\n",
    "# exp_id = \"fgfr4_v4\"  # experiment id, fgfr4_v1, cdk46_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data_code = 'cdk46_ccle_dynamic_features_v3'\n",
    "drug_code = 'gdsc-2-Palbociclib'\n",
    "match_rules_data_code = 'cdk_model_match_rules'\n",
    "folder_name = \"CDK46-tree-model-refined-training\"\n",
    "exp_id = \"cdk46_v4\" # experiment id, fgfr4_v1, cdk46_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded for code generic-gdsc-2-Palbociclib-LN_IC50-cdk46_ccle_dynamic_features_v3-true-Unnamed: 0 Feature Shape (584, 590) Label Shape (584,)\n",
      "Data loaded for code ccle-gdsc-2-Palbociclib-LN_IC50 Feature Shape (584, 19221) Label Shape (584,)\n"
     ]
    }
   ],
   "source": [
    "# load in dynamic features data \n",
    "loading_code = f'generic-{drug_code}-LN_IC50-{dynamic_data_code}-true-Unnamed: 0'\n",
    "# generic-gdsc-{number}-{drug_name}-{target_label}-{dataset_name}-{replace_index}-{row_index}\n",
    "dynamic_feature_data, dynamic_label_data = data_link.get_data_using_code(loading_code)\n",
    "print(f'Data loaded for code {loading_code} Feature Shape {dynamic_feature_data.shape} Label Shape {dynamic_label_data.shape}')\n",
    "\n",
    "# load in ccle static gene expression data\n",
    "loading_code = f'ccle-{drug_code}-LN_IC50'\n",
    "feature_data, label_data = data_link.get_data_using_code(loading_code)\n",
    "print(f'Data loaded for code {loading_code} Feature Shape {feature_data.shape} Label Shape {label_data.shape}')\n",
    "\n",
    "match_rules = data_link.get_data_from_code(match_rules_data_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from toolkit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['RandomForestRegressor', 'XGBRegressor']\n",
    "\n",
    "if not os.path.exists(f'{path_loader.get_data_path()}data/results/{folder_name}'):\n",
    "    os.makedirs(f'{path_loader.get_data_path()}data/results/{folder_name}')\n",
    "\n",
    "file_save_path = f'{path_loader.get_data_path()}data/results/{folder_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(584, 19221)\n",
      "(584,)\n"
     ]
    }
   ],
   "source": [
    "# remove samples present in feature data but not in dynamic feature data\n",
    "new_feature_data = feature_data.loc[feature_data.index.isin(dynamic_feature_data.index)]\n",
    "new_label_data = label_data.loc[label_data.index.isin(dynamic_feature_data.index)]\n",
    "\n",
    "# check size \n",
    "print(new_feature_data.shape)\n",
    "print(new_label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_feature_data = pd.concat([new_feature_data, dynamic_feature_data], axis=1)\n",
    "combined_label_data = new_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data): \n",
    "    data = (data - data.mean()) / data.std()\n",
    "    return data\n",
    "\n",
    "def pipeline_func(X_train, y_train, rng, model_used, normalised=False, **kwargs):\n",
    "    # impute missing values by first quantile first \n",
    "    # normalise X_train \n",
    "    if normalised:\n",
    "        X_train = normalise_data(X_train)\n",
    "    X_train, _ = impute_by_first_quantile(X_train, y_train)\n",
    "    k = X_train.shape[1]\n",
    "    if k > 500: \n",
    "        k = 500 # limit the number of features to 500\n",
    "    selected_features, scores = f_regression_select(X_train, y_train, k)\n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(X_train, y_train, selected_features)\n",
    "    # print(f'{rng} {model_used}')\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}\n",
    "    \n",
    "def eval_func(X_test, y_test, pipeline_components=None, \n",
    "              normalised=False,\n",
    "              save_model=False, \n",
    "              **kwargs):\n",
    "    if normalised:\n",
    "        X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "    X_test, _ = impute_by_first_quantile(X_test, y_test)\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    returned_data = {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp}\n",
    "    if save_model:\n",
    "        returned_data['model'] = pipeline_components['model']\n",
    "    return returned_data\n",
    "\n",
    "def eval_func_best(X_test, y_test, pipeline_components=None, normalised=False, **kwargs):\n",
    "    if normalised:\n",
    "        X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "    X_test, _ = impute_by_first_quantile(X_test, y_test)\n",
    "    selected_features, X_selected = select_preset_features(X_test, y_test, pipeline_components['filter_selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    feat_imp = (pipeline_components['filter_selected_features'], pipeline_components['filter_scores'])\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, 'feature_importance': feat_imp, 'y_test': y_test, 'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector # type: ignore\n",
    "\n",
    "def pipeline_func_ffs(X_train, y_train, rng, model_used, k=25, normalised=False, **kwargs):\n",
    "    # impute missing values by first quantile first\n",
    "    # normalise X_train\n",
    "    if normalised:\n",
    "        X_train = normalise_data(X_train)\n",
    "    X_train, _ = impute_by_first_quantile(X_train, y_train)\n",
    "    ffs_model = get_model_from_string(model_used, **kwargs)\n",
    "    sfs_forward = SequentialFeatureSelector(\n",
    "        ffs_model, n_features_to_select=k, direction=\"forward\"\n",
    "    ).fit(X_train, y_train)\n",
    "    # unfortunately, scikit-learn does not provide the order of the features selected\n",
    "    selected_features = list(X_train.columns[sfs_forward.get_support()])\n",
    "    _, scores = f_regression_select(X_train, y_train, k) # use f_regression score as a substitute, not ideal \n",
    "    model = get_model_from_string(model_used, **kwargs)\n",
    "    selected_features, X_selected = select_preset_features(\n",
    "        X_train, y_train, selected_features)\n",
    "    model.fit(X_selected, y_train)\n",
    "    return {'model': model,\n",
    "            'filter_selected_features': selected_features,\n",
    "            'filter_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of transformed gene features: 39\n",
      "Length of transformed dynamic features: 629\n"
     ]
    }
   ],
   "source": [
    "transformed_gene_list = [] \n",
    "# we deduce these from the original match rules file\n",
    "\n",
    "gene_match_rules = match_rules.dropna(subset=['reference'])\n",
    "for index, row in gene_match_rules.iterrows():\n",
    "    gene = row['reference'].split(';')\n",
    "    for g in gene:\n",
    "        if g not in transformed_gene_list:\n",
    "            transformed_gene_list.append(g)    \n",
    "            \n",
    "dynamic_features = list(dynamic_feature_data.columns)\n",
    "transformed_dynamic_features = []\n",
    "for f in dynamic_features:\n",
    "    transformed_dynamic_features.append(f)\n",
    "for g in transformed_gene_list:\n",
    "    if g not in transformed_dynamic_features:\n",
    "        transformed_dynamic_features.append(g)\n",
    "        \n",
    "print(f'Length of transformed gene features: {len(transformed_gene_list)}')\n",
    "print(f'Length of transformed dynamic features: {len(transformed_dynamic_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Feature Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_exp_id = 'dynamic_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerkit = Powerkit(dynamic_feature_data, dynamic_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_used in all_models:\n",
    "    pipeline_args = {'model_used': model_used}\n",
    "    eval_args = {'save_model': save_model, 'normalised': normalised}\n",
    "    powerkit.add_condition(model_used, True, pipeline_func_ffs, pipeline_args, eval_func, eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SVR to path\n",
      "Running RandomForestRegressor...\n",
      "Saved RandomForestRegressor to path\n",
      "Running XGBRegressor...\n",
      "Saved XGBRegressor to path\n",
      "Running MLPRegressor...\n",
      "Saved MLPRegressor to path\n",
      "Running KNeighborsRegressor...\n",
      "Saved KNeighborsRegressor to path\n",
      "Running ElasticNet...\n",
      "Saved ElasticNet to path\n"
     ]
    }
   ],
   "source": [
    "for model_used in all_models:\n",
    "    print(f'Running {model_used}...')\n",
    "    df = powerkit.run_selected_condition(model_used, rngs, core_used, True)\n",
    "    if save_data:\n",
    "        print(f'Saved {model_used} to path')\n",
    "        df.to_pickle(f'{file_save_path}{exp_id}_{model_used}_{sub_exp_id}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
