{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(torch.__version__)\n",
    "    # Setup device agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0090,  0.0598,  0.0650,  ..., -0.0213,  0.0829, -0.0441],\n",
      "        [ 0.0277,  0.0530, -0.0753,  ..., -0.0688,  0.0103,  0.0825],\n",
      "        [-0.0821, -0.0764, -0.0328,  ..., -0.0160, -0.0610, -0.0584],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0061,  0.0946,  ..., -0.0458, -0.0836,  0.0106],\n",
      "        [ 0.0959,  0.0464, -0.0313,  ..., -0.0645, -0.0206,  0.0351],\n",
      "        [ 0.0149, -0.0357,  0.0108,  ...,  0.0474, -0.0857,  0.0859]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0879, -0.0866,  0.0578,  0.0964,  0.0697,  0.0899, -0.0384,  0.0873,\n",
      "        -0.0778, -0.0947, -0.0395,  0.0850, -0.0734,  0.0968, -0.0006,  0.0943,\n",
      "         0.0725, -0.0674,  0.0271,  0.0244,  0.0090,  0.0619,  0.0139,  0.0510,\n",
      "        -0.0160, -0.0450, -0.0920,  0.0423, -0.0395,  0.0322, -0.0424,  0.0532,\n",
      "         0.0665,  0.0037,  0.0541,  0.0222, -0.0343, -0.0205,  0.0278, -0.0575,\n",
      "         0.0917,  0.0818,  0.0265, -0.0672,  0.0827,  0.0066, -0.0924,  0.0863,\n",
      "         0.0942,  0.0534,  0.0114, -0.0853, -0.0312, -0.0494,  0.0087, -0.0966,\n",
      "         0.0485,  0.0488, -0.0111,  0.0460, -0.0528,  0.0672,  0.0332,  0.0571,\n",
      "        -0.0223, -0.0263, -0.0087, -0.0019,  0.0879,  0.0146, -0.0494, -0.0916,\n",
      "         0.0006,  0.0330, -0.0952, -0.1000, -0.0439, -0.0982, -0.0460,  0.0092,\n",
      "        -0.0719,  0.0120, -0.0252, -0.0396, -0.0087,  0.0999,  0.0791, -0.0919,\n",
      "         0.0254, -0.0017,  0.0571,  0.0802,  0.0428, -0.0155, -0.0114,  0.0815,\n",
      "         0.0073, -0.0805,  0.0090,  0.0775,  0.0116,  0.0306, -0.0202, -0.0119,\n",
      "         0.0845,  0.0745, -0.0986,  0.0709, -0.0407,  0.0926, -0.0707, -0.0139,\n",
      "        -0.0994,  0.0901, -0.0601, -0.0669,  0.0374,  0.0019,  0.0778,  0.0923,\n",
      "        -0.0034, -0.0291, -0.0487, -0.0904, -0.0341, -0.0001,  0.0449,  0.0630,\n",
      "         0.0633, -0.0494, -0.0492, -0.0416, -0.0013,  0.0923, -0.0614,  0.0608,\n",
      "        -0.0570,  0.0775,  0.0696,  0.0699, -0.0984,  0.0427, -0.0916, -0.0372,\n",
      "        -0.0878,  0.0738,  0.0815, -0.0325, -0.0567,  0.0449,  0.0339,  0.0219,\n",
      "         0.0673,  0.0354, -0.0047, -0.0514, -0.0144, -0.0413,  0.0124, -0.0744,\n",
      "        -0.0101, -0.0933,  0.0778,  0.0704, -0.0918,  0.0747, -0.0691,  0.0514,\n",
      "        -0.0223,  0.0489, -0.0169, -0.0220, -0.0186, -0.0595, -0.0843,  0.0165,\n",
      "         0.0129, -0.0348,  0.0525, -0.0490,  0.0466,  0.0950, -0.0070, -0.0865,\n",
      "         0.0213,  0.0402,  0.0140,  0.0156, -0.0493,  0.0510,  0.0087, -0.0246,\n",
      "         0.0311,  0.0049,  0.0691,  0.0965, -0.0946, -0.0388, -0.0990,  0.0250],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0568,  0.0520,  0.0196,  ..., -0.0273,  0.0129, -0.0443],\n",
      "        [ 0.0593,  0.0133, -0.0043,  ..., -0.0628,  0.0220,  0.0143],\n",
      "        [ 0.0687,  0.0433, -0.0439,  ...,  0.0435,  0.0132,  0.0635],\n",
      "        ...,\n",
      "        [ 0.0334, -0.0219,  0.0070,  ..., -0.0499,  0.0477, -0.0423],\n",
      "        [ 0.0617, -0.0337,  0.0075,  ...,  0.0417, -0.0167, -0.0594],\n",
      "        [-0.0234,  0.0411,  0.0393,  ...,  0.0289,  0.0128,  0.0402]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0384,  0.0703, -0.0687,  0.0627, -0.0129, -0.0510, -0.0652,  0.0574,\n",
      "         0.0616,  0.0431], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0568,  0.0520,  0.0196,  ..., -0.0273,  0.0129, -0.0443],\n",
      "        [ 0.0593,  0.0133, -0.0043,  ..., -0.0628,  0.0220,  0.0143],\n",
      "        [ 0.0687,  0.0433, -0.0439,  ...,  0.0435,  0.0132,  0.0635],\n",
      "        ...,\n",
      "        [ 0.0334, -0.0219,  0.0070,  ..., -0.0499,  0.0477, -0.0423],\n",
      "        [ 0.0617, -0.0337,  0.0075,  ...,  0.0417, -0.0167, -0.0594],\n",
      "        [-0.0234,  0.0411,  0.0393,  ...,  0.0289,  0.0128,  0.0402]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0384,  0.0703, -0.0687,  0.0627, -0.0129, -0.0510, -0.0652,  0.0574,\n",
      "         0.0616,  0.0431], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MySmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySmallModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.network1 = MySmallModel()\n",
    "        self.network2 = MySmallModel()\n",
    "        self.network3 = MySmallModel()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc_out = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = F.relu(self.network1(x1))\n",
    "        x2 = F.relu(self.network2(x2))\n",
    "        x3 = F.relu(self.network3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "N = 10\n",
    "x1, x2, x3 = torch.randn(N, 5), torch.randn(N, 5), torch.randn(N, 5)\n",
    "\n",
    "output = model(x1, x2, x3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
