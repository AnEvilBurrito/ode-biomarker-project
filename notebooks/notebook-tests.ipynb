{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader\n",
    "\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "\n",
    "### Load data\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# import GDSC2 drug response data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/drug-response/GDSC2/cache_gdsc2.pkl', 'rb') as f:\n",
    "    gdsc2 = pickle.load(f)\n",
    "    gdsc2_info = pickle.load(f)\n",
    "    \n",
    "# import CCLE gene expression data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/gene-expression/CCLE_Public_22Q2/ccle_expression.pkl', 'rb') as f:\n",
    "    gene_entrez = pickle.load(f)\n",
    "    ccle = pickle.load(f)\n",
    "\n",
    "# import CCLE sample info data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/gene-expression/CCLE_Public_22Q2/ccle_sample_info.pkl', 'rb') as f:\n",
    "    ccle_sample_info = pickle.load(f)\n",
    "\n",
    "# import STRING database using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/string_df.pkl', 'rb') as f:\n",
    "    string_df = pickle.load(f)\n",
    "    string_df_info = pickle.load(f)\n",
    "    string_df_alias = pickle.load(f)\n",
    "\n",
    "\n",
    "# import proteomic expression\n",
    "with open(f'{path_loader.get_data_path()}data/proteomic-expression/goncalves-2022-cell/goncalve_proteome_fillna_processed.pkl', 'rb') as f:\n",
    "    joined_full_protein_matrix = pickle.load(f)\n",
    "    joined_sin_peptile_exclusion_matrix = pickle.load(f)\n",
    "\n",
    "# import STRING database using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/string_df.pkl', 'rb') as f:\n",
    "    string_df = pickle.load(f)\n",
    "    string_df_info = pickle.load(f)\n",
    "    string_df_alias = pickle.load(f)\n",
    "\n",
    "# open STRING to goncalves mapping file\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data\\protein-interaction\\STRING\\goncalve_to_string_id_df.pkl', 'rb') as f:\n",
    "    goncalve_to_string_id_df = pickle.load(f)\n",
    "\n",
    "# open the cache for neighbourhood calculations\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/palbociclib_nth_degree_neighbours.pkl', 'rb') as f:\n",
    "    nth_degree_neighbours = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolkit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading cell line proteomic expression data\n",
    "\n",
    "cancercell2022 = pd.read_csv('data\\preprocessed\\SY-Processed\\CancerCell2022_PRISM.csv')\n",
    "\n",
    "cancercell2022_dropnan = cancercell2022.dropna(subset=['AUC'])\n",
    "\n",
    "import DataFunctions as dfunc \n",
    "\n",
    "feature_data, label_data = dfunc.create_feature_and_label(cancercell2022_dropnan, label_name='AUC')\n",
    "\n",
    "feature_data_no_row = feature_data.drop(['Row'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import FeatureTransformer\n",
    "from toolkit import impute_by_zero, impute_by_first_quantile, get_network_stat_features, get_random_features\n",
    "\n",
    "F = FeatureTransformer()\n",
    "\n",
    "F.add_transform_function('impute_by_zero', impute_by_zero)\n",
    "F.add_selection_function('random_select', get_random_features, {\"selection_size\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, random_state=42)\n",
    "\n",
    "# Print the shapes of the new X objects\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "# Run Feature Transformer \n",
    "\n",
    "selected_features, sel_train, sel_test = F.run(X_train, y_train, X_test)\n",
    "\n",
    "print(selected_features, sel_train.shape, sel_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables in juptyer notebook\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create controlled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "\n",
    "# turn X and Y into dataframes\n",
    "X, y = make_regression(n_samples=500, n_features=1000, n_informative=10, random_state=1, shuffle=False)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "# turn columns into strings\n",
    "\n",
    "X.columns = [str(i) for i in range(X.shape[1])]\n",
    "\n",
    "print(f'Original informative columns: {X.columns[:10]}')\n",
    "\n",
    "# shuffle columns around for X\n",
    "\n",
    "X = X.sample(frac=1, axis=1, random_state=0)\n",
    "\n",
    "print(f'Newly shuffled columns: {X.columns[:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import mrmr_select_fcq\n",
    "\n",
    "features, scores = mrmr_select_fcq(X, y, K=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import enet_select\n",
    "\n",
    "features, scores = enet_select(X, y, 10, max_iter=10000, alpha=0.1, l1_ratio=0.7)\n",
    "\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import rf_select\n",
    "\n",
    "features, scores = rf_select(X, y, k=10, n_estimators=100, max_depth=5, n_jobs=-1)\n",
    "\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import f_regression_select\n",
    "\n",
    "features, scores = f_regression_select(X, y, k=10)\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import relieff_select\n",
    "\n",
    "features, scores = relieff_select(X, y, k=10, n_jobs=4)\n",
    "print(features)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables in juptyer notebook\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toolkit import select_random_features\n",
    "\n",
    "selected_features, selected_X = select_random_features(X, y, 10)\n",
    "\n",
    "print(selected_features, selected_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import transform_impute_by_zero\n",
    "\n",
    "imputed_X, imputed_y = transform_impute_by_zero(X, y)\n",
    "\n",
    "print(imputed_X.shape, imputed_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Powerkit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing code, for reference ONLY\n",
    "'''\n",
    "\n",
    "# rng = 45\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "# get_feature_importance = False\n",
    "# pipeline_comps = pipeline_func(X_train, y_train)\n",
    "# eval_returns = eval_func(X_test, y_test, pipeline_components=pipeline_comps)\n",
    "# # print(eval_returns)\n",
    "\n",
    "# # combine pipeline_comps and eval_returns into a single dictionary\n",
    "\n",
    "# final_returns = {}\n",
    "# final_returns['rng'] = rng\n",
    "# final_returns['condition'] = 'test'\n",
    "# final_returns.update(eval_returns)\n",
    "\n",
    "# if not get_feature_importance:\n",
    "#     final_returns.pop('feature_importance')\n",
    "\n",
    "# # convert final_returns into a dataframe, test if it works for multiple rows\n",
    "\n",
    "# df = pd.DataFrame([final_returns, final_returns])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# feature_importance = final_returns['feature_importance']\n",
    "\n",
    "# for x,y in zip(feature_importance[0], feature_importance[1]):\n",
    "#     print(f'Feature: {x}, Score: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n",
      "Original informative columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object')\n",
      "Newly shuffled columns: Index(['993', '859', '298', '553', '672', '971', '27', '231', '306', '706'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n",
    "\n",
    "from toolkit import Powerkit, transform_impute_by_zero, select_random_features, select_preset_features, select_stat_features, f_regression_select, mrmr_select_fcq, hypertune_svr, get_variation\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# create a Powerkit object\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "\n",
    "# turn X and Y into dataframes\n",
    "X, y = make_regression(n_samples=500, n_features=1000, n_informative=10, random_state=1, shuffle=False)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "# turn columns into strings\n",
    "\n",
    "X.columns = [str(i) for i in range(X.shape[1])]\n",
    "\n",
    "print(f'Original informative columns: {X.columns[:10]}')\n",
    "\n",
    "# shuffle columns around for X\n",
    "\n",
    "X = X.sample(frac=1, axis=1, random_state=0)\n",
    "\n",
    "print(f'Newly shuffled columns: {X.columns[:10]}')\n",
    "\n",
    "\n",
    "def pipeline_func(X_train, y_train, **kwargs):\n",
    "    \n",
    "    X_transformed, y_transformed = transform_impute_by_zero(X_train, y_train)\n",
    "    # selected_features, scores = f_regression_select(X_transformed, y_transformed, k=10)\n",
    "    selected_features, scores = mrmr_select_fcq(X_transformed, y_transformed, K=10, return_index=False)\n",
    "    selected_features, X_selected = select_preset_features(X_transformed, y_transformed, selected_features)\n",
    "    model = SVR()\n",
    "    model.fit(X_selected, y_transformed)\n",
    "    \n",
    "    return {'model': model, 'selected_features': selected_features, 'scores': scores}\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    \n",
    "    '''\n",
    "    example function to evaluate the performance of a pipeline\n",
    "    inputs\n",
    "        X_test: test set features\n",
    "        y_test: test set labels\n",
    "        pipeline_components: dictionary of pipeline components, e.g. {'model': model, 'selected_features': selected_features, 'scores': scores}\n",
    "    '''\n",
    "    \n",
    "    _, X_selected = select_preset_features(X_test, y_test, pipeline_components['selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    # at the end, return a dictionary of all the information you want to return\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, \n",
    "            'feature_importance': (pipeline_components['selected_features'], pipeline_components['scores'])}\n",
    "\n",
    "powerkit = Powerkit(X, y) \n",
    "powerkit.add_condition('test', True, pipeline_func, {}, eval_func, {})\n",
    "rng_list = [i for i in range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = powerkit._abstract_run(rng_list, n_jobs=4, verbose=False)\n",
    "df.head()\n",
    "\n",
    "contribution = powerkit.get_mean_contribution(df, 'test', adjust_for_accuracy=True, strict_mean=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_contrib: ['6', '916', '5', '7', '1']\n",
      "current iteration: 0 current_tol: 10000000000.000000, abs_diff: 10000000000.000000, performance: 0.879906\n",
      "current_contrib: ['6', '585', '916', '5', '7']\n",
      "current iteration: 1 current_tol: 0.020879, abs_diff: 387916.700400, abs_prev: 18579071.919319, performance: 0.879906\n",
      "current_contrib: ['6', '281', '585', '916', '5']\n",
      "current iteration: 2 current_tol: 0.016558, abs_diff: 301433.608019, abs_prev: 18204218.439142, performance: 0.879906\n",
      "current_contrib: ['6', '433', '281', '585', '916']\n",
      "current iteration: 3 current_tol: 0.005982, abs_diff: 107395.777279, abs_prev: 17954326.873790, performance: 0.879906\n",
      "current_contrib: ['6', '433', '281', '585', '916']\n",
      "current iteration: 4 current_tol: 0.005168, abs_diff: 92555.428836, abs_prev: 17910447.674833, performance: 0.879906\n",
      "current_contrib: ['6', '433', '281', '585', '5']\n",
      "current iteration: 5 current_tol: 0.021683, abs_diff: 390338.169403, abs_prev: 18002381.125687, performance: 0.879906\n",
      "current_contrib: ['6', '433', '281', '585', '5']\n",
      "current iteration: 6 current_tol: 0.002705, abs_diff: 49747.545372, abs_prev: 18392687.972414, performance: 0.878514\n",
      "current_contrib: ['6', '433', '281', '833', '585']\n",
      "current iteration: 7 current_tol: 0.009215, abs_diff: 169036.781316, abs_prev: 18343218.306354, performance: 0.878514\n",
      "current_contrib: ['6', '433', '281', '833', '585']\n",
      "current iteration: 8 current_tol: 0.002468, abs_diff: 44920.701445, abs_prev: 18202878.526191, performance: 0.878514\n",
      "current_contrib: ['6', '433', '281', '833', '585']\n",
      "current iteration: 9 current_tol: 0.000601, abs_diff: 10918.583048, abs_prev: 18164888.865825, performance: 0.878514\n",
      "current_contrib: ['6', '697', '433', '281', '833']\n",
      "current iteration: 10 current_tol: 0.007378, abs_diff: 134095.777098, abs_prev: 18176040.522242, performance: 0.878514\n",
      "current_contrib: ['6', '697', '433', '281', '833']\n",
      "current iteration: 11 current_tol: 0.004476, abs_diff: 82397.128439, abs_prev: 18407321.530077, performance: 0.878514\n",
      "current_contrib: ['6', '697', '433', '281', '833']\n",
      "current iteration: 12 current_tol: 0.004657, abs_diff: 86154.455165, abs_prev: 18500810.485517, performance: 0.873318\n",
      "current_contrib: ['6', '697', '433', '281', '833']\n",
      "current iteration: 13 current_tol: 0.003494, abs_diff: 64430.375940, abs_prev: 18439165.804318, performance: 0.873318\n",
      "current_contrib: ['6', '697', '433', '281', '833']\n",
      "current iteration: 14 current_tol: 0.000252, abs_diff: 4674.891278, abs_prev: 18514420.910578, performance: 0.873318\n",
      "current_contrib: ['6', '697', '454', '433', '281']\n",
      "current iteration: 15 current_tol: 0.000213, abs_diff: 3938.498406, abs_prev: 18524843.105407, performance: 0.873318\n",
      "current_contrib: ['6', '697', '454', '433', '281']\n",
      "current iteration: 16 current_tol: 0.001902, abs_diff: 35354.278743, abs_prev: 18589714.414161, performance: 0.873318\n",
      "current_contrib: ['6', '697', '433', '281', '454']\n",
      "current iteration: 17 current_tol: 0.004668, abs_diff: 86959.868547, abs_prev: 18627485.038611, performance: 0.873318\n",
      "current_contrib: ['6', '740', '697', '433', '281']\n",
      "current iteration: 18 current_tol: 0.001608, abs_diff: 29824.552408, abs_prev: 18542006.185639, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '433', '281']\n",
      "current iteration: 19 current_tol: 0.001195, abs_diff: 22304.329680, abs_prev: 18663985.912475, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '433', '281']\n",
      "current iteration: 20 current_tol: 0.000889, abs_diff: 16585.650065, abs_prev: 18653824.303103, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 21 current_tol: 0.008048, abs_diff: 150124.256415, abs_prev: 18653650.415453, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 22 current_tol: 0.005199, abs_diff: 97752.405053, abs_prev: 18803650.103175, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 23 current_tol: 0.001648, abs_diff: 31146.041524, abs_prev: 18901267.472698, performance: 0.844835\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 24 current_tol: 0.003420, abs_diff: 64631.370624, abs_prev: 18895852.384261, performance: 0.879152\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 25 current_tol: 0.001874, abs_diff: 35320.077630, abs_prev: 18846893.195742, performance: 0.879152\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 26 current_tol: 0.001136, abs_diff: 21381.448649, abs_prev: 18820956.890836, performance: 0.879152\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 27 current_tol: 0.002778, abs_diff: 52258.973650, abs_prev: 18813098.310516, performance: 0.879152\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 28 current_tol: 0.000807, abs_diff: 15142.051208, abs_prev: 18766213.259305, performance: 0.879152\n",
      "current_contrib: ['6', '740', '697', '821', '433']\n",
      "current iteration: 29 current_tol: 0.001541, abs_diff: 28914.810146, abs_prev: 18762964.851199, performance: 0.879152\n",
      "current_contrib: ['6', '740', '905', '697', '821']\n",
      "current iteration: 30 current_tol: 0.006772, abs_diff: 127368.677619, abs_prev: 18808915.348693, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '821']\n",
      "current iteration: 31 current_tol: 0.001426, abs_diff: 26981.034244, abs_prev: 18925180.739459, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '821']\n",
      "current iteration: 32 current_tol: 0.002140, abs_diff: 40549.267117, abs_prev: 18951842.941632, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '433']\n",
      "current iteration: 33 current_tol: 0.001476, abs_diff: 28024.259874, abs_prev: 18992374.359466, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '433']\n",
      "current iteration: 34 current_tol: 0.004323, abs_diff: 81991.944863, abs_prev: 18964383.626384, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '433']\n",
      "current iteration: 35 current_tol: 0.002004, abs_diff: 37839.369027, abs_prev: 18883146.347407, performance: 0.878424\n",
      "current_contrib: ['6', '740', '905', '697', '433']\n",
      "current iteration: 36 current_tol: 0.000431, abs_diff: 8137.882332, abs_prev: 18872485.912988, performance: 0.882394\n",
      "current_contrib: ['6', '443', '740', '905', '697']\n",
      "current iteration: 37 current_tol: 0.000076, abs_diff: 1435.966113, abs_prev: 18921386.069144, performance: 0.882394\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 38 current_tol: 0.007769, abs_diff: 148285.526106, abs_prev: 19087940.222779, performance: 0.882394\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 39 current_tol: 0.000613, abs_diff: 11691.701608, abs_prev: 19078571.469166, performance: 0.882394\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 40 current_tol: 0.001260, abs_diff: 24047.229497, abs_prev: 19079023.252072, performance: 0.882394\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 41 current_tol: 0.000754, abs_diff: 14392.733876, abs_prev: 19099773.815396, performance: 0.882394\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 42 current_tol: 0.001022, abs_diff: 19535.670364, abs_prev: 19114499.888771, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 43 current_tol: 0.001289, abs_diff: 24679.429426, abs_prev: 19152412.064366, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 44 current_tol: 0.001846, abs_diff: 35526.154834, abs_prev: 19249660.842933, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 45 current_tol: 0.001125, abs_diff: 21700.774930, abs_prev: 19290995.336481, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 46 current_tol: 0.000213, abs_diff: 4119.819286, abs_prev: 19333041.787329, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 47 current_tol: 0.000790, abs_diff: 15292.106787, abs_prev: 19367121.452260, performance: 0.889827\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 48 current_tol: 0.000508, abs_diff: 9836.646015, abs_prev: 19358265.048060, performance: 0.887002\n",
      "current_contrib: ['6', '740', '905', '443', '697']\n",
      "current iteration: 49 current_tol: 0.001196, abs_diff: 23164.863943, abs_prev: 19375472.620036, performance: 0.887002\n",
      "Consensus Run: condition test is done in 50 iterations\n",
      "Consensus Run under condition test is NOT converged within 1e-06 relative tolerance\n",
      "Consensus Run under condition test is NOT converged within 1e-06 absolute tolerance\n",
      "WARNING: Consensus Run under condition test is not converged within 50 iterations\n"
     ]
    }
   ],
   "source": [
    "rngs, total_df, meta_df = powerkit.run_until_consensus('test', n_jobs=6, abs_tol=0.000001, \n",
    "                                                       rel_tol=0.000001, max_iter=50,\n",
    "                                                       verbose=True, verbose_level=1, \n",
    "                                                       return_meta_df=True, crunch_factor=1)\n",
    "\n",
    "# single core can be more efficient when the computational cost of each iteration is low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution = powerkit.get_mean_contribution(total_df, 'test', adjust_for_accuracy=True, strict_mean=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation = get_variation(total_df, 'test', strict_mean=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variability_for_feature(df, condition, use_iqr=True, strict_mean=0.75):\n",
    "    \n",
    "    # NOTE: df must contain a column called 'feature_importance' with a tuple of (features, scores)\n",
    "    # a column called 'rng' with the rng values is also required\n",
    "    # ideally, n needs to be large enough to get a good estimate of the variability\n",
    "    \n",
    "    # first, condition is needed to filter the dataframe\n",
    "    df = df[df['condition'] == condition]\n",
    "    \n",
    "    \n",
    "    list_of_dict = []\n",
    "    feature_importance_df = df['feature_importance']\n",
    "    for row in feature_importance_df:\n",
    "        for x,y in zip(row[0], row[1]):\n",
    "            # print(f'Feature: {x}, Score: {y}')\n",
    "            list_of_dict.append({'feature': x, 'variability_score': y})\n",
    "            \n",
    "    feature_importance_all = pd.DataFrame(list_of_dict)\n",
    "    feature_count = feature_importance_all.groupby('feature').count()\n",
    "    \n",
    "    if use_iqr: \n",
    "        iqr_df = (feature_importance_all.groupby('feature').quantile(0.75) - feature_importance_all.groupby('feature').quantile(0.25))\n",
    "        iqr_df_div_mean = iqr_df / abs(feature_importance_all.groupby('feature').mean())\n",
    "        # add IQR and mean columns\n",
    "        iqr_df_div_mean['iqr'] = iqr_df\n",
    "        iqr_df_div_mean['mean'] = feature_importance_all.groupby('feature').mean()\n",
    "\n",
    "        iqr_df_div_mean['count'] = feature_count['variability_score']\n",
    "        iqr_df_div_mean = iqr_df_div_mean[iqr_df_div_mean['count'] >= total_df['rng'].nunique() * strict_mean]\n",
    "\n",
    "        # sort the dataframe by iqr, ascending\n",
    "        iqr_df_div_mean = iqr_df_div_mean.sort_values(by='variability_score', ascending=True)\n",
    "        \n",
    "\n",
    "        return iqr_df_div_mean\n",
    "    else:\n",
    "        # calculate the std for each feature divided by the mean\n",
    "        # NOTE: only use this if a normal distribution is assumed\n",
    "        std_df = feature_importance_all.groupby('feature').std() / abs(feature_importance_all.groupby('feature').mean())\n",
    "\n",
    "        # add std and mean columns\n",
    "        std_df['std'] = feature_importance_all.groupby('feature').std()\n",
    "        std_df['mean'] = feature_importance_all.groupby('feature').mean()\n",
    "        \n",
    "        std_df['count'] = feature_count['variability_score']\n",
    "        std_df = std_df[std_df['count'] >= df['rng'].nunique() * strict_mean]\n",
    "\n",
    "        # sort the dataframe by std, ascending\n",
    "\n",
    "        std_df = std_df.sort_values(by='variability_score', ascending=True)\n",
    "        return std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variability_score</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.045472</td>\n",
       "      <td>834002.906249</td>\n",
       "      <td>1.834117e+07</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.143476</td>\n",
       "      <td>107.234729</td>\n",
       "      <td>7.474069e+02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234991</td>\n",
       "      <td>113.759009</td>\n",
       "      <td>4.840999e+02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.248086</td>\n",
       "      <td>361.324631</td>\n",
       "      <td>1.456446e+03</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.291802</td>\n",
       "      <td>598.753161</td>\n",
       "      <td>2.051919e+03</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variability_score            std          mean  count\n",
       "feature                                                       \n",
       "6                 0.045472  834002.906249  1.834117e+07    100\n",
       "4                 0.143476     107.234729  7.474069e+02    100\n",
       "3                 0.234991     113.759009  4.840999e+02    100\n",
       "1                 0.248086     361.324631  1.456446e+03    100\n",
       "7                 0.291802     598.753161  2.051919e+03    100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varia_df = get_variability_for_feature(total_df, 'test', use_iqr=False, strict_mean=0.75)\n",
    "\n",
    "varia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchApp Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SBML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading anthony's sbml model\n",
    "\n",
    "from libsbml import *\n",
    "\n",
    "reader = SBMLReader()\n",
    "\n",
    "document = reader.readSBML(\"data\\export_ECC_Base.xml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = document.getModel()\n",
    "\n",
    "print(f'Document errors: {document.getNumErrors()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of species: {model.getNumSpecies()}')\n",
    "\n",
    "print(f'Number of reactions: {model.getNumReactions()}')\n",
    "\n",
    "print(f'Number of compartments: {model.getNumCompartments()}')\n",
    "\n",
    "print(f'Number of parameters: {model.getNumParameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rules: {model.getNumRules()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roadrunner\n",
    "\n",
    "rr = roadrunner.RoadRunner(\"data\\export_ECC_Base.xml\")\n",
    "\n",
    "print(f'Number of floating species: {len(rr.model.getFloatingSpeciesIds())}')\n",
    "\n",
    "print(f'Number of boundary species: {len(rr.model.getBoundarySpeciesIds())}')\n",
    "\n",
    "print(f'Number of global parameters: {len(rr.model.getGlobalParameterIds())}')\n",
    "\n",
    "print(f'Number of compartments: {len(rr.model.getCompartmentIds())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rr.simulate(0, 10, 100)\n",
    "\n",
    "rr.plot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{rr.model.getFloatingSpeciesIds()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model[\"init(IRS)\"]\n",
    "\n",
    "# rr.model[\"init(IRS)\"] = 0.5 # for changing initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model.getGlobalParameterIds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.kc_INSULIN_INSR_INSRpY\n",
    "\n",
    "# rr.kc_INSULIN_INSR_INSRpY = 0.1 # for changing parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model['kc_INSULIN_INSR_INSRpY'] # another method for changing parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model['INSR'] # another method for changing initial condition values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking CCLE data to Anthony's SBML model initial conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing parameters in SBML model to the calibrated set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing dynamic simulation data back to singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanODEApp Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load in core data and libraries'''\n",
    "\n",
    "# libraries used \n",
    "\n",
    "# load CCLE expression data  \n",
    "\n",
    "# load Anthony's model and optimal parameter sets \n",
    "\n",
    "document = reader.readSBML(\"data\\export_ECC_Base.xml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
