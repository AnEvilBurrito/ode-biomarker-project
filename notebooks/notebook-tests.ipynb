{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader\n",
    "\n",
    "path_loader = PathLoader('data_config.env', 'current_user.env')\n",
    "\n",
    "### Load data\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# import GDSC2 drug response data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/drug-response/GDSC2/cache_gdsc2.pkl', 'rb') as f:\n",
    "    gdsc2 = pickle.load(f)\n",
    "    gdsc2_info = pickle.load(f)\n",
    "    \n",
    "# import CCLE gene expression data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/gene-expression/CCLE_Public_22Q2/ccle_expression.pkl', 'rb') as f:\n",
    "    gene_entrez = pickle.load(f)\n",
    "    ccle = pickle.load(f)\n",
    "\n",
    "# import CCLE sample info data using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/gene-expression/CCLE_Public_22Q2/ccle_sample_info.pkl', 'rb') as f:\n",
    "    ccle_sample_info = pickle.load(f)\n",
    "\n",
    "# import STRING database using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/string_df.pkl', 'rb') as f:\n",
    "    string_df = pickle.load(f)\n",
    "    string_df_info = pickle.load(f)\n",
    "    string_df_alias = pickle.load(f)\n",
    "\n",
    "\n",
    "# import proteomic expression\n",
    "with open(f'{path_loader.get_data_path()}data/proteomic-expression/goncalves-2022-cell/goncalve_proteome_fillna_processed.pkl', 'rb') as f:\n",
    "    joined_full_protein_matrix = pickle.load(f)\n",
    "    joined_sin_peptile_exclusion_matrix = pickle.load(f)\n",
    "\n",
    "# import STRING database using pickle\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/string_df.pkl', 'rb') as f:\n",
    "    string_df = pickle.load(f)\n",
    "    string_df_info = pickle.load(f)\n",
    "    string_df_alias = pickle.load(f)\n",
    "\n",
    "# open STRING to goncalves mapping file\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data\\protein-interaction\\STRING\\goncalve_to_string_id_df.pkl', 'rb') as f:\n",
    "    goncalve_to_string_id_df = pickle.load(f)\n",
    "\n",
    "# open the cache for neighbourhood calculations\n",
    "\n",
    "with open(f'{path_loader.get_data_path()}data/protein-interaction/STRING/palbociclib_nth_degree_neighbours.pkl', 'rb') as f:\n",
    "    nth_degree_neighbours = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolkit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading cell line proteomic expression data\n",
    "\n",
    "cancercell2022 = pd.read_csv('data\\preprocessed\\SY-Processed\\CancerCell2022_PRISM.csv')\n",
    "\n",
    "cancercell2022_dropnan = cancercell2022.dropna(subset=['AUC'])\n",
    "\n",
    "import DataFunctions as dfunc \n",
    "\n",
    "feature_data, label_data = dfunc.create_feature_and_label(cancercell2022_dropnan, label_name='AUC')\n",
    "\n",
    "feature_data_no_row = feature_data.drop(['Row'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import FeatureTransformer\n",
    "from toolkit import impute_by_zero, impute_by_first_quantile, get_network_stat_features, get_random_features\n",
    "\n",
    "F = FeatureTransformer()\n",
    "\n",
    "F.add_transform_function('impute_by_zero', impute_by_zero)\n",
    "F.add_selection_function('random_select', get_random_features, {\"selection_size\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, random_state=42)\n",
    "\n",
    "# Print the shapes of the new X objects\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "# Run Feature Transformer \n",
    "\n",
    "selected_features, sel_train, sel_test = F.run(X_train, y_train, X_test)\n",
    "\n",
    "print(selected_features, sel_train.shape, sel_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables in juptyer notebook\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create controlled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "\n",
    "# turn X and Y into dataframes\n",
    "X, y = make_regression(n_samples=500, n_features=1000, n_informative=10, random_state=1, shuffle=False)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "# turn columns into strings\n",
    "\n",
    "X.columns = [str(i) for i in range(X.shape[1])]\n",
    "\n",
    "print(f'Original informative columns: {X.columns[:10]}')\n",
    "\n",
    "# shuffle columns around for X\n",
    "\n",
    "X = X.sample(frac=1, axis=1, random_state=0)\n",
    "\n",
    "print(f'Newly shuffled columns: {X.columns[:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import mrmr_select_fcq\n",
    "\n",
    "features, scores = mrmr_select_fcq(X, y, K=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import enet_select\n",
    "\n",
    "features, scores = enet_select(X, y, 10, max_iter=10000, alpha=0.1, l1_ratio=0.7)\n",
    "\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import rf_select\n",
    "\n",
    "features, scores = rf_select(X, y, k=10, n_estimators=100, max_depth=5, n_jobs=-1)\n",
    "\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import f_regression_select\n",
    "\n",
    "features, scores = f_regression_select(X, y, k=10)\n",
    "print(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import relieff_select\n",
    "\n",
    "features, scores = relieff_select(X, y, k=10, n_jobs=4)\n",
    "print(features)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables in juptyer notebook\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toolkit import select_random_features\n",
    "\n",
    "selected_features, selected_X = select_random_features(X, y, 10)\n",
    "\n",
    "print(selected_features, selected_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import transform_impute_by_zero\n",
    "\n",
    "imputed_X, imputed_y = transform_impute_by_zero(X, y)\n",
    "\n",
    "print(imputed_X.shape, imputed_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Powerkit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing code, for reference ONLY\n",
    "'''\n",
    "\n",
    "# rng = 45\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "# get_feature_importance = False\n",
    "# pipeline_comps = pipeline_func(X_train, y_train)\n",
    "# eval_returns = eval_func(X_test, y_test, pipeline_components=pipeline_comps)\n",
    "# # print(eval_returns)\n",
    "\n",
    "# # combine pipeline_comps and eval_returns into a single dictionary\n",
    "\n",
    "# final_returns = {}\n",
    "# final_returns['rng'] = rng\n",
    "# final_returns['condition'] = 'test'\n",
    "# final_returns.update(eval_returns)\n",
    "\n",
    "# if not get_feature_importance:\n",
    "#     final_returns.pop('feature_importance')\n",
    "\n",
    "# # convert final_returns into a dataframe, test if it works for multiple rows\n",
    "\n",
    "# df = pd.DataFrame([final_returns, final_returns])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# feature_importance = final_returns['feature_importance']\n",
    "\n",
    "# for x,y in zip(feature_importance[0], feature_importance[1]):\n",
    "#     print(f'Feature: {x}, Score: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n",
      "Original informative columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object')\n",
      "Newly shuffled columns: Index(['993', '859', '298', '553', '672', '971', '27', '231', '306', '706'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f'Project path set to: {os.getcwd()}')\n",
    "\n",
    "from toolkit import Powerkit, transform_impute_by_zero, select_random_features, select_preset_features, select_stat_features, f_regression_select, mrmr_select_fcq, hypertune_svr\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# create a Powerkit object\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "\n",
    "# turn X and Y into dataframes\n",
    "X, y = make_regression(n_samples=500, n_features=1000, n_informative=10, random_state=1, shuffle=False)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "# turn columns into strings\n",
    "\n",
    "X.columns = [str(i) for i in range(X.shape[1])]\n",
    "\n",
    "print(f'Original informative columns: {X.columns[:10]}')\n",
    "\n",
    "# shuffle columns around for X\n",
    "\n",
    "X = X.sample(frac=1, axis=1, random_state=0)\n",
    "\n",
    "print(f'Newly shuffled columns: {X.columns[:10]}')\n",
    "\n",
    "\n",
    "def pipeline_func(X_train, y_train, **kwargs):\n",
    "    \n",
    "    X_transformed, y_transformed = transform_impute_by_zero(X_train, y_train)\n",
    "    # selected_features, scores = f_regression_select(X_transformed, y_transformed, k=10)\n",
    "    selected_features, scores = mrmr_select_fcq(X_transformed, y_transformed, K=10, return_index=False)\n",
    "    selected_features, X_selected = select_preset_features(X_transformed, y_transformed, selected_features)\n",
    "    model = SVR()\n",
    "    model.fit(X_selected, y_transformed)\n",
    "    \n",
    "    return {'model': model, 'selected_features': selected_features, 'scores': scores}\n",
    "\n",
    "def eval_func(X_test, y_test, pipeline_components=None, **kwargs):\n",
    "    \n",
    "    '''\n",
    "    example function to evaluate the performance of a pipeline\n",
    "    inputs\n",
    "        X_test: test set features\n",
    "        y_test: test set labels\n",
    "        pipeline_components: dictionary of pipeline components, e.g. {'model': model, 'selected_features': selected_features, 'scores': scores}\n",
    "    '''\n",
    "    \n",
    "    _, X_selected = select_preset_features(X_test, y_test, pipeline_components['selected_features'])\n",
    "    y_pred = pipeline_components['model'].predict(X_selected)\n",
    "    # assess performance by pearson correlation\n",
    "    corr, p_vals = pearsonr(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    # at the end, return a dictionary of all the information you want to return\n",
    "    return {'model_performance': corr, 'p_vals': p_vals, \n",
    "            'feature_importance': (pipeline_components['selected_features'], pipeline_components['scores'])}\n",
    "\n",
    "powerkit = Powerkit(X, y) \n",
    "powerkit.add_condition('test', True, pipeline_func, {}, eval_func, {})\n",
    "rng_list = [i for i in range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = powerkit._abstract_run(rng_list, n_jobs=4, verbose=False)\n",
    "df.head()\n",
    "\n",
    "contribution = powerkit.get_mean_contribution(df, 'test', adjust_for_accuracy=True, strict_mean=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_contrib: ['6', '905', '420', '917', '195']\n",
      "current iteration: 4 current_tol: 0.027931, abs_diff: 490024.096651, abs_prev: 17544047.417729, performance: 0.906521\n",
      "current_contrib: ['6', '905', '420', '917', '195']\n",
      "current iteration: 6 current_tol: 0.002560, abs_diff: 46340.458463, abs_prev: 18105015.824048, performance: 0.856003\n",
      "Consensus Run: condition test is done in 6 iterations\n",
      "Consensus Run under condition test is NOT converged within 0.001 absolute tolerance\n"
     ]
    }
   ],
   "source": [
    "rngs, total_df, meta_df = powerkit.run_until_consensus('test', n_jobs=2, verbose=True, verbose_level=1, return_meta_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution = powerkit.get_mean_contribution(total_df, 'test', adjust_for_accuracy=True, strict_mean=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchApp Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SBML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading anthony's sbml model\n",
    "\n",
    "from libsbml import *\n",
    "\n",
    "reader = SBMLReader()\n",
    "\n",
    "document = reader.readSBML(\"data\\export_ECC_Base.xml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = document.getModel()\n",
    "\n",
    "print(f'Document errors: {document.getNumErrors()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of species: {model.getNumSpecies()}')\n",
    "\n",
    "print(f'Number of reactions: {model.getNumReactions()}')\n",
    "\n",
    "print(f'Number of compartments: {model.getNumCompartments()}')\n",
    "\n",
    "print(f'Number of parameters: {model.getNumParameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rules: {model.getNumRules()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roadrunner\n",
    "\n",
    "rr = roadrunner.RoadRunner(\"data\\export_ECC_Base.xml\")\n",
    "\n",
    "print(f'Number of floating species: {len(rr.model.getFloatingSpeciesIds())}')\n",
    "\n",
    "print(f'Number of boundary species: {len(rr.model.getBoundarySpeciesIds())}')\n",
    "\n",
    "print(f'Number of global parameters: {len(rr.model.getGlobalParameterIds())}')\n",
    "\n",
    "print(f'Number of compartments: {len(rr.model.getCompartmentIds())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rr.simulate(0, 10, 100)\n",
    "\n",
    "rr.plot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{rr.model.getFloatingSpeciesIds()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model[\"init(IRS)\"]\n",
    "\n",
    "# rr.model[\"init(IRS)\"] = 0.5 # for changing initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model.getGlobalParameterIds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.kc_INSULIN_INSR_INSRpY\n",
    "\n",
    "# rr.kc_INSULIN_INSR_INSRpY = 0.1 # for changing parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model['kc_INSULIN_INSR_INSRpY'] # another method for changing parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.model['INSR'] # another method for changing initial condition values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking CCLE data to Anthony's SBML model initial conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing parameters in SBML model to the calibrated set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing dynamic simulation data back to singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanODEApp Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load in core data and libraries'''\n",
    "\n",
    "# libraries used \n",
    "\n",
    "# load CCLE expression data  \n",
    "\n",
    "# load Anthony's model and optimal parameter sets \n",
    "\n",
    "document = reader.readSBML(\"data\\export_ECC_Base.xml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-marker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
