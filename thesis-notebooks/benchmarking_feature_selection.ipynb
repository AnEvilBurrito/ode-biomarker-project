{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook in Jupytext format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab02b3e",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c9c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\ode-biomarker-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find(\"project\")\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[: index_project + 7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f\"Project path set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e128c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader #noqa: E402\n",
    "\n",
    "path_loader = PathLoader(\"data_config.env\", \"current_user.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d82e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLink import DataLink #noqa: E402\n",
    "\n",
    "data_link = DataLink(path_loader, \"data_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1b589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"ThesisResult4-FeatureSelectionBenchmark\"\n",
    "exp_id = \"v1\"\n",
    "\n",
    "if not os.path.exists(f\"{path_loader.get_data_path()}data/results/{folder_name}\"):\n",
    "    os.makedirs(f\"{path_loader.get_data_path()}data/results/{folder_name}\")\n",
    "\n",
    "file_save_path = f\"{path_loader.get_data_path()}data/results/{folder_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d787584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteomic feature data shape: (737, 6692)\n",
      "Proteomic label data shape: (737,)\n"
     ]
    }
   ],
   "source": [
    "# Load Proteomics Palbociclib dataset\n",
    "loading_code = \"goncalves-gdsc-2-Palbociclib-LN_IC50-sin\"\n",
    "proteomic_feature_data, proteomic_label_data = data_link.get_data_using_code(\n",
    "    loading_code\n",
    ")\n",
    "\n",
    "print(f\"Proteomic feature data shape: {proteomic_feature_data.shape}\")\n",
    "print(f\"Proteomic label data shape: {proteomic_label_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98baa56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final aligned dataset shape: (737, 6692)\n",
      "Final aligned label shape: (737,)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation and alignment\n",
    "import numpy as np #noqa: E402\n",
    "\n",
    "# Ensure numeric only\n",
    "proteomic_feature_data = proteomic_feature_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Align indices\n",
    "common_indices = sorted(\n",
    "    set(proteomic_feature_data.index) & set(proteomic_label_data.index)\n",
    ")\n",
    "feature_data = proteomic_feature_data.loc[common_indices]\n",
    "label_data = proteomic_label_data.loc[common_indices]\n",
    "\n",
    "print(f\"Final aligned dataset shape: {feature_data.shape}\")\n",
    "print(f\"Final aligned label shape: {label_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d621fb",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fbd0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Literal #noqa: E402\n",
    "import numpy as np #noqa: E402\n",
    "import pandas as pd #noqa: E402\n",
    "from scipy.stats import pearsonr, spearmanr #noqa: E402\n",
    "from sklearn.metrics import r2_score #noqa: E402\n",
    "from sklearn.dummy import DummyRegressor #noqa: E402\n",
    "from sklearn.preprocessing import StandardScaler #noqa: E402\n",
    "from toolkit import FirstQuantileImputer, f_regression_select, get_model_from_string #noqa: E402\n",
    "from toolkit import (\n",
    "    mrmr_select_fcq, \n",
    "    mutual_information_select, \n",
    ") #noqa: E402\n",
    "import time #noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34919b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_correlated_columns(X: pd.DataFrame, threshold: float = 0.95) -> List[str]:\n",
    "    \"\"\"Drop highly correlated columns to reduce redundancy\"\"\"\n",
    "    corr = X.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = set()\n",
    "    for col in sorted(upper.columns):\n",
    "        if col in to_drop:\n",
    "            continue\n",
    "        high_corr = upper.index[upper[col] > threshold].tolist()\n",
    "        to_drop.update(high_corr)\n",
    "    return [c for c in X.columns if c not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140eb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_selection_pipeline(\n",
    "    selection_method: callable, k: int, method_name: str, model_name: str\n",
    "):\n",
    "    \"\"\"Create pipeline for feature selection methods\"\"\"\n",
    "\n",
    "    def pipeline_function(X_train: pd.DataFrame, y_train: pd.Series, rng: int):\n",
    "        # 1) Imputation\n",
    "        imputer = FirstQuantileImputer().fit(X_train)\n",
    "        X_train_imp = imputer.transform(X_train, return_df=True)\n",
    "\n",
    "        # 2) Optional: variance threshold and correlation filtering\n",
    "        vt_keep_cols = list(X_train_imp.columns)  # Keep all initially\n",
    "        corr_keep_cols = _drop_correlated_columns(X_train_imp, threshold=0.95)\n",
    "        X_train_filtered = X_train_imp[corr_keep_cols]\n",
    "\n",
    "        # 3) Feature selection with timing\n",
    "        start_time = time.time()\n",
    "        selected_features, selector_scores = selection_method(\n",
    "            X_train_filtered, y_train, k\n",
    "        )\n",
    "        selection_time = time.time() - start_time\n",
    "\n",
    "        # 4) Standardization (mandatory for our model suite)\n",
    "        scaler = StandardScaler()\n",
    "        sel_train_scaled = scaler.fit_transform(X_train_filtered[selected_features])\n",
    "        sel_train_scaled = pd.DataFrame(\n",
    "            sel_train_scaled, index=X_train_filtered.index, columns=selected_features\n",
    "        )\n",
    "\n",
    "        # 5) Train model\n",
    "        if len(selected_features) == 0:\n",
    "            model = DummyRegressor(strategy=\"mean\")\n",
    "            model_type = \"DummyRegressor(mean)\"\n",
    "            model_params = {\"strategy\": \"mean\"}\n",
    "        else:\n",
    "            # Get model based on model_name parameter\n",
    "            if model_name == \"LinearRegression\":\n",
    "                model = get_model_from_string(\"LinearRegression\")\n",
    "                model_params = {\"fit_intercept\": True}\n",
    "            elif model_name == \"KNeighborsRegressor\":\n",
    "                model = get_model_from_string(\n",
    "                    \"KNeighborsRegressor\", n_neighbors=5, weights=\"distance\", p=2\n",
    "                )\n",
    "                model_params = {\"n_neighbors\": 5, \"weights\": \"distance\", \"p\": 2}\n",
    "            elif model_name == \"SVR\":\n",
    "                model = get_model_from_string(\"SVR\", kernel=\"linear\", C=1.0)\n",
    "                model_params = {\"kernel\": \"linear\", \"C\": 1.0}\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "            model.fit(sel_train_scaled, y_train)\n",
    "            model_type = model_name\n",
    "\n",
    "        return {\n",
    "            \"imputer\": imputer,\n",
    "            \"scaler\": scaler,\n",
    "            \"selected_features\": list(selected_features),\n",
    "            \"selector_scores\": np.array(selector_scores),\n",
    "            \"selection_time\": selection_time,\n",
    "            \"method_name\": method_name,\n",
    "            \"model\": model,\n",
    "            \"model_type\": model_type,\n",
    "            \"model_params\": model_params,\n",
    "            \"rng\": rng,\n",
    "        }\n",
    "\n",
    "    return pipeline_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df1722e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def feature_selection_eval(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    *,\n",
    "    pipeline_components: Dict,\n",
    "    metric_primary: Literal[\"r2\", \"pearson_r\", \"spearman_r\"] = \"r2\",\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluation function for feature selection benchmarking\"\"\"\n",
    "\n",
    "    # Unpack pipeline components\n",
    "    imputer = pipeline_components[\"imputer\"]\n",
    "    scaler = pipeline_components[\"scaler\"]\n",
    "    selected = pipeline_components[\"selected_features\"]\n",
    "    selection_time = pipeline_components[\"selection_time\"]\n",
    "    model = pipeline_components[\"model\"]\n",
    "    model_name = pipeline_components[\"model_type\"]\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_imp = imputer.transform(X_test, return_df=True)\n",
    "\n",
    "    # Apply same filtering as in training\n",
    "    corr_keep_cols = _drop_correlated_columns(X_test_imp, threshold=0.95)\n",
    "    X_test_filtered = X_test_imp[corr_keep_cols]\n",
    "\n",
    "    # Select features and standardize\n",
    "    X_test_sel = (\n",
    "        X_test_filtered[selected] if len(selected) > 0 else X_test_filtered.iloc[:, :0]\n",
    "    )\n",
    "    X_test_scaled = scaler.transform(X_test_sel) if len(selected) > 0 else X_test_sel\n",
    "    X_test_scaled = (\n",
    "        pd.DataFrame(X_test_scaled, index=X_test_filtered.index, columns=selected)\n",
    "        if len(selected) > 0\n",
    "        else X_test_sel\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    if len(selected) == 0:\n",
    "        y_pred = np.full_like(\n",
    "            y_test.values, fill_value=float(y_test.mean()), dtype=float\n",
    "        )\n",
    "    else:\n",
    "        y_pred = np.asarray(model.predict(X_test_scaled), dtype=float)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mask_fin = np.isfinite(y_test.values) & np.isfinite(y_pred)\n",
    "    y_t = y_test.values[mask_fin]\n",
    "    y_p = y_pred[mask_fin]\n",
    "\n",
    "    if len(y_t) < 2:\n",
    "        r2 = np.nan\n",
    "        pearson_r = pearson_p = np.nan\n",
    "        spearman_rho = spearman_p = np.nan\n",
    "    else:\n",
    "        r2 = r2_score(y_t, y_p)\n",
    "        pearson_r, pearson_p = pearsonr(y_t, y_p)\n",
    "        spearman_rho, spearman_p = spearmanr(y_t, y_p)\n",
    "\n",
    "    metrics = {\n",
    "        \"r2\": float(r2) if np.isfinite(r2) else np.nan,\n",
    "        \"pearson_r\": float(pearson_r) if np.isfinite(pearson_r) else np.nan,\n",
    "        \"pearson_p\": float(pearson_p) if np.isfinite(pearson_p) else np.nan,\n",
    "        \"spearman_rho\": float(spearman_rho) if np.isfinite(spearman_rho) else np.nan,\n",
    "        \"spearman_p\": float(spearman_p) if np.isfinite(spearman_p) else np.nan,\n",
    "        \"n_test_samples_used\": len(y_t),\n",
    "    }\n",
    "\n",
    "    # Feature importance\n",
    "    if hasattr(model, \"feature_importances_\") and len(selected) > 0:\n",
    "        fi = (np.array(selected), model.feature_importances_)\n",
    "    elif model_name in (\"LinearRegression\",) and len(selected) > 0:\n",
    "        coef = getattr(model, \"coef_\", np.zeros(len(selected)))\n",
    "        fi = (np.array(selected), np.abs(coef))\n",
    "    else:\n",
    "        fi = (np.array(selected), np.zeros(len(selected)))\n",
    "\n",
    "    primary = metrics.get(metric_primary, metrics[\"r2\"])\n",
    "\n",
    "    return {\n",
    "        \"feature_importance\": fi,\n",
    "        \"feature_importance_from\": \"model\",\n",
    "        \"model_performance\": float(primary) if primary is not None else np.nan,\n",
    "        \"metrics\": metrics,\n",
    "        \"selection_time\": selection_time,\n",
    "        \"n_features_selected\": len(selected),\n",
    "        \"model_name\": model_name,\n",
    "        \"rng\": pipeline_components.get(\"rng\", None),\n",
    "        \"selected_features\": selected,\n",
    "        \"selector_scores\": pipeline_components.get(\"selector_scores\", []),\n",
    "        \"y_pred\": y_p,\n",
    "        \"y_true_index\": y_test.index[mask_fin],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e6f3b",
   "metadata": {},
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "034e9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking 3 methods across 8 feature sizes and 3 models\n",
      "Total conditions: 72\n",
      "Methods: ANOVA-filter, MRMR, and Mutual Information\n"
     ]
    }
   ],
   "source": [
    "# Setup experiment parameters - using only the three requested methods\n",
    "feature_set_sizes = [10, 20, 40, 80, 160, 320, 640, 1280]\n",
    "models = [\"KNeighborsRegressor\", \"LinearRegression\", \"SVR\"]\n",
    "\n",
    "# Define only the three feature selection methods you requested\n",
    "feature_selection_methods = {\n",
    "    \"anova_filter\": f_regression_select,        # Renamed as ANOVA-filter\n",
    "    \"mrmr\": mrmr_select_fcq,                    # MRMR method [2]\n",
    "    \"mutual_info\": mutual_information_select    # Mutual Information method [2]\n",
    "}\n",
    "\n",
    "print(f\"Benchmarking {len(feature_selection_methods)} methods across {len(feature_set_sizes)} feature sizes and {len(models)} models\")\n",
    "print(f\"Total conditions: {len(feature_selection_methods) * len(feature_set_sizes) * len(models)}\")\n",
    "print(\"Methods: ANOVA-filter, MRMR, and Mutual Information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7398a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import Powerkit\n",
    "import numpy as np\n",
    "import time #noqa: F811\n",
    "\n",
    "# Initialize Powerkit with proteomics data\n",
    "pk = Powerkit(feature_data, label_data)\n",
    "\n",
    "# Register all conditions (method × size × model combinations)\n",
    "rngs = np.random.RandomState(42).randint(0, 100000, size=1)  # 50 repeats for robustness\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for method_name, selection_method in feature_selection_methods.items():\n",
    "    for k in feature_set_sizes:\n",
    "        for model_name in models:\n",
    "            # Create condition name using the requested naming convention\n",
    "            condition = f\"{method_name}_k{k}_{model_name}\"\n",
    "            \n",
    "            # Create pipeline for this method and size\n",
    "            pipeline_func = create_feature_selection_pipeline(selection_method, k, method_name, model_name)\n",
    "            \n",
    "            # Add condition to Powerkit following the structure from previous notebook [1]\n",
    "            pk.add_condition(\n",
    "                condition=condition,\n",
    "                get_importance=True,\n",
    "                pipeline_function=pipeline_func,\n",
    "                pipeline_args={},\n",
    "                eval_function=feature_selection_eval,\n",
    "                eval_args={\"metric_primary\": \"r2\"}\n",
    "            )\n",
    "\n",
    "print(f\"Registered {len(pk.conditions)} conditions in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all conditions using Powerkit's parallel processing [1]\n",
    "print(\"Starting feature selection benchmark (ANOVA-filter, MRMR, Mutual Information)...\")\n",
    "print(f\"Running with {len(rngs)} random seeds and -1 n_jobs for maximum parallelization\")\n",
    "\n",
    "benchmark_start = time.time()\n",
    "df_benchmark = pk.run_all_conditions(rng_list=rngs, n_jobs=-1, verbose=True)\n",
    "benchmark_time = time.time() - benchmark_start\n",
    "\n",
    "print(f\"Benchmark completed in {benchmark_time:.2f} seconds\")\n",
    "print(f\"Results shape: {df_benchmark.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8808bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k value and model name from condition for easier analysis\n",
    "df_benchmark[\"k_value\"] = df_benchmark[\"condition\"].str.extract(r'k(\\d+)').astype(int)\n",
    "df_benchmark[\"method\"] = df_benchmark[\"condition\"].str.split('_').str[0]\n",
    "df_benchmark[\"model_name\"] = df_benchmark[\"condition\"].str.split('_').str[2]\n",
    "\n",
    "print(\"Condition breakdown:\")\n",
    "print(df_benchmark[[\"condition\", \"method\", \"k_value\", \"model_name\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial results\n",
    "df_benchmark.to_pickle(f\"{file_save_path}feature_selection_benchmark_{exp_id}.pkl\")\n",
    "print(f\"Initial results saved to: {file_save_path}feature_selection_benchmark_{exp_id}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of results\n",
    "print(\"Benchmark Summary:\")\n",
    "print(f\"Total runs: {len(df_benchmark)}\")\n",
    "print(f\"Unique conditions: {df_benchmark['condition'].nunique()}\")\n",
    "print(f\"Performance range (R²): {df_benchmark['model_performance'].min():.4f} to {df_benchmark['model_performance'].max():.4f}\")\n",
    "\n",
    "# Show performance by method (using the three requested methods)\n",
    "method_summary = df_benchmark.groupby(\"method\")[\"model_performance\"].agg([\"mean\", \"std\", \"count\"])\n",
    "print(\"\\nPerformance by method (ANOVA-filter, MRMR, Mutual Information):\")\n",
    "print(method_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8cdb08",
   "metadata": {},
   "source": [
    "## Results and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1fff9",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "ode-biomarker-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
