{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook in Jupytext format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab02b3e",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find(\"project\")\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[: index_project + 7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f\"Project path set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader #noqa: E402\n",
    "\n",
    "path_loader = PathLoader(\"data_config.env\", \"current_user.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d82e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLink import DataLink #noqa: E402\n",
    "\n",
    "data_link = DataLink(path_loader, \"data_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"ThesisResult4-FeatureSelectionBenchmark\"\n",
    "exp_id = \"v1\"\n",
    "\n",
    "if not os.path.exists(f\"{path_loader.get_data_path()}data/results/{folder_name}\"):\n",
    "    os.makedirs(f\"{path_loader.get_data_path()}data/results/{folder_name}\")\n",
    "\n",
    "file_save_path = f\"{path_loader.get_data_path()}data/results/{folder_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d787584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Proteomics Palbociclib dataset\n",
    "loading_code = \"goncalves-gdsc-2-Palbociclib-LN_IC50-sin\"\n",
    "proteomic_feature_data, proteomic_label_data = data_link.get_data_using_code(\n",
    "    loading_code\n",
    ")\n",
    "\n",
    "print(f\"Proteomic feature data shape: {proteomic_feature_data.shape}\")\n",
    "print(f\"Proteomic label data shape: {proteomic_label_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and alignment\n",
    "import numpy as np #noqa: E402\n",
    "\n",
    "# Ensure numeric only\n",
    "proteomic_feature_data = proteomic_feature_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Align indices\n",
    "common_indices = sorted(\n",
    "    set(proteomic_feature_data.index) & set(proteomic_label_data.index)\n",
    ")\n",
    "feature_data = proteomic_feature_data.loc[common_indices]\n",
    "label_data = proteomic_label_data.loc[common_indices]\n",
    "\n",
    "print(f\"Final aligned dataset shape: {feature_data.shape}\")\n",
    "print(f\"Final aligned label shape: {label_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d621fb",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Literal #noqa: E402\n",
    "import numpy as np #noqa: E402\n",
    "import pandas as pd #noqa: E402\n",
    "from scipy.stats import pearsonr, spearmanr #noqa: E402\n",
    "from sklearn.metrics import r2_score #noqa: E402\n",
    "from sklearn.dummy import DummyRegressor #noqa: E402\n",
    "from sklearn.preprocessing import StandardScaler #noqa: E402\n",
    "from toolkit import FirstQuantileImputer, f_regression_select, get_model_from_string #noqa: E402\n",
    "from toolkit import (\n",
    "    mrmr_select_fcq, \n",
    "    mrmr_select_fcq_fast,\n",
    "    mutual_information_select,\n",
    "    select_random_features,\n",
    ") #noqa: E402\n",
    "import time #noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select_wrapper(X: pd.DataFrame, y: pd.Series, k: int) -> tuple:\n",
    "    \"\"\"Wrapper function for random feature selection that returns dummy scores\"\"\"\n",
    "    selected_features, _ = select_random_features(X, y, k)\n",
    "    # Return dummy scores (all zeros) since random selection has no meaningful scores\n",
    "    dummy_scores = np.zeros(len(selected_features))\n",
    "    return selected_features, dummy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34919b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_correlated_columns(X: pd.DataFrame, threshold: float = 0.95) -> List[str]:\n",
    "    \"\"\"Drop highly correlated columns to reduce redundancy\"\"\"\n",
    "    corr = X.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = set()\n",
    "    for col in sorted(upper.columns):\n",
    "        if col in to_drop:\n",
    "            continue\n",
    "        high_corr = upper.index[upper[col] > threshold].tolist()\n",
    "        to_drop.update(high_corr)\n",
    "    return [c for c in X.columns if c not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_selection_pipeline(\n",
    "    selection_method: callable, k: int, method_name: str, model_name: str\n",
    "):\n",
    "    \"\"\"Create pipeline for feature selection methods\"\"\"\n",
    "\n",
    "    def pipeline_function(X_train: pd.DataFrame, y_train: pd.Series, rng: int):\n",
    "        # 1) Sanitize inputs and imputation\n",
    "        X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "        y_train = pd.Series(y_train).replace([np.inf, -np.inf], np.nan)\n",
    "        mask = ~y_train.isna()\n",
    "        X_train, y_train = X_train.loc[mask], y_train.loc[mask]\n",
    "\n",
    "        # 2) Imputation\n",
    "        imputer = FirstQuantileImputer().fit(X_train)\n",
    "        Xtr = imputer.transform(X_train, return_df=True).astype(float)\n",
    "        Xtr = Xtr.fillna(0.0)\n",
    "\n",
    "        # 3) Correlation filtering (applied to both train and test)\n",
    "        # Use the working function from your baseline code [1]\n",
    "        corr_keep_cols = _drop_correlated_columns(Xtr, threshold=0.95)\n",
    "        Xtr_filtered = Xtr[corr_keep_cols]\n",
    "\n",
    "        # 4) Feature selection\n",
    "        k_sel = min(k, Xtr_filtered.shape[1]) if Xtr_filtered.shape[1] > 0 else 0\n",
    "        if k_sel == 0:\n",
    "            selected_features, selector_scores = [], np.array([])\n",
    "            no_features = True\n",
    "        else:\n",
    "            selected_features, selector_scores = selection_method(\n",
    "                Xtr_filtered, y_train, k_sel\n",
    "            )\n",
    "            no_features = False\n",
    "\n",
    "        # 5) Standardization and model training\n",
    "        if no_features or len(selected_features) == 0:\n",
    "            model = DummyRegressor(strategy=\"mean\")\n",
    "            model_type = \"DummyRegressor(mean)\"\n",
    "            model_params = {\"strategy\": \"mean\"}\n",
    "            sel_train = Xtr_filtered.iloc[:, :0]\n",
    "        else:\n",
    "            sel_train = Xtr_filtered[selected_features]\n",
    "            scaler = StandardScaler()\n",
    "            sel_train_scaled = scaler.fit_transform(sel_train)\n",
    "            sel_train_scaled = pd.DataFrame(\n",
    "                sel_train_scaled, index=sel_train.index, columns=selected_features\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            if model_name == \"LinearRegression\":\n",
    "                model = get_model_from_string(\"LinearRegression\")\n",
    "            elif model_name == \"KNeighborsRegressor\":\n",
    "                model = get_model_from_string(\n",
    "                    \"KNeighborsRegressor\", n_neighbors=5, weights=\"distance\", p=2\n",
    "                )\n",
    "            elif model_name == \"SVR\":\n",
    "                model = get_model_from_string(\"SVR\", kernel=\"linear\", C=1.0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "            model.fit(sel_train_scaled, y_train)\n",
    "            model_type = model_name\n",
    "            model_params = (\n",
    "                model.get_params(deep=False) if hasattr(model, \"get_params\") else {}\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"imputer\": imputer,\n",
    "            \"corr_keep_cols\": corr_keep_cols,\n",
    "            \"selected_features\": list(selected_features),\n",
    "            \"selector_scores\": np.array(selector_scores),\n",
    "            \"model\": model,\n",
    "            \"model_type\": model_type,\n",
    "            \"model_params\": model_params,\n",
    "            \"scaler\": scaler if not no_features else None,\n",
    "            \"no_features\": no_features,\n",
    "            \"rng\": rng,\n",
    "        }\n",
    "\n",
    "    return pipeline_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_eval(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    *,\n",
    "    pipeline_components: Dict,\n",
    "    metric_primary: Literal[\"r2\", \"pearson_r\", \"spearman_r\"] = \"r2\",\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluation function for feature selection benchmarking\"\"\"\n",
    "\n",
    "    # Unpack components following the structure from working baseline code [1]\n",
    "    imputer = pipeline_components[\"imputer\"]\n",
    "    corr_keep = set(pipeline_components[\"corr_keep_cols\"])\n",
    "    selected = list(pipeline_components[\"selected_features\"])\n",
    "    selector_scores = pipeline_components[\"selector_scores\"]\n",
    "    model = pipeline_components[\"model\"]\n",
    "    model_name = pipeline_components[\"model_type\"]\n",
    "    scaler = pipeline_components.get(\"scaler\", None)\n",
    "    no_features = pipeline_components.get(\"no_features\", False)\n",
    "\n",
    "    # Apply identical transforms as training\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "    y_test = pd.Series(y_test).replace([np.inf, -np.inf], np.nan)\n",
    "    mask_y = ~y_test.isna()\n",
    "    X_test, y_test = X_test.loc[mask_y], y_test.loc[mask_y]\n",
    "\n",
    "    Xti = imputer.transform(X_test, return_df=True).astype(float).fillna(0.0)\n",
    "\n",
    "    # Apply same correlation filtering as training [1]\n",
    "    cols_after_corr = [c for c in Xti.columns if c in corr_keep]\n",
    "    Xti = Xti[cols_after_corr]\n",
    "\n",
    "    # Select features\n",
    "    Xsel = Xti[selected] if len(selected) > 0 else Xti.iloc[:, :0]\n",
    "\n",
    "    # Standardize if scaler exists (i.e., features were selected)\n",
    "    if scaler is not None and len(selected) > 0:\n",
    "        Xsel_scaled = scaler.transform(Xsel)\n",
    "        Xsel_scaled = pd.DataFrame(Xsel_scaled, index=Xsel.index, columns=selected)\n",
    "    else:\n",
    "        Xsel_scaled = Xsel\n",
    "\n",
    "    # Predict\n",
    "    if no_features or Xsel.shape[1] == 0:\n",
    "        y_pred = np.full_like(\n",
    "            y_test.values, fill_value=float(y_test.mean()), dtype=float\n",
    "        )\n",
    "    else:\n",
    "        y_pred = np.asarray(model.predict(Xsel_scaled), dtype=float)\n",
    "\n",
    "    # Calculate metrics (following the exact structure from baseline_eval [1])\n",
    "    mask_fin = np.isfinite(y_test.values) & np.isfinite(y_pred)\n",
    "    y_t = y_test.values[mask_fin]\n",
    "    y_p = y_pred[mask_fin]\n",
    "\n",
    "    if len(y_t) < 2:\n",
    "        r2 = np.nan\n",
    "        pearson_r = pearson_p = np.nan\n",
    "        spearman_rho = spearman_p = np.nan\n",
    "    else:\n",
    "        r2 = r2_score(y_t, y_p)\n",
    "        pearson_r, pearson_p = pearsonr(y_t, y_p)\n",
    "        spearman_rho, spearman_p = spearmanr(y_t, y_p)\n",
    "\n",
    "    metrics = {\n",
    "        \"r2\": float(r2) if np.isfinite(r2) else np.nan,\n",
    "        \"pearson_r\": float(pearson_r) if np.isfinite(pearson_r) else np.nan,\n",
    "        \"pearson_p\": float(pearson_p) if np.isfinite(pearson_p) else np.nan,\n",
    "        \"spearman_rho\": float(spearman_rho) if np.isfinite(spearman_rho) else np.nan,\n",
    "        \"spearman_p\": float(spearman_p) if np.isfinite(spearman_p) else np.nan,\n",
    "        \"n_test_samples_used\": len(y_t),\n",
    "    }\n",
    "\n",
    "    # Feature importance\n",
    "    if not no_features and hasattr(model, \"feature_importances_\") and len(selected) > 0:\n",
    "        fi = (np.array(selected), model.feature_importances_)\n",
    "    elif not no_features and model_name in (\"LinearRegression\",) and len(selected) > 0:\n",
    "        coef = getattr(model, \"coef_\", np.zeros(len(selected)))\n",
    "        fi = (np.array(selected), np.abs(coef))\n",
    "    else:\n",
    "        fi = (np.array(selected), np.zeros(len(selected)))\n",
    "\n",
    "    primary = metrics.get(metric_primary, metrics[\"r2\"])\n",
    "\n",
    "    return {\n",
    "        \"feature_importance\": fi,\n",
    "        \"feature_importance_from\": \"model\",\n",
    "        \"model_performance\": float(primary) if primary is not None else np.nan,\n",
    "        \"metrics\": metrics,\n",
    "        \"selected_features\": selected,\n",
    "        \"model_name\": model_name,\n",
    "        \"selected_scores\": selector_scores,\n",
    "        \"k\": len(selected),\n",
    "        \"rng\": pipeline_components.get(\"rng\", None),\n",
    "        \"y_pred\": y_p,\n",
    "        \"y_true_index\": y_test.index[mask_fin],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99161ff2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25969b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup experiment parameters - using only the three requested methods\n",
    "feature_set_sizes = [10, 20, 40, 80, 160, 320, 640, 1280]\n",
    "models = [\"KNeighborsRegressor\", \"LinearRegression\", \"SVR\"]\n",
    "\n",
    "# Define the feature selection methods including random selection as negative control\n",
    "feature_selection_methods = {\n",
    "    \"anova_filter\": f_regression_select,        # Renamed as ANOVA-filter\n",
    "    \"mrmr\": mrmr_select_fcq_fast,                    # MRMR method [2]\n",
    "    \"mutual_info\": mutual_information_select,   # Mutual Information method [2]\n",
    "    \"random_select\": random_select_wrapper      # Random selection as negative control\n",
    "}\n",
    "\n",
    "print(f\"Benchmarking {len(feature_selection_methods)} methods across {len(feature_set_sizes)} feature sizes and {len(models)} models\")\n",
    "print(f\"Total conditions: {len(feature_selection_methods) * len(feature_set_sizes) * len(models)}\")\n",
    "print(\"Methods: ANOVA-filter, MRMR, Mutual Information, and Random Selection (negative control)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit import Powerkit #noqa: E402\n",
    "import numpy as np #noqa: E402\n",
    "import time #noqa: F811, E402\n",
    "\n",
    "# Initialize Powerkit with proteomics data\n",
    "pk = Powerkit(feature_data, label_data)\n",
    "\n",
    "# Register all conditions (method × size × model combinations)\n",
    "rngs = np.random.RandomState(42).randint(0, 100000, size=1)  # 50 repeats for robustness\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for method_name, selection_method in feature_selection_methods.items():\n",
    "    for k in feature_set_sizes:\n",
    "        for model_name in models:\n",
    "            # Create condition name using the requested naming convention\n",
    "            condition = f\"{method_name}_k{k}_{model_name}\"\n",
    "            \n",
    "            # Create pipeline for this method and size\n",
    "            pipeline_func = create_feature_selection_pipeline(selection_method, k, method_name, model_name)\n",
    "            \n",
    "            # Add condition to Powerkit following the structure from previous notebook [1]\n",
    "            pk.add_condition(\n",
    "                condition=condition,\n",
    "                get_importance=True,\n",
    "                pipeline_function=pipeline_func,\n",
    "                pipeline_args={},\n",
    "                eval_function=feature_selection_eval,\n",
    "                eval_args={\"metric_primary\": \"r2\"}\n",
    "            )\n",
    "\n",
    "print(f\"Registered {len(pk.conditions)} conditions in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all conditions using Powerkit's parallel processing [1]\n",
    "print(\"Starting feature selection benchmark (ANOVA-filter, MRMR, Mutual Information, Random Selection)...\")\n",
    "print(f\"Running with {len(rngs)} random seeds and -1 n_jobs for maximum parallelization\")\n",
    "\n",
    "benchmark_start = time.time()\n",
    "df_benchmark = pk.run_all_conditions(rng_list=rngs, n_jobs=-1, verbose=True)\n",
    "benchmark_time = time.time() - benchmark_start\n",
    "\n",
    "print(f\"Benchmark completed in {benchmark_time:.2f} seconds\")\n",
    "print(f\"Results shape: {df_benchmark.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k value and model name from condition for easier analysis\n",
    "df_benchmark[\"k_value\"] = df_benchmark[\"condition\"].str.extract(r'k(\\d+)').astype(int)\n",
    "df_benchmark[\"method\"] = df_benchmark[\"condition\"].str.split('_').str[0]\n",
    "df_benchmark[\"model_name\"] = df_benchmark[\"condition\"].str.split('_').str[2]\n",
    "\n",
    "print(\"Condition breakdown:\")\n",
    "print(df_benchmark[[\"condition\", \"method\", \"k_value\", \"model_name\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial results\n",
    "df_benchmark.to_pickle(f\"{file_save_path}feature_selection_benchmark_{exp_id}.pkl\")\n",
    "print(f\"Initial results saved to: {file_save_path}feature_selection_benchmark_{exp_id}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff27a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Quick summary of results\n",
    "print(\"Benchmark Summary:\")\n",
    "print(f\"Total runs: {len(df_benchmark)}\")\n",
    "print(f\"Unique conditions: {df_benchmark['condition'].nunique()}\")\n",
    "print(f\"Performance range (R²): {df_benchmark['model_performance'].min():.4f} to {df_benchmark['model_performance'].max():.4f}\")\n",
    "\n",
    "# Show performance by method (including random selection as negative control)\n",
    "method_summary = df_benchmark.groupby(\"method\")[\"model_performance\"].agg([\"mean\", \"std\", \"count\"])\n",
    "print(\"\\nPerformance by method (ANOVA-filter, MRMR, Mutual Information, Random Selection):\")\n",
    "print(method_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8cdb08",
   "metadata": {},
   "source": [
    "## Results and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee83c7",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c02227",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_and_print(message, report_file=None, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Print message to console and save to report file with proper formatting.\n",
    "    \n",
    "    Args:\n",
    "        message: The message to print and save\n",
    "        report_file: File object to save to (optional)\n",
    "        level: Formatting level - \"header\", \"section\", \"subsection\", or \"info\"\n",
    "    \"\"\"\n",
    "    # Print to console\n",
    "    print(message)\n",
    "    \n",
    "    # Save to report with proper formatting\n",
    "    if report_file:\n",
    "        if level == \"header\":\n",
    "            report_file.write(f\"# {message}\\n\\n\")\n",
    "        elif level == \"section\":\n",
    "            report_file.write(f\"## {message}\\n\\n\")\n",
    "        elif level == \"subsection\":\n",
    "            report_file.write(f\"### {message}\\n\\n\")\n",
    "        else:  # info level\n",
    "            report_file.write(f\"{message}\\n\\n\")\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cef46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved feature selection benchmark (feature_selection_benchmark_v1.pkl)\n",
    "import os\n",
    "import pandas as pd\n",
    "import time #noqa: E402\n",
    "\n",
    "# Create a new report file for capturing print statements\n",
    "print_report_path = f\"{file_save_path}feature_selection_print_report_{exp_id}.md\"\n",
    "print_report_file = open(print_report_path, 'w', encoding='utf-8')\n",
    "\n",
    "# Write header to the print report\n",
    "print_report_file.write(f\"# Feature Selection Print Report - {exp_id}\\n\\n\")\n",
    "print_report_file.write(f\"**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "print_report_file.write(\"This report captures all print statements from the Results section with proper formatting.\\n\\n\")\n",
    "\n",
    "pkl_path = f\"{path_loader.get_data_path()}data/results/{folder_name}/feature_selection_benchmark_{exp_id}.pkl\"\n",
    "if not os.path.exists(pkl_path):\n",
    "    raise FileNotFoundError(f\"Pickle not found: {pkl_path}\")\n",
    "\n",
    "df_benchmark = pd.read_pickle(pkl_path)\n",
    "save_and_print(f\"Loaded df_benchmark with shape: {df_benchmark.shape}\", print_report_file, level=\"section\")\n",
    "# Display first rows (works in notebook)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(df_benchmark.head())\n",
    "except Exception:\n",
    "    save_and_print(df_benchmark.head().to_string(), print_report_file, level=\"info\")\n",
    "\n",
    "# Re-define variables that might be needed in the loaded section\n",
    "# Use actual k-values present in the data instead of predefined list\n",
    "feature_set_sizes = sorted(df_benchmark['k_value'].unique())\n",
    "models = [\"KNeighborsRegressor\", \"LinearRegression\", \"SVR\"]\n",
    "method_labels = {\n",
    "    'anova': 'ANOVA-Filter',\n",
    "    'mrmr': 'MRMR', \n",
    "    'mutual': 'Mutual Information',\n",
    "    'random': 'Random Selection'\n",
    "}\n",
    "\n",
    "print(f\"Actual k-values present in data: {feature_set_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3cdc7",
   "metadata": {},
   "source": [
    "### Performance Comparison: Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96010ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "# Create publication-quality box plot comparing methods across all feature sizes and models\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "sns.boxplot(data=df_benchmark, x='method', y='model_performance', \n",
    "            order=['anova', 'mrmr', 'mutual', 'random'],\n",
    "            palette=colors, width=0.6, fliersize=3)\n",
    "plt.title('Feature Selection Method Performance Comparison', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Feature Selection Method', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('R² Score', fontsize=14, fontweight='bold')\n",
    "plt.xticks(ticks=range(4), \n",
    "           labels=[method_labels[m] for m in ['anova', 'mrmr', 'mutual', 'random']],\n",
    "           rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}method_comparison_boxplot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each method\n",
    "method_stats = df_benchmark.groupby('method')['model_performance'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# Create publication-quality bar plot with error bars\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "# Create bar plot with error bars\n",
    "bars = plt.bar(range(len(method_stats)), method_stats['mean'], \n",
    "               yerr=method_stats['std'], capsize=8, alpha=0.8,\n",
    "               color=colors, edgecolor='black', linewidth=1)\n",
    "\n",
    "plt.title('Mean Performance of Feature Selection Methods', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Feature Selection Method', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Mean R² Score ± Std. Dev.', fontsize=14, fontweight='bold')\n",
    "plt.xticks(ticks=range(len(method_stats)), \n",
    "           labels=[method_labels.get(m, m) for m in method_stats['method']],\n",
    "           rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add value labels on bars with improved formatting\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f} ± {method_stats.iloc[i][\"std\"]:.3f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}method_performance_bar.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistical summary\n",
    "summary_table = df_benchmark.groupby('method')['model_performance'].agg([\n",
    "    ('count', 'count'),\n",
    "    ('mean', 'mean'),\n",
    "    ('std', 'std'),\n",
    "    ('min', 'min'),\n",
    "    ('25%', lambda x: x.quantile(0.25)),\n",
    "    ('median', 'median'),\n",
    "    ('75%', lambda x: x.quantile(0.75)),\n",
    "    ('max', 'max')\n",
    "]).round(4)\n",
    "\n",
    "save_and_print(\"Performance Statistics by Feature Selection Method:\", print_report_file, level=\"section\")\n",
    "save_and_print(summary_table.to_string(), print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4b681",
   "metadata": {},
   "source": [
    "### Performance vs. Feature Set Size (k value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive feature_set_sizes from the dataframe to handle different runs\n",
    "feature_set_sizes_viz = sorted(df_benchmark['k_value'].unique())\n",
    "# Use actual k-values from data for consistency\n",
    "feature_set_sizes = feature_set_sizes_viz\n",
    "\n",
    "# Calculate mean and standard deviation for each method and k value\n",
    "k_performance_stats = df_benchmark.groupby(['method', 'k_value'])['model_performance'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# Create publication-quality line plot with standard deviation bands\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "markers = ['o', 's', '^', 'D']  # Circle, Square, Triangle, Diamond\n",
    "\n",
    "# Plot each method with error bands\n",
    "for i, method in enumerate(k_performance_stats['method'].unique()):\n",
    "    method_data = k_performance_stats[k_performance_stats['method'] == method]\n",
    "    \n",
    "    # Sort by k_value to ensure proper line plotting\n",
    "    method_data = method_data.sort_values('k_value')\n",
    "    \n",
    "    # Plot the mean line\n",
    "    plt.plot(method_data['k_value'], method_data['mean'], \n",
    "             marker=markers[i], linewidth=2.5, markersize=8, \n",
    "             color=colors[i], markeredgecolor='white', markeredgewidth=1,\n",
    "             label=method_labels.get(method, method))\n",
    "    \n",
    "    # Add standard deviation bands (shaded area)\n",
    "    plt.fill_between(method_data['k_value'], \n",
    "                     method_data['mean'] - method_data['std'],\n",
    "                     method_data['mean'] + method_data['std'],\n",
    "                     alpha=0.15, color=colors[i])\n",
    "\n",
    "plt.title('Feature Selection Performance vs. Number of Features Selected', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Features Selected (k)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Mean R² Score ± Std. Dev.', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')  # Use log scale for better visualization of wide k range\n",
    "plt.xticks(feature_set_sizes_viz, feature_set_sizes_viz, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, alpha=0.2, linestyle='--')\n",
    "plt.legend(title='Feature Selection Method', fontsize=11, framealpha=0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}performance_vs_k_value.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality faceted line plots by model type\n",
    "plt.figure(figsize=(18, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Calculate stats by method, k_value, and model_name\n",
    "model_k_stats = df_benchmark.groupby(['method', 'k_value', 'model_name'])['model_performance'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Create subplots for each model\n",
    "models = ['KNeighborsRegressor', 'LinearRegression', 'SVR']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Define publication-quality color palette and markers\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "markers = ['o', 's', '^', 'D']  # Circle, Square, Triangle, Diamond\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = model_k_stats[model_k_stats['model_name'] == model]\n",
    "    \n",
    "    for j, method in enumerate(model_data['method'].unique()):\n",
    "        method_model_data = model_data[model_data['method'] == method].sort_values('k_value')\n",
    "        \n",
    "        axes[i].plot(method_model_data['k_value'], method_model_data['mean'], \n",
    "                     marker=markers[j], linewidth=2.5, markersize=6,\n",
    "                     color=colors[j], markeredgecolor='white', markeredgewidth=1,\n",
    "                     label=method_labels.get(method, method))\n",
    "        \n",
    "        axes[i].fill_between(method_model_data['k_value'],\n",
    "                            method_model_data['mean'] - method_model_data['std'],\n",
    "                            method_model_data['mean'] + method_model_data['std'],\n",
    "                            alpha=0.15, color=colors[j])\n",
    "    \n",
    "    axes[i].set_title(f'{model}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel('Number of Features Selected (k)', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Mean R² Score', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xscale('log')\n",
    "    axes[i].set_xticks(feature_set_sizes_viz, feature_set_sizes_viz, fontsize=10)\n",
    "    axes[i].tick_params(axis='y', labelsize=10)\n",
    "    axes[i].grid(True, alpha=0.2, linestyle='--')\n",
    "    axes[i].legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "plt.suptitle('Feature Selection Performance vs. k Value by ML Model', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}performance_vs_k_by_model.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05147f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of k value effect\n",
    "save_and_print(\"Performance Trend Analysis by k Value:\", print_report_file, level=\"section\")\n",
    "for method in df_benchmark['method'].unique():\n",
    "    method_data = df_benchmark[df_benchmark['method'] == method]\n",
    "    \n",
    "    # Calculate correlation between k_value and performance\n",
    "    correlation = method_data['k_value'].corr(method_data['model_performance'])\n",
    "    \n",
    "    # Calculate performance change from smallest to largest k\n",
    "    k_min_perf = method_data[method_data['k_value'] == 10]['model_performance'].mean()\n",
    "    k_max_perf = method_data[method_data['k_value'] == 1280]['model_performance'].mean()\n",
    "    performance_change = k_max_perf - k_min_perf\n",
    "    \n",
    "    save_and_print(f\"\\n{method_labels.get(method, method)}:\", print_report_file, level=\"subsection\")\n",
    "    save_and_print(f\"  Correlation (k vs performance): {correlation:.4f}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Performance change (k=10 to k=1280): {performance_change:.4f}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Optimal k range: {method_data.groupby('k_value')['model_performance'].mean().idxmax()} features\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d33214",
   "metadata": {},
   "source": [
    "### Performance vs. Feature Set Size (k value) - Excluding Specific k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da443bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization excluding specific k values (e.g., k=500)\n",
    "k_values_to_exclude = [500]  # Add any k values you want to exclude here\n",
    "\n",
    "# Filter dataframe to exclude specific k values\n",
    "df_filtered = df_benchmark[~df_benchmark['k_value'].isin(k_values_to_exclude)]\n",
    "\n",
    "# Derive feature_set_sizes from the filtered dataframe\n",
    "feature_set_sizes_filtered = sorted(df_filtered['k_value'].unique())\n",
    "\n",
    "# Calculate mean and standard deviation for each method and k value\n",
    "k_performance_stats_filtered = df_filtered.groupby(['method', 'k_value'])['model_performance'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# Create publication-quality line plot with standard deviation bands (excluding specific k values)\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "markers = ['o', 's', '^', 'D']  # Circle, Square, Triangle, Diamond\n",
    "\n",
    "# Plot each method with error bands\n",
    "for i, method in enumerate(k_performance_stats_filtered['method'].unique()):\n",
    "    method_data = k_performance_stats_filtered[k_performance_stats_filtered['method'] == method]\n",
    "    \n",
    "    # Sort by k_value to ensure proper line plotting\n",
    "    method_data = method_data.sort_values('k_value')\n",
    "    \n",
    "    # Plot the mean line\n",
    "    plt.plot(method_data['k_value'], method_data['mean'], \n",
    "             marker=markers[i], linewidth=2.5, markersize=8, \n",
    "             color=colors[i], markeredgecolor='white', markeredgewidth=1,\n",
    "             label=method_labels.get(method, method))\n",
    "    \n",
    "    # Add standard deviation bands (shaded area)\n",
    "    plt.fill_between(method_data['k_value'], \n",
    "                     method_data['mean'] - method_data['std'],\n",
    "                     method_data['mean'] + method_data['std'],\n",
    "                     alpha=0.15, color=colors[i])\n",
    "\n",
    "plt.title(f'Feature Selection Performance vs. Number of Features Selected\\n(Excluding k={k_values_to_exclude})', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Features Selected (k)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Mean R² Score ± Std. Dev.', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')  # Use log scale for better visualization of wide k range\n",
    "plt.xticks(feature_set_sizes_filtered, feature_set_sizes_filtered, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, alpha=0.2, linestyle='--')\n",
    "plt.legend(title='Feature Selection Method', fontsize=11, framealpha=0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}performance_vs_k_value_excluding_{'_'.join(map(str, k_values_to_exclude))}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "save_and_print(f\"Created visualization excluding k values: {k_values_to_exclude}\", print_report_file, level=\"info\")\n",
    "save_and_print(f\"Remaining k values in visualization: {feature_set_sizes_filtered}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bb7dd",
   "metadata": {},
   "source": [
    "### Performance vs. Feature Set Size (k value) - Without Standard Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2335555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line plot without standard error bars (cleaner visualization)\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "markers = ['o', 's', '^', 'D']  # Circle, Square, Triangle, Diamond\n",
    "\n",
    "# Plot each method without error bands\n",
    "for i, method in enumerate(k_performance_stats_filtered[\"method\"].unique()):\n",
    "    method_data = k_performance_stats_filtered[k_performance_stats_filtered[\"method\"] == method]\n",
    "    \n",
    "    # Sort by k_value to ensure proper line plotting\n",
    "    method_data = method_data.sort_values('k_value')\n",
    "    \n",
    "    # Plot the mean line only (no error bands)\n",
    "    plt.plot(method_data['k_value'], method_data['mean'], \n",
    "             marker=markers[i], linewidth=2.5, markersize=8, \n",
    "             color=colors[i], markeredgecolor='white', markeredgewidth=1,\n",
    "             label=method_labels.get(method, method))\n",
    "\n",
    "plt.title('Feature Selection Performance vs. Number of Features Selected', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Features Selected (k)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Mean R² Score', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')  # Use log scale for better visualization of wide k range\n",
    "plt.xticks(feature_set_sizes_viz, feature_set_sizes_viz, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim(left=9, right=170)\n",
    "plt.grid(True, alpha=0.2, linestyle='--')\n",
    "plt.legend(title='Feature Selection Method', fontsize=14, framealpha=0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}performance_vs_k_value_no_error_bars.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Created line plot without standard error bars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d48cf",
   "metadata": {},
   "source": [
    "## Feature Selection Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b34c77",
   "metadata": {},
   "source": [
    "### Feature Selection Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeaa633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature selection frequency across all methods\n",
    "save_and_print(\"Analyzing feature selection patterns...\", print_report_file, level=\"section\")\n",
    "\n",
    "# Extract all selected features and create frequency analysis\n",
    "all_selected_features = []\n",
    "for idx, row in df_benchmark.iterrows():\n",
    "    selected_features = row['selected_features']\n",
    "    # Handle different data types (list, numpy array, etc.)\n",
    "    if hasattr(selected_features, '__iter__') and not isinstance(selected_features, (str, dict)):\n",
    "        # Convert to list if it's an iterable (like numpy array)\n",
    "        if hasattr(selected_features, 'tolist'):\n",
    "            selected_features = selected_features.tolist()\n",
    "        elif not isinstance(selected_features, list):\n",
    "            selected_features = list(selected_features)\n",
    "        \n",
    "        if len(selected_features) > 0:\n",
    "            all_selected_features.extend(selected_features)\n",
    "\n",
    "# Calculate overall feature frequency\n",
    "feature_frequency = pd.Series(all_selected_features).value_counts().sort_values(ascending=False)\n",
    "\n",
    "save_and_print(f\"Total feature selections: {len(all_selected_features)}\", print_report_file, level=\"info\")\n",
    "save_and_print(f\"Unique features selected: {len(feature_frequency)}\", print_report_file, level=\"info\")\n",
    "save_and_print(\"Top 20 most frequently selected features:\", print_report_file, level=\"subsection\")\n",
    "save_and_print(feature_frequency.head(20).to_string(), print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ed09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality bar plot of top selected features\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Plot top 20 features\n",
    "top_features = feature_frequency.head(20)\n",
    "bars = plt.bar(range(len(top_features)), top_features.values, \n",
    "               color='#2ca02c', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "plt.title('Top 20 Most Frequently Selected Features (All Methods)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Feature', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Selection Frequency', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(top_features)), top_features.index, rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}top_features_frequency.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature selection frequency by method\n",
    "method_feature_freq = {}\n",
    "for method in df_benchmark['method'].unique():\n",
    "    method_features = []\n",
    "    method_data = df_benchmark[df_benchmark['method'] == method]\n",
    "    \n",
    "    for idx, row in method_data.iterrows():\n",
    "        selected_features = row['selected_features']\n",
    "        # Handle different data types (list, numpy array, etc.)\n",
    "        if hasattr(selected_features, '__iter__') and not isinstance(selected_features, (str, dict)):\n",
    "            # Convert to list if it's an iterable (like numpy array)\n",
    "            if hasattr(selected_features, 'tolist'):\n",
    "                selected_features = selected_features.tolist()\n",
    "            elif not isinstance(selected_features, list):\n",
    "                selected_features = list(selected_features)\n",
    "            \n",
    "            if len(selected_features) > 0:\n",
    "                method_features.extend(selected_features)\n",
    "    \n",
    "    method_feature_freq[method] = pd.Series(method_features).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Display top features for each method\n",
    "save_and_print(\"Top 10 features by method:\", print_report_file, level=\"section\")\n",
    "for method, freq in method_feature_freq.items():\n",
    "    save_and_print(f\"\\n{method_labels.get(method, method)}:\", print_report_file, level=\"subsection\")\n",
    "    save_and_print(freq.head(10).to_string(), print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality faceted bar plots by method\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "methods = ['anova', 'mrmr', 'mutual', 'random']\n",
    "for i, method in enumerate(methods):\n",
    "    ax = axes[i//2, i%2]\n",
    "    top_method_features = method_feature_freq[method].head(10)\n",
    "    \n",
    "    bars = ax.bar(range(len(top_method_features)), top_method_features.values,\n",
    "                  color=colors[i], alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_title(f'{method_labels.get(method, method)}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Feature', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Selection Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(top_method_features)))\n",
    "    ax.set_xticklabels(top_method_features.index, rotation=45, ha='right', fontsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    ax.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Top 10 Most Frequently Selected Features by Method', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}top_features_by_method.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1d30a",
   "metadata": {},
   "source": [
    "### Method-Specific Feature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af96906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature selection patterns between methods\n",
    "save_and_print(\"Comparing feature selection patterns between methods...\", print_report_file, level=\"section\")\n",
    "\n",
    "# Create sets of top features for each method (top 50 features)\n",
    "top_features_by_method = {}\n",
    "for method in methods:\n",
    "    top_features_by_method[method] = set(method_feature_freq[method].head(50).index)\n",
    "\n",
    "# Calculate overlaps between methods\n",
    "overlap_analysis = {}\n",
    "for i, method1 in enumerate(methods):\n",
    "    for j, method2 in enumerate(methods):\n",
    "        if i < j:  # Avoid duplicate comparisons\n",
    "            overlap = len(top_features_by_method[method1] & top_features_by_method[method2])\n",
    "            total_union = len(top_features_by_method[method1] | top_features_by_method[method2])\n",
    "            jaccard_similarity = overlap / total_union if total_union > 0 else 0\n",
    "            \n",
    "            overlap_analysis[f\"{method1}_{method2}\"] = {\n",
    "                'overlap_count': overlap,\n",
    "                'jaccard_similarity': jaccard_similarity,\n",
    "                'method1_features': len(top_features_by_method[method1]),\n",
    "                'method2_features': len(top_features_by_method[method2])\n",
    "            }\n",
    "\n",
    "# Display overlap analysis\n",
    "save_and_print(\"Feature Selection Overlap Analysis (Top 50 features per method):\", print_report_file, level=\"subsection\")\n",
    "for comparison, stats in overlap_analysis.items():\n",
    "    method1, method2 = comparison.split('_')\n",
    "    save_and_print(f\"\\n{method_labels.get(method1, method1)} vs {method_labels.get(method2, method2)}:\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Overlap: {stats['overlap_count']} features\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Jaccard Similarity: {stats['jaccard_similarity']:.3f}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  {method_labels.get(method1, method1)} features: {stats['method1_features']}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  {method_labels.get(method2, method2)} features: {stats['method2_features']}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify consensus features selected by multiple methods\n",
    "consensus_features = {}\n",
    "for n_methods in range(2, len(methods) + 1):\n",
    "    # Get features selected by at least n_methods\n",
    "    feature_method_count = {}\n",
    "    for feature in feature_frequency.index:\n",
    "        count = 0\n",
    "        for method in methods:\n",
    "            if feature in top_features_by_method[method]:\n",
    "                count += 1\n",
    "        feature_method_count[feature] = count\n",
    "    \n",
    "    consensus_features[n_methods] = [f for f, c in feature_method_count.items() if c >= n_methods]\n",
    "\n",
    "save_and_print(\"Consensus Features Analysis:\", print_report_file, level=\"subsection\")\n",
    "for n_methods, features in consensus_features.items():\n",
    "    save_and_print(f\"\\nFeatures selected by at least {n_methods} methods: {len(features)} features\", print_report_file, level=\"info\")\n",
    "    if len(features) > 0:\n",
    "        save_and_print(f\"Top 10: {features[:10]}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality heatmap showing feature selection frequency by method\n",
    "plt.figure(figsize=(10, 7), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Get top 30 features overall\n",
    "top_30_features = feature_frequency.head(30).index\n",
    "\n",
    "# Create frequency matrix for heatmap\n",
    "freq_matrix = []\n",
    "for feature in top_30_features:\n",
    "    row = []\n",
    "    for method in methods:\n",
    "        if feature in method_feature_freq[method]:\n",
    "            row.append(method_feature_freq[method][feature])\n",
    "        else:\n",
    "            row.append(0)\n",
    "    freq_matrix.append(row)\n",
    "\n",
    "freq_df = pd.DataFrame(freq_matrix, index=top_30_features, \n",
    "                       columns=[method_labels.get(m, m) for m in methods])\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(freq_df, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Selection Frequency'}, \n",
    "            linewidths=0.5, linecolor='white')\n",
    "plt.title('Feature Selection Frequency by Method (Top 30 Features)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Feature Selection Method', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}feature_selection_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for feature selection analysis\n",
    "save_and_print(\"Feature Selection Analysis Summary:\", print_report_file, level=\"section\")\n",
    "save_and_print(f\"Total feature selection events: {len(all_selected_features)}\", print_report_file, level=\"info\")\n",
    "save_and_print(f\"Unique features selected: {len(feature_frequency)}\", print_report_file, level=\"info\")\n",
    "save_and_print(f\"Average selections per feature: {len(all_selected_features) / len(feature_frequency):.1f}\", print_report_file, level=\"info\")\n",
    "\n",
    "# Method-specific statistics\n",
    "save_and_print(\"Method-specific statistics:\", print_report_file, level=\"subsection\")\n",
    "for method in methods:\n",
    "    method_selections = sum(len(row['selected_features']) for idx, row in df_benchmark[df_benchmark['method'] == method].iterrows() \n",
    "                           if isinstance(row['selected_features'], list))\n",
    "    unique_features = len(method_feature_freq[method])\n",
    "    avg_selections = method_selections / unique_features if unique_features > 0 else 0\n",
    "    \n",
    "    save_and_print(f\"\\n{method_labels.get(method, method)}:\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Total selections: {method_selections}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Unique features: {unique_features}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Average selections per feature: {avg_selections:.1f}\", print_report_file, level=\"info\")\n",
    "    save_and_print(f\"  Most frequent feature: {method_feature_freq[method].index[0]} ({method_feature_freq[method].iloc[0]} selections)\", print_report_file, level=\"info\")\n",
    "\n",
    "# Close the print report file\n",
    "print_report_file.close()\n",
    "print(f\"Print report saved to: {print_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e82a5",
   "metadata": {},
   "source": [
    "### Feature Selection Stability Analysis (Intra-Method Jaccard Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-open the print report file for stability analysis\n",
    "print_report_file = open(print_report_path, 'a', encoding='utf-8')\n",
    "\n",
    "# Calculate Jaccard similarity within methods across different runs\n",
    "save_and_print(\"Analyzing feature selection stability using intra-method Jaccard similarity...\", print_report_file, level=\"section\")\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets\"\"\"\n",
    "    if len(set1) == 0 and len(set2) == 0:\n",
    "        return 1.0  # Both empty sets are considered identical\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Group runs by method and k-value\n",
    "stability_analysis = {}\n",
    "methods = ['anova', 'mrmr', 'mutual', 'random']\n",
    "\n",
    "for method in methods:\n",
    "    method_data = df_benchmark[df_benchmark['method'] == method]\n",
    "    stability_analysis[method] = {}\n",
    "    \n",
    "    for k_value in feature_set_sizes:\n",
    "        k_data = method_data[method_data['k_value'] == k_value]\n",
    "        \n",
    "        if len(k_data) < 2:\n",
    "            # Need at least 2 runs to calculate similarity\n",
    "            stability_analysis[method][k_value] = {'mean_jaccard': np.nan, 'std_jaccard': np.nan, 'n_runs': len(k_data)}\n",
    "            continue\n",
    "        \n",
    "        # Extract feature sets for this method and k-value\n",
    "        feature_sets = []\n",
    "        for idx, row in k_data.iterrows():\n",
    "            selected_features = row['selected_features']\n",
    "            # Handle different data types\n",
    "            if hasattr(selected_features, '__iter__') and not isinstance(selected_features, (str, dict)):\n",
    "                if hasattr(selected_features, 'tolist'):\n",
    "                    selected_features = selected_features.tolist()\n",
    "                elif not isinstance(selected_features, list):\n",
    "                    selected_features = list(selected_features)\n",
    "                feature_sets.append(set(selected_features))\n",
    "        \n",
    "        # Calculate pairwise Jaccard similarities\n",
    "        jaccard_similarities = []\n",
    "        for i in range(len(feature_sets)):\n",
    "            for j in range(i + 1, len(feature_sets)):\n",
    "                similarity = jaccard_similarity(feature_sets[i], feature_sets[j])\n",
    "                jaccard_similarities.append(similarity)\n",
    "        \n",
    "        if len(jaccard_similarities) > 0:\n",
    "            stability_analysis[method][k_value] = {\n",
    "                'mean_jaccard': np.mean(jaccard_similarities),\n",
    "                'std_jaccard': np.std(jaccard_similarities),\n",
    "                'n_runs': len(k_data),\n",
    "                'n_comparisons': len(jaccard_similarities)\n",
    "            }\n",
    "        else:\n",
    "            stability_analysis[method][k_value] = {'mean_jaccard': np.nan, 'std_jaccard': np.nan, 'n_runs': len(k_data)}\n",
    "\n",
    "# Display stability analysis results\n",
    "save_and_print(\"Intra-Method Feature Selection Stability (Jaccard Similarity):\", print_report_file, level=\"subsection\")\n",
    "for method in methods:\n",
    "    save_and_print(f\"\\n{method_labels.get(method, method)}:\", print_report_file, level=\"info\")\n",
    "    for k_value in feature_set_sizes:\n",
    "        if k_value in stability_analysis[method]:\n",
    "            stats = stability_analysis[method][k_value]\n",
    "            if not np.isnan(stats['mean_jaccard']):\n",
    "                save_and_print(f\"  k={k_value}: Mean Jaccard = {stats['mean_jaccard']:.3f} ± {stats['std_jaccard']:.3f} \"\n",
    "                      f\"(n_runs={stats['n_runs']}, comparisons={stats['n_comparisons']})\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality line plot showing stability vs. k-value\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Define publication-quality color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "markers = ['o', 's', '^', 'D']  # Circle, Square, Triangle, Diamond\n",
    "\n",
    "# Plot each method's stability\n",
    "for i, method in enumerate(methods):\n",
    "    k_values = []\n",
    "    mean_jaccards = []\n",
    "    std_jaccards = []\n",
    "    \n",
    "    for k_value in feature_set_sizes:\n",
    "        if k_value in stability_analysis[method] and not np.isnan(stability_analysis[method][k_value]['mean_jaccard']):\n",
    "            k_values.append(k_value)\n",
    "            mean_jaccards.append(stability_analysis[method][k_value]['mean_jaccard'])\n",
    "            std_jaccards.append(stability_analysis[method][k_value]['std_jaccard'])\n",
    "    \n",
    "    if len(k_values) > 0:\n",
    "        plt.plot(k_values, mean_jaccards, \n",
    "                 marker=markers[i], linewidth=2.5, markersize=8, \n",
    "                 color=colors[i], markeredgecolor='white', markeredgewidth=1,\n",
    "                 label=method_labels.get(method, method))\n",
    "        \n",
    "        # Add standard deviation bands\n",
    "        plt.fill_between(k_values, \n",
    "                         np.array(mean_jaccards) - np.array(std_jaccards),\n",
    "                         np.array(mean_jaccards) + np.array(std_jaccards),\n",
    "                         alpha=0.15, color=colors[i])\n",
    "\n",
    "plt.title('Feature Selection Stability vs. Number of Features Selected', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Features Selected (k)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Mean Jaccard Similarity ± Std. Dev.', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')  # Use log scale for better visualization\n",
    "plt.xticks(feature_set_sizes, feature_set_sizes, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, alpha=0.2, linestyle='--')\n",
    "plt.legend(title='Feature Selection Method', fontsize=11, framealpha=0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}stability_vs_k_value.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f64587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall stability metrics for each method\n",
    "overall_stability = {}\n",
    "for method in methods:\n",
    "    all_jaccards = []\n",
    "    for k_value in feature_set_sizes:\n",
    "        if k_value in stability_analysis[method] and not np.isnan(stability_analysis[method][k_value]['mean_jaccard']):\n",
    "            # Get individual Jaccard values for this k-value (approximate by using mean)\n",
    "            n_comparisons = stability_analysis[method][k_value]['n_comparisons']\n",
    "            if n_comparisons > 0:\n",
    "                mean_jaccard = stability_analysis[method][k_value]['mean_jaccard']\n",
    "                # Add multiple instances weighted by number of comparisons\n",
    "                all_jaccards.extend([mean_jaccard] * n_comparisons)\n",
    "    \n",
    "    if len(all_jaccards) > 0:\n",
    "        overall_stability[method] = {\n",
    "            'mean_jaccard': np.mean(all_jaccards),\n",
    "            'std_jaccard': np.std(all_jaccards),\n",
    "            'total_comparisons': len(all_jaccards)\n",
    "        }\n",
    "    else:\n",
    "        overall_stability[method] = {'mean_jaccard': np.nan, 'std_jaccard': np.nan, 'total_comparisons': 0}\n",
    "\n",
    "# Display overall stability summary\n",
    "save_and_print(\"Overall Feature Selection Stability Summary:\", print_report_file, level=\"subsection\")\n",
    "for method in methods:\n",
    "    if method in overall_stability and not np.isnan(overall_stability[method]['mean_jaccard']):\n",
    "        stats = overall_stability[method]\n",
    "        save_and_print(f\"{method_labels.get(method, method)}: \"\n",
    "              f\"Mean Jaccard = {stats['mean_jaccard']:.3f} ± {stats['std_jaccard']:.3f} \"\n",
    "              f\"(total comparisons={stats['total_comparisons']})\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-quality bar plot comparing overall stability\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Prepare data for bar plot\n",
    "methods_plot = []\n",
    "mean_jaccards = []\n",
    "std_jaccards = []\n",
    "\n",
    "for method in methods:\n",
    "    if method in overall_stability and not np.isnan(overall_stability[method]['mean_jaccard']):\n",
    "        methods_plot.append(method_labels.get(method, method))\n",
    "        mean_jaccards.append(overall_stability[method]['mean_jaccard'])\n",
    "        std_jaccards.append(overall_stability[method]['std_jaccard'])\n",
    "\n",
    "if len(methods_plot) > 0:\n",
    "    bars = plt.bar(range(len(methods_plot)), mean_jaccards, \n",
    "                   yerr=std_jaccards, capsize=8, alpha=0.8,\n",
    "                   color=colors[:len(methods_plot)], edgecolor='black', linewidth=1)\n",
    "    \n",
    "    plt.title('Overall Feature Selection Stability by Method', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Feature Selection Method', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Mean Jaccard Similarity ± Std. Dev.', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(methods_plot)), methods_plot, rotation=45, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.3f} ± {std_jaccards[i]:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{file_save_path}overall_stability_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stability heatmap by method and k-value\n",
    "plt.figure(figsize=(9, 6), dpi=300)\n",
    "plt.rcParams['font.family'] = 'sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "# Create stability matrix for heatmap\n",
    "stability_matrix = []\n",
    "for method in methods:\n",
    "    row = []\n",
    "    for k_value in feature_set_sizes:\n",
    "        if k_value in stability_analysis[method] and not np.isnan(stability_analysis[method][k_value]['mean_jaccard']):\n",
    "            row.append(stability_analysis[method][k_value]['mean_jaccard'])\n",
    "        else:\n",
    "            row.append(np.nan)\n",
    "    stability_matrix.append(row)\n",
    "\n",
    "stability_df = pd.DataFrame(stability_matrix, index=[method_labels.get(m, m) for m in methods], \n",
    "                           columns=feature_set_sizes)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(stability_df, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Jaccard Similarity'}, \n",
    "            linewidths=0.5, linecolor='white')\n",
    "plt.title('Feature Selection Stability by Method and k-value', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Features Selected (k)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Feature Selection Method', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{file_save_path}stability_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of stability between methods\n",
    "save_and_print(\"Statistical Comparison of Feature Selection Stability:\", print_report_file, level=\"subsection\")\n",
    "\n",
    "# Collect all Jaccard values for statistical comparison\n",
    "method_jaccards = {}\n",
    "for method in methods:\n",
    "    jaccards = []\n",
    "    for k_value in feature_set_sizes:\n",
    "        if k_value in stability_analysis[method] and not np.isnan(stability_analysis[method][k_value]['mean_jaccard']):\n",
    "            n_comparisons = stability_analysis[method][k_value]['n_comparisons']\n",
    "            mean_jaccard = stability_analysis[method][k_value]['mean_jaccard']\n",
    "            jaccards.extend([mean_jaccard] * n_comparisons)\n",
    "    method_jaccards[method] = jaccards\n",
    "\n",
    "# Compare methods pairwise\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "for i, method1 in enumerate(methods):\n",
    "    for j, method2 in enumerate(methods):\n",
    "        if i < j and len(method_jaccards[method1]) > 0 and len(method_jaccards[method2]) > 0:\n",
    "            t_stat, p_value = ttest_ind(method_jaccards[method1], method_jaccards[method2])\n",
    "            save_and_print(f\"{method_labels.get(method1, method1)} vs {method_labels.get(method2, method2)}: \"\n",
    "                  f\"t={t_stat:.3f}, p={p_value:.4f}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of stability analysis\n",
    "save_and_print(\"Feature Selection Stability Analysis Summary:\", print_report_file, level=\"section\")\n",
    "\n",
    "# Rank methods by stability\n",
    "stability_ranking = []\n",
    "for method in methods:\n",
    "    if method in overall_stability and not np.isnan(overall_stability[method]['mean_jaccard']):\n",
    "        stability_ranking.append((method, overall_stability[method]['mean_jaccard']))\n",
    "\n",
    "stability_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "save_and_print(\"Stability Ranking (Highest to Lowest):\", print_report_file, level=\"subsection\")\n",
    "for i, (method, stability) in enumerate(stability_ranking, 1):\n",
    "    save_and_print(f\"{i}. {method_labels.get(method, method)}: {stability:.3f}\", print_report_file, level=\"info\")\n",
    "\n",
    "# Close the print report file\n",
    "print_report_file.close()\n",
    "print(f\"Print report saved to: {print_report_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "ode-biomarker-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
