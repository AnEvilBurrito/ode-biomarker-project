{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook in Jupytext format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545f70d",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis for Network-Specific Gene Subsets\n",
    "## Cross-Dataset Visualizations with Publication-Quality Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258a649",
   "metadata": {},
   "source": [
    "This notebook performs comprehensive analysis of feature importance results from the gene subset SHAP analysis and generates publication-quality visualizations including:\n",
    "- Matrix plots comparing feature importance across networks and dataset types (gene subset vs dynamic vs combined)\n",
    "- Paired comparison plots showing context-dependent importance changes\n",
    "- SHAP directional impact analysis with natural language titles emphasizing network-specific gene subsets\n",
    "- Cross-dataset pattern identification for targeted biomarker discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22846715",
   "metadata": {},
   "source": [
    "## Initialisation and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47955f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root to Python path for imports\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find(\"project\")\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[: index_project + 7]\n",
    "# set the working directory\n",
    "os.chdir(project_path)\n",
    "print(f\"Project path set to: {os.getcwd()}\")\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "sys.path.insert(0, project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PathLoader import PathLoader #noqa: E402\n",
    "\n",
    "path_loader = PathLoader(\"data_config.env\", \"current_user.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff465e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLink import DataLink #noqa: E402\n",
    "\n",
    "data_link = DataLink(path_loader, \"data_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6165e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Use the gene subset folder structure instead of original\n",
    "folder_name = \"ThesisResult-FeatureImportanceGeneSubsets-SHAP\"\n",
    "exp_id = \"v1_rf_config1_genesubsets_shap_seeds20_batch4\" \n",
    "\n",
    "# Create both the main folder and exp_id subfolder\n",
    "main_folder = f\"{path_loader.get_data_path()}data/results/{folder_name}\"\n",
    "exp_folder = f\"{main_folder}/{exp_id}\"\n",
    "\n",
    "if not os.path.exists(main_folder):\n",
    "    os.makedirs(main_folder)\n",
    "if not os.path.exists(exp_folder):\n",
    "    os.makedirs(exp_folder)\n",
    "\n",
    "file_save_path = f\"{exp_folder}/\"\n",
    "\n",
    "# Create a new report file for capturing print statements\n",
    "print_report_path = f\"{file_save_path}feature_importance_gene_subset_analysis_report_{exp_id}.md\"\n",
    "print_report_file = open(print_report_path, 'w', encoding='utf-8')\n",
    "\n",
    "# Write header to the print report\n",
    "print_report_file.write(f\"# Feature Importance Analysis Report - Gene Subsets - {exp_id}\\n\\n\")\n",
    "import time\n",
    "print_report_file.write(f\"**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "print_report_file.write(\"This report captures all print statements from the feature importance analysis for network-specific gene subsets with proper formatting.\\n\\n\")\n",
    "\n",
    "def save_and_print(message, report_file=None, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Print message to console and save to report file with proper formatting.\n",
    "    \n",
    "    Args:\n",
    "        message: The message to print and save\n",
    "        report_file: File object to save to (optional)\n",
    "        level: Formatting level - \"header\", \"section\", \"subsection\", or \"info\"\n",
    "    \"\"\"\n",
    "    # Print to console\n",
    "    print(message)\n",
    "    \n",
    "    # Save to report with proper formatting\n",
    "    if report_file:\n",
    "        if level == \"header\":\n",
    "            report_file.write(f\"# {message}\\n\\n\")\n",
    "        elif level == \"section\":\n",
    "            report_file.write(f\"## {message}\\n\\n\")\n",
    "        elif level == \"subsection\":\n",
    "            report_file.write(f\"### {message}\\n\\n\")\n",
    "        else:  # info level\n",
    "            report_file.write(f\"{message}\\n\\n\")\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bde34",
   "metadata": {},
   "source": [
    "## Data Loading Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Define expected file patterns based on the gene subset file structure\n",
    "file_patterns = [\n",
    "    \"dataset_type_analysis_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\",\n",
    "    \"feature_importance_analysis_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\",\n",
    "    \"shap_consensus_importance_signed_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\",\n",
    "    \"shap_consensus_importance_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\",\n",
    "    \"shap_iteration_importance_signed_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\",\n",
    "    \"shap_iteration_importance_v1_rf_config1_genesubsets_shap_seeds20_batch4.pkl\"\n",
    "]\n",
    "\n",
    "save_and_print(\"## Data Loading Results - Gene Subsets\", print_report_file, level=\"section\")\n",
    "\n",
    "# Load datasets with comprehensive error handling\n",
    "datasets = {}\n",
    "\n",
    "for pattern in file_patterns:\n",
    "    file_path = f\"{exp_folder}/{pattern}\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        save_and_print(f\"‚ö†Ô∏è  File not found: {file_path}\", print_report_file, level=\"info\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load the pickle file\n",
    "        data = pd.read_pickle(file_path)\n",
    "        dataset_name = pattern.replace(f\"_{exp_id}.pkl\", \"\").replace(\"v1_rf_config1_genesubsets_shap_\", \"\")\n",
    "        datasets[dataset_name] = data\n",
    "        \n",
    "        save_and_print(f\"‚úÖ Loaded: {pattern}\", print_report_file, level=\"info\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        save_and_print(f\"‚ùå Error loading {pattern}: {str(e)}\", print_report_file, level=\"info\")\n",
    "        save_and_print(f\"   Detailed error: {sys.exc_info()[0]}\", print_report_file, level=\"info\")\n",
    "\n",
    "save_and_print(f\"üìä Total datasets loaded: {len(datasets)}\", print_report_file, level=\"info\")\n",
    "save_and_print(f\"Available datasets: {list(datasets.keys())}\", print_report_file, level=\"info\")\n",
    "\n",
    "# Print RNG seeds used in the experiment\n",
    "save_and_print(\"### RNG Seeds Used in Gene Subset Feature Importance Analysis\", print_report_file, level=\"subsection\")\n",
    "\n",
    "if 'feature_importance_analysis' in datasets:\n",
    "    dataset = datasets['feature_importance_analysis']\n",
    "    if isinstance(dataset, pd.DataFrame) and 'rng' in dataset.columns:\n",
    "        rng_seeds = sorted(dataset['rng'].unique())\n",
    "        save_and_print(f\"RNG seeds found: {len(rng_seeds)} unique seeds\", print_report_file, level=\"info\")\n",
    "        save_and_print(f\"RNG seed values: {rng_seeds}\", print_report_file, level=\"info\")\n",
    "    else:\n",
    "        save_and_print(\"RNG column not found in feature_importance_analysis dataset\", print_report_file, level=\"info\")\n",
    "else:\n",
    "    save_and_print(\"feature_importance_analysis dataset not found\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111784c",
   "metadata": {},
   "source": [
    "## Section 1: Basic Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_memory_usage(obj):\n",
    "    \"\"\"Get memory usage of an object in MB\"\"\"\n",
    "    return sys.getsizeof(obj) / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "save_and_print(\"## Basic Statistics - Gene Subsets\", print_report_file, level=\"section\")\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    save_and_print(f\"### Dataset: {name}\", print_report_file, level=\"subsection\")\n",
    "    \n",
    "    # Shape and basic info\n",
    "    if hasattr(data, 'shape'):\n",
    "        save_and_print(f\"Shape: {data.shape}\", print_report_file, level=\"info\")\n",
    "        if hasattr(data, 'columns'):\n",
    "            save_and_print(f\"Columns: {len(data.columns)}\", print_report_file, level=\"info\")\n",
    "            if len(data.columns) <= 10:\n",
    "                save_and_print(f\"Column names: {list(data.columns)}\", print_report_file, level=\"info\")\n",
    "            else:\n",
    "                save_and_print(f\"First 10 columns: {list(data.columns[:10])}\", print_report_file, level=\"info\")\n",
    "        if hasattr(data, 'index'):\n",
    "            if isinstance(data.index, pd.MultiIndex):\n",
    "                save_and_print(f\"Index levels: {data.index.nlevels}\", print_report_file, level=\"info\")\n",
    "            else:\n",
    "                save_and_print(f\"Index length: {len(data.index)}\", print_report_file, level=\"info\")\n",
    "                if len(data.index) <= 10:\n",
    "                    save_and_print(f\"Index names: {list(data.index[:10])}\", print_report_file, level=\"info\")\n",
    "    else:\n",
    "        save_and_print(f\"Type: {type(data)}\", print_report_file, level=\"info\")\n",
    "        if isinstance(data, dict):\n",
    "            save_and_print(f\"Dictionary keys: {len(data)}\", print_report_file, level=\"info\")\n",
    "            if len(data) <= 5:\n",
    "                save_and_print(f\"First keys: {list(data.keys())[:5]}\", print_report_file, level=\"info\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = get_memory_usage(data)\n",
    "    save_and_print(f\"Memory usage: {memory_mb:.2f} MB\", print_report_file, level=\"info\")\n",
    "    \n",
    "    # Data types (for DataFrames)\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        save_and_print(f\"Data types:\", print_report_file, level=\"info\")\n",
    "        dtype_summary = data.dtypes.value_counts()\n",
    "        for dtype, count in dtype_summary.items():\n",
    "            save_and_print(f\"  {dtype}: {count} columns\", print_report_file, level=\"info\")\n",
    "    \n",
    "    # Sample data preview\n",
    "    if isinstance(data, pd.DataFrame) and len(data) > 0:\n",
    "        save_and_print(f\"First few rows:\", print_report_file, level=\"info\")\n",
    "        save_and_print(data.head(3).to_string(), print_report_file, level=\"info\")\n",
    "    elif isinstance(data, dict) and len(data) > 0:\n",
    "        first_key = list(data.keys())[0]\n",
    "        save_and_print(f\"First key sample: {first_key}\", print_report_file, level=\"info\")\n",
    "        if isinstance(data[first_key], pd.Series):\n",
    "            preview_data = data[first_key].head(3) if len(data[first_key]) > 3 else data[first_key]\n",
    "            save_and_print(f\"First key data: {preview_data.to_string()}\", print_report_file, level=\"info\")\n",
    "        else:\n",
    "            save_and_print(f\"First key value type: {type(data[first_key])}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a459a",
   "metadata": {},
   "source": [
    "## Section 2: Consensus Analysis - Gene Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a22493",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "save_and_print(\"## Consensus Analysis - Gene Subsets\", print_report_file, level=\"section\")\n",
    "\n",
    "# Analyze consensus datasets\n",
    "consensus_datasets = {k: v for k, v in datasets.items() if 'consensus' in k}\n",
    "\n",
    "if not consensus_datasets:\n",
    "    save_and_print(\"No consensus datasets found in loaded data.\", print_report_file, level=\"info\")\n",
    "else:\n",
    "    for name, data in consensus_datasets.items():\n",
    "        save_and_print(f\"### Consensus Dataset: {name}\", print_report_file, level=\"subsection\")\n",
    "        \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            # Basic metrics\n",
    "            save_and_print(f\"Total features: {len(data)}\", print_report_file, level=\"info\")\n",
    "            \n",
    "            # Check if this is a consensus importance dataset\n",
    "            importance_columns = [col for col in data.columns if 'importance' in col.lower() or 'mean' in col.lower()]\n",
    "            \n",
    "            if importance_columns:\n",
    "                # Analyze top features\n",
    "                if 'mean_importance' in data.columns:\n",
    "                    top_feature_col = 'mean_importance'\n",
    "                elif 'importance_score' in data.columns:\n",
    "                    top_feature_col = 'importance_score'\n",
    "                else:\n",
    "                    # Use the first importance-like column\n",
    "                    top_feature_col = importance_columns[0]\n",
    "                \n",
    "                # Get top 10 features\n",
    "                top_10 = data.nlargest(10, top_feature_col)\n",
    "                save_and_print(f\"Top 10 features by {top_feature_col}:\", print_report_file, level=\"info\")\n",
    "                for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "                    importance_val = row[top_feature_col]\n",
    "                    if 'std' in data.columns:\n",
    "                        std_val = row.get('std_importance', row.get('std', 0))\n",
    "                        save_and_print(f\"  {i}. {idx}: {importance_val:.4f} ¬± {std_val:.4f}\", print_report_file, level=\"info\")\n",
    "                    else:\n",
    "                        save_and_print(f\"  {i}. {idx}: {importance_val:.4f}\", print_report_file, level=\"info\")\n",
    "                \n",
    "                # Stability metrics\n",
    "                if 'std_importance' in data.columns and 'mean_importance' in data.columns:\n",
    "                    cv_scores = data['std_importance'] / data['mean_importance']\n",
    "                    cv_scores = cv_scores.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                    \n",
    "                    save_and_print(f\"Stability metrics:\", print_report_file, level=\"info\")\n",
    "                    save_and_print(f\"  - Mean CV (std/mean): {cv_scores.mean():.4f}\", print_report_file, level=\"info\")\n",
    "                    save_and_print(f\"  - Std of CV: {cv_scores.std():.4f}\", print_report_file, level=\"info\")\n",
    "                    save_and_print(f\"  - Min CV: {cv_scores.min():.4f}\", print_report_file, level=\"info\")\n",
    "                    save_and_print(f\"  - Max CV: {cv_scores.max():.4f}\", print_report_file, level=\"info\")\n",
    "                \n",
    "                # Feature importance distribution\n",
    "                importance_values = data[top_feature_col]\n",
    "                save_and_print(f\"Importance distribution:\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Mean: {importance_values.mean():.4f}\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Std: {importance_values.std():.4f}\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Min: {importance_values.min():.4f}\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Max: {importance_values.max():.4f}\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Median: {importance_values.median():.4f}\", print_report_file, level=\"info\")\n",
    "                \n",
    "                # Count features with high/low importance\n",
    "                high_importance = len(importance_values[importance_values > importance_values.quantile(0.75)])\n",
    "                low_importance = len(importance_values[importance_values < importance_values.quantile(0.25)])\n",
    "                save_and_print(f\"  - High importance (top 25%): {high_importance} features\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Low importance (bottom 25%): {low_importance} features\", print_report_file, level=\"info\")\n",
    "            \n",
    "        elif isinstance(data, dict):\n",
    "            # Handle dictionary format consensus data\n",
    "            save_and_print(f\"Dictionary with {len(data)} features\", print_report_file, level=\"info\")\n",
    "            \n",
    "            # Convert to DataFrame for analysis if possible\n",
    "            if all(isinstance(v, dict) for v in data.values()):\n",
    "                try:\n",
    "                    df_data = []\n",
    "                    for feature, metrics in data.items():\n",
    "                        if isinstance(metrics, dict):\n",
    "                            row = {'feature': feature}\n",
    "                            row.update(metrics)\n",
    "                            df_data.append(row)\n",
    "                    \n",
    "                    if df_data:\n",
    "                        df = pd.DataFrame(df_data)\n",
    "                        df.set_index('feature', inplace=True)\n",
    "                        save_and_print(f\"Converted to DataFrame with {len(df)} features\", print_report_file, level=\"info\")\n",
    "                        \n",
    "                        # Analyze similar to DataFrame case\n",
    "                        importance_cols = [col for col in df.columns if 'importance' in col.lower() or 'mean' in col.lower()]\n",
    "                        if importance_cols:\n",
    "                            top_col = importance_cols[0]\n",
    "                            top_5 = df.nlargest(5, top_col)\n",
    "                            save_and_print(f\"Top 5 features:\", print_report_file, level=\"info\")\n",
    "                            for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "                                save_and_print(f\"  {i}. {idx}: {row[top_col]:.4f}\", print_report_file, level=\"info\")\n",
    "                except Exception as e:\n",
    "                    save_and_print(f\"Could not convert dictionary to DataFrame: {e}\", print_report_file, level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bcdc5",
   "metadata": {},
   "source": [
    "## Section 3: SHAP Bidirectional Bar Charts (Individual Conditions) - Gene Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65a5ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def parse_condition_title_genesubset(condition):\n",
    "    \"\"\"\n",
    "    Parse technical condition string and generate natural language title for gene subsets.\n",
    "    \n",
    "    Args:\n",
    "        condition: String like \"RandomForestRegressor_config1_k500_mrmr_cdk46_genesubset\"\n",
    "    \n",
    "    Returns:\n",
    "        Natural language title for the visualization\n",
    "    \"\"\"\n",
    "    parts = condition.split('_')\n",
    "    \n",
    "    # Map components to natural language\n",
    "    model_mapping = {\n",
    "        'RandomForestRegressor': 'Random Forest',\n",
    "        'MLPRegressor': 'Multi-layer Perceptron',\n",
    "        'KNeighborsRegressor': 'K-Neighbors'\n",
    "    }\n",
    "    \n",
    "    network_mapping = {\n",
    "        'cdk46': 'CDK4/6',\n",
    "        'fgfr4': 'FGFR4'\n",
    "    }\n",
    "    \n",
    "    dataset_mapping = {\n",
    "        'dynamic': 'Dynamic Features',\n",
    "        'genesubset': 'Network-Specific Gene Subsets',  # Changed from 'rnaseq'\n",
    "        'combined': 'Combined Dataset'\n",
    "    }\n",
    "    \n",
    "    # Extract components (handle variable length parts)\n",
    "    model = parts[0] if len(parts) > 0 else 'Unknown'\n",
    "    network = parts[-2] if len(parts) > 1 else 'Unknown'\n",
    "    dataset_type = parts[-1] if len(parts) > 0 else 'Unknown'\n",
    "    \n",
    "    # Apply natural language mapping\n",
    "    model_name = model_mapping.get(model, model)\n",
    "    network_name = network_mapping.get(network, network)\n",
    "    dataset_name = dataset_mapping.get(dataset_type, dataset_type)\n",
    "    \n",
    "    # Generate natural title emphasizing gene subsets\n",
    "    return f\"SHAP Analysis: {network_name} Target Model\\n{model_name} - {dataset_name}\\nDirectional Impact of Network-Specific Gene Subsets\"\n",
    "\n",
    "save_and_print(\"## SHAP Bidirectional Bar Charts: Gene Subsets - Positive vs Negative Impact\", print_report_file, level=\"section\")\n",
    "\n",
    "# Check if we have signed SHAP data\n",
    "if 'shap_consensus_importance_signed' in datasets:\n",
    "    signed_consensus = datasets['shap_consensus_importance_signed']\n",
    "    \n",
    "    save_and_print(\"### Signed SHAP Data Analysis - Gene Subsets\", print_report_file, level=\"subsection\")\n",
    "    \n",
    "    # Analyze the structure of signed data\n",
    "    if isinstance(signed_consensus, pd.DataFrame):\n",
    "        # Extract conditions from multiindex\n",
    "        conditions = signed_consensus.index.get_level_values(0).unique()\n",
    "        \n",
    "        save_and_print(f\"Available conditions in signed SHAP data: {list(conditions)}\", print_report_file, level=\"info\")\n",
    "        \n",
    "        # Create bidirectional bar charts for each condition\n",
    "        for condition in conditions:\n",
    "            save_and_print(f\"### Analysis for condition: {condition}\", print_report_file, level=\"subsection\")\n",
    "            \n",
    "            # Extract data for this condition\n",
    "            condition_data = signed_consensus.xs(condition, level=0, drop_level=False)\n",
    "            \n",
    "            if len(condition_data) == 0:\n",
    "                save_and_print(f\"No data found for condition: {condition}\", print_report_file, level=\"info\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze directional effects\n",
    "            if 'mean_importance_signed' in condition_data.columns:\n",
    "                positive_effects = condition_data[condition_data['mean_importance_signed'] > 0]\n",
    "                negative_effects = condition_data[condition_data['mean_importance_signed'] < 0]\n",
    "                neutral_effects = condition_data[condition_data['mean_importance_signed'] == 0]\n",
    "                \n",
    "                save_and_print(f\"Directional effects:\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Positive effects: {len(positive_effects)} features\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Negative effects: {len(negative_effects)} features\", print_report_file, level=\"info\")\n",
    "                save_and_print(f\"  - Neutral effects: {len(neutral_effects)} features\", print_report_file, level=\"info\")\n",
    "                \n",
    "                # Create bidirectional bar chart\n",
    "                try:\n",
    "                    # Set up publication-quality styling\n",
    "                    plt.style.use('seaborn-v0_8')\n",
    "                    plt.rcParams['font.family'] = 'sans-serif'\n",
    "                    plt.rcParams['font.size'] = 14\n",
    "                    plt.rcParams['axes.linewidth'] = 1.2\n",
    "                    \n",
    "                    # Take top 20 features by absolute signed importance (both positive and negative)\n",
    "                    condition_data_copy = condition_data.copy()\n",
    "                    condition_data_copy['abs_importance'] = condition_data_copy['mean_importance_signed'].abs()\n",
    "                    \n",
    "                    # Get top 20 features by absolute importance\n",
    "                    top_features = condition_data_copy.nlargest(20, 'abs_importance')\n",
    "                    \n",
    "                    # Sort by signed value for better visualization (negative to positive)\n",
    "                    top_features = top_features.sort_values('mean_importance_signed', ascending=True)\n",
    "                    \n",
    "                    # Create horizontal bar plot with color coding for direction\n",
    "                    fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
    "                    \n",
    "                    # Create color mapping based on sign\n",
    "                    colors = ['red' if x < 0 else 'blue' for x in top_features['mean_importance_signed']]\n",
    "                    \n",
    "                    # Create the bar plot\n",
    "                    bars = ax.barh(range(len(top_features)), top_features['mean_importance_signed'], \n",
    "                                  color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "                    \n",
    "                    # Set y-axis labels to feature names\n",
    "                    y_positions = range(len(top_features))\n",
    "                    feature_names = [str(feature) for feature in top_features.index.get_level_values(1)]\n",
    "                    ax.set_yticks(y_positions)\n",
    "                    ax.set_yticklabels(feature_names, fontsize=12)\n",
    "                    \n",
    "                    # Label axes\n",
    "                    ax.set_xlabel('Mean Signed SHAP Value', fontsize=14, fontweight='bold')\n",
    "                    ax.set_ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "                    # Generate natural title from condition (using gene subset version)\n",
    "                    natural_title = parse_condition_title_genesubset(condition)\n",
    "                    ax.set_title(f'SHAP Directional Feature Effects\\n{natural_title}\\n(Top 20 Features by Absolute Importance)', \n",
    "                                fontsize=16, fontweight='bold', pad=20)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "ode-biomarker-project",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
